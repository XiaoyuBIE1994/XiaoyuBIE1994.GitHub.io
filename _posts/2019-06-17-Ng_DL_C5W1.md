---

layout: post
title: Deep Learning by Andrew (5-1) 递归神经网络（Recurrent Neural Networks）
category: 笔记(DeepLearning)
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 1. 递归神经网络（Recurrent Neural Networks）



#### 1.1 时序模型

除了图片或者三维物体这样的没有时间维度而只有空间维度的数据之外，我们也经常需要处理含有时间序列的数据，如下图所示

![][1]



#### 1.2 数学符号

时序模型有着一些特定的数学符号，例如输入为 "Harry Potter and Herminone Granger invented a new spell",输出为 "1 1 0 1 1 0 0 0 0"（人名定位），则相关符号如下所示：

- $x^{<t>}$ 表示语句中第 t 个单词， $y^{<t>}$ 表示输出中第 t 个符号
- $T_x,T_y$ 表示输入和输出的长度
- $x^{(i)<t>}$ 表示第 i 个样本的第 t 个符号
- 利用单词字典来表示每一个输入的符号（几千到百万级别），利用一定的编码（如 one-hot）来实现输入和输出的映射关系



#### 1.2 RNN模型

对于学习 X 到 y 的映射，并不能使用传统的标准神经网络模型，如下所示：

![][2]

这是因为传统的模型存在以下两个问题：

- 模型的输入和输出在不同的例子中有不同的长度
- 不能够共享从文本的不同位置学习到的特征（文本信息主要在时序上存在特征，但传统的网络或者卷积网络只能提起空间上的特征，而文本的空间是多变的）

所以，我们需要一种新的网络结构来解决这一问题，这就是循环神经网络，这一网络结构会讲上一时刻的激活值送入到下一时刻的计算中，如下图所示：

![][3]



在零时刻，我们需要设置一个初始值（通常为零向量或者随机值），RNN 从左往右扫描，共享每一个时间步骤，其中：

- $W_{ax}$ 是输入 $x^{<i> }$ 到隐藏层的参数
- $W_{aa}$ 是上一时刻的激活值 $a^{<t-1>}$ 到隐藏层的参数
- $W_{ya}$ 是隐藏层到激活值的参数



**前向传播**

![][4]

如上图所示，前向传播的公式如下：


$$
\begin{align}
& a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\
& \hat{y}^{<t>} = g(W_{ya} a^{<t>} + b_y )
\end{align}
$$


也可以进行一定的简化：


$$
\begin{align}
& a^{<t>} = g(W_{a}\lbrack a^{<t-1>} ,x^{<t>}\rbrack + b_a) \\
& \hat{y}^{<t>} = g(W_{y} a^{<t>} + b_y )
\end{align}
$$


**反向传播**



损失函数可以用交叉熵来定义：


$$
\begin{align}
& L^{<t>}(\hat{y}^{<t>},y^{<t>}) = -y^{<t>} log\hat{y}^{<t>} - (1 - y^{<t>})log(1 - \hat{y}^{<t>}) \\
& L(\hat{y}^{<t>},y^{<t>}) = \sum^{T_y}_{t=1} L^{<t>}(\hat{y}^{<t>},y^{<t>})
\end{align}
$$


在RNN中，反向传播是基于时间来进行



#### 1.3 不同种类的 RNN



![][5]



- **One to one:** 单一的输入输出，$a^{<0>}$ 不需要
- **One to many:** 音乐生成
- **Many to one:** 判断是否喜爱某一部电影
- **Many to many:** 输入和输出的大小一致，比如判断一句话中每个单词是否是人名
- **Many to many:** 输入和输出的大小不一致，比如翻译



#### 1.4 语言模型与序列生成

语言建模（language modeling）是自然语言处理（natural language processing）中最基础和最重要的任务之一，也是 RNN 非常擅长的领域之一



**什么是语言模型**

语言模型的功能在于给定任意句子，它都会给出特定句子的概率，比如在语音识别中，

- The apple and pair salad
- The apple and pear salad

这两句话听起来非常的相似，但语言会告诉你两个句子出现的概率：

- $P(\text{The apple and pair salad}) = 3.2 \times 10^{-13} $
- $P(\text{The apple and pear salad}) = 5.7 \times 10^{-10} $

显然听到是第二个句子的概率更高，所以语音shib8ie系统会选择第二个选项。所以语言模型的功能在于给定任意的句子，它都会给出这个句子的概率



**基于RNN的语言模型：**

- 训练集：大文集（le.g. arge corpus of english text）

- tokenize：讲句子标记化，其中句子结尾用 $<EOS> $，不在字典中的单词用 $<UNK>$ 表示

- 第一步，使用零向量进行输出入，预测后面一个单词出现的概率

- 第二步，通过前面的输出，预测后面一个单词出现的概率

- 训练网络，使用 softmax 计算损失

  

![][6]



#### 1.5  新序列采样

在训练了一个序列模型后，我们可以通过采样新的序列非正式的了解它学到了什么，如下：



![][7]

这是一个已经训练好的 RNN 模型，我们为了采样，需要：

- 首先输入 $x^{<1>} = 0， a^{<1>} = 0$ ，在这第一个时间步，我们得到所有可能的输出经过 softmax 层后可能的概率，根据这个概率分布进行随机采样，获取第一个随机采样的单词 $y^{<1>}$ 
- 将 $y^{<1>}$ 作为下一个时间步的输入，进而得到下一个时间步的输出，以此类推
- 如果字典中有结束的标志如 $<EOS>$ ，则表示结束，若没有，我们可以自行设定结束时间



#### 1.6 RNN 中的梯度消失

RNN 也存在梯度消失的问题，如下两句：

- The cat, which already ate ..., was full
- The cats, which already ate ..., were full

在这两个句子中， cat 对应 was，cats 对应 were，省略号是被省略的单词，此时句子中存在着长期依赖（long-term dependencies），前面的单词对后面的单词有很重要的影响，但基础的 RNN 模型是不擅长处理这种依赖关系的

**Notation**

- 梯度爆炸问题也会出现，网络参数会直接输出 Nan，但由于很容易发现，所以可以采用梯度修剪来应对这一问题（观察梯度向量，若大于某一阈值，则对其进行缩放，以免梯度过大）



#### 1.7 GRU 单元





#### 1.8 LSTM





#### 1.9 双向 RNN





#### 1.10 深层 RNN









[1]:https://res.cloudinary.com/bxy1994/image/upload/v1560780159/DL_coursera/SequenceData_example.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1561151331/DL_coursera/RNN_StandardNet.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1561151331/DL_coursera/RNN_model.jpg
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1561151762/DL_coursera/RNN_ForwardPropagation.png
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1561155848/DL_coursera/RNN_architecture.jpg
[6]: https://res.cloudinary.com/bxy1994/image/upload/v1561467001/DL_coursera/RNN_LanguageModel.jpg
[7]:https://res.cloudinary.com/bxy1994/image/upload/v1561497015/DL_coursera/RNN_sampling.png