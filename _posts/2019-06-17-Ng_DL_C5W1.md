---

layout: post
title: Deep Learning by Andrew (5-1) 递归神经网络（Recurrent Neural Networks）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 1. 递归神经网络（Recurrent Neural Networks）



#### 1.1 时序模型

除了图片或者三维物体这样的没有时间维度而只有空间维度的数据之外，我们也经常需要处理含有时间序列的数据，如下图所示

![][1]



#### 1.2 数学符号

时序模型有着一些特定的数学符号，例如输入为 "Harry Potter and Herminone Granger invented a new spell",输出为 "1 1 0 1 1 0 0 0 0"（人名定位），则相关符号如下所示：

- $x^{<t>}$ 表示语句中第 t 个单词， $y^{<t>}$ 表示输出中第 t 个符号
- $T_x,T_y$ 表示输入和输出的长度
- $x^{(i)<t>}$ 表示第 i 个样本的第 t 个符号
- 利用单词字典来表示每一个输入的符号（几千到百万级别），利用一定的编码（如 one-hot）来实现输入和输出的映射关系



#### 1.2 RNN模型

对于学习 X 到 y 的映射，并不能使用传统的标准神经网络模型，如下所示：

![][2]

这是因为传统的模型存在以下两个问题：

- 模型的输入和输出在不同的例子中有不同的长度
- 不能够共享从文本的不同位置学习到的特征（文本信息主要在时序上存在特征，但传统的网络或者卷积网络只能提起空间上的特征，而文本的空间是多变的）

所以，我们需要一种新的网络结构来解决这一问题，这就是循环神经网络，这一网络结构会讲上一时刻的激活值送入到下一时刻的计算中，如下图所示：

![][3]



在零时刻，我们需要设置一个初始值（通常为零向量或者随机值），RNN 从左往右扫描，共享每一个时间步骤，其中：

- $W_{ax}$ 是输入 $x^{<i> }$ 到隐藏层的参数
- $W_{aa}$ 是上一时刻的激活值 $a^{<t-1>}$ 到隐藏层的参数
- $W_{ya}$ 是隐藏层到激活值的参数



**前向传播**

![][4]

如上图所示，前向传播的公式如下：


$$
\begin{align}
& a^{<t>} = g(W_{aa}a^{<t-1>} + W_{ax}x^{<t>} + b_a) \\
& \hat{y}^{<t>} = g(W_{ya} a^{<t>} + b_y )
\end{align}
$$


也可以进行一定的简化：


$$
\begin{align}
& a^{<t>} = g(W_{a}\lbrack a^{<t-1>} ,x^{<t>}\rbrack + b_a) \\
& \hat{y}^{<t>} = g(W_{y} a^{<t>} + b_y )
\end{align}
$$


**反向传播**



损失函数可以用交叉熵来定义：


$$
\begin{align}
& L^{<t>}(\hat{y}^{<t>},y^{<t>}) = -y^{<t>} log\hat{y}^{<t>} - (1 - y^{<t>})log(1 - \hat{y}^{<t>}) \\
& L(\hat{y}^{<t>},y^{<t>}) = \sum^{T_y}_{t=1} L^{<t>}(\hat{y}^{<t>},y^{<t>})
\end{align}
$$


在RNN中，反向传播是基于时间来进行



#### 1.3 不同种类的 RNN



![][5]



- **One to one:** 单一的输入输出，$a^{<0>}$ 不需要
- **One to many:** 音乐生成等
- **Many to one:** 判断是否喜爱某一部电影，性别判定等
- **Many to many:** 输入和输出的大小一致，比如判断一句话中每个单词是否是人名等
- **Many to many:** 输入和输出的大小不一致，比如翻译



#### 1.4 语言模型与序列生成

语言建模（language modeling）是自然语言处理（natural language processing）中最基础和最重要的任务之一，也是 RNN 非常擅长的领域之一



**什么是语言模型**

语言模型的功能在于给定任意句子，它都会给出特定句子的概率，比如在语音识别中，

- The apple and pair salad
- The apple and pear salad

这两句话听起来非常的相似，但语言会告诉你两个句子出现的概率：

- $P(\text{The apple and pair salad}) = 3.2 \times 10^{-13} $
- $P(\text{The apple and pear salad}) = 5.7 \times 10^{-10} $

显然听到是第二个句子的概率更高，所以语音shib8ie系统会选择第二个选项。所以语言模型的功能在于给定任意的句子，它都会给出这个句子的概率



**基于RNN的语言模型：**

- 训练集：大文集（le.g. arge corpus of english text）

- tokenize：讲句子标记化，其中句子结尾用 $<EOS> $，不在字典中的单词用 $<UNK>$ 表示

- 第一步，使用零向量进行输出入，预测后面一个单词出现的概率

- 第二步，通过前面的输出，预测后面一个单词出现的概率

- 训练网络，使用 softmax 计算损失

  

![][6]



#### 1.5  新序列采样

在训练了一个序列模型后，我们可以通过采样新的序列非正式的了解它学到了什么，如下：



![][7]

这是一个已经训练好的 RNN 模型，我们为了采样，需要：

- 首先输入 $x^{<1>} = 0， a^{<1>} = 0$ ，在这第一个时间步，我们得到所有可能的输出经过 softmax 层后可能的概率，根据这个概率分布进行随机采样，获取第一个随机采样的单词 $y^{<1>}$ 
- 将 $y^{<1>}$ 作为下一个时间步的输入，进而得到下一个时间步的输出，以此类推
- 如果字典中有结束的标志如 $<EOS>$ ，则表示结束，若没有，我们可以自行设定结束时间



#### 1.6 RNN 中的梯度消失

RNN 也存在梯度消失的问题，如下两句：

- The cat, which already ate ..., was full
- The cats, which already ate ..., were full

在这两个句子中， cat 对应 was，cats 对应 were，省略号是被省略的单词，此时句子中存在着长期依赖（long-term dependencies），前面的单词对后面的单词有很重要的影响，但基础的 RNN 模型是不擅长处理这种依赖关系的

**Notation**

- 梯度爆炸问题也会出现，网络参数会直接输出 Nan，但由于很容易发现，所以可以采用梯度修剪来应对这一问题（观察梯度向量，若大于某一阈值，则对其进行缩放，以免梯度过大）



#### 1.7 GRU 单元

门控循环单元（Gated Recurrent Unit, GRU）是利用 sigmoid 函数很容易达到0或者1的特性来对新的输入进行一个门控。在从左到右的计算中，GRU单元存在一个新的变量称为 $c$ (cell)，作为“记忆细胞”提供长期记忆能力，并且解决一部分RNN中梯度消失的问题

**GRU(simplified)**

- $\tilde{c}^{<t>} = tanh(w_c \lbrack c^{<t-1>}, x^{<t>} \rbrack + b_c)$

- $\Gamma_u = \sigma(w_u \lbrack c^{<t-1>}, x^{<t>} \rbrack + b_u)$

- $c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1- \Gamma_u) * c^{<t-1>}$

- $a^{<t>} = c^{<t>}$

- $c^{<t>}$， $\tilde{c}^{<t>}$和$\Gamma_u$ 拥有相同的纬度

  

**GRU(full)**

- $\tilde{c}^{<t>} = tanh(w_c \lbrack \Gamma_r * c^{<t-1>}, x^{<t>} \rbrack + b_c)$
- $\Gamma_u = \sigma(w_u \lbrack c^{<t-1>}, x^{<t-1>} \rbrack + b_u)$
- $\Gamma_r = \sigma(w_r \lbrack c^{<t-1>}, x^{<t-1>} \rbrack + b_r)$
- $c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1- \Gamma_u) * c^{<t-1>}$
- $a^{<t>} = c^{<t>}$ 



#### 1.8 LSTM

LSTM是GRU的一个改进版，更加的强大和并且拥有更好的泛化功能，公式及示意图如下：



- $\tilde{c}^{<t>} = tanh(w_c \lbrack a^{<t-1>}, x^{<t>} \rbrack + b_c)$
- $\Gamma_u = \sigma(w_u \lbrack a^{<t-1>}, x^{<t>} \rbrack + b_u)$ （更新门）
- $\Gamma_f = \sigma(w_f \lbrack a^{<t-1>}, x^{<t>} \rbrack + b_f)$ （遗忘门）
- $\Gamma_o = \sigma(w_o \lbrack a^{<t-1>}, x^{<t>} \rbrack + b_o)$ （输出门）
- $c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + \Gamma_f * c^{<t-1>}$
- $a^{<t>} = \Gamma_o * tanh \; c^{<t>}$
- $y^{<t>} = softmax(w_y \cdot a^{<t>} + b_y )$

![][8]



可以看出，当合理的设置遗忘和更新门，$c^{<t>}$的值很容易传递下去，储存在记忆单元里的值可以维持很多步骤。LSTM也有很多变体，最常用的是在门控中不仅使用 $a^{<t-1>}$ 和 $x^{<t>}$，同时也加入 $c^{<t-1>}$ ，这个叫做窥孔连接（peephole connection）



#### 1.9 双向 RNN

单向RNN只能使用之前的输出，但实际的语义理解中，我们需要同时结合前后文来进行判断，在NLP问题中，结合LSTM的BRNN网络被证明是非常有效的。

双向RNN的示意图如下，实际上是在原有的前向连接层中加入后向连阶层，由这两个层的输出来共同决定预测的输出：


$$
\hat{y}^{<t>} = g(W_y \lbrack \overrightarrow{a}^{<t>},\;  \overleftarrow{a}^{<t>} \rbrack + b_y)
$$




![][9]



#### 1.10 深层 RNN

如同CNN一样，RNN也可以进行层数的堆叠，如下图所示。以第二行第三列的模块为例，其输入输出关系如下：


$$
a^{[2]<3>} = g(W_a^{[2]} \lbrack a^{[2]<2>}, \; a^{[1]<3>} \rbrack + b_a^{[3]})
$$


但和CNN中堆叠上百层不同，RNN由于存在时间序列的原因，层数一般不会堆叠太多（3层已经比较多了），但可以在RNN后面在连接上另一个深层神经网络。同时，Deep RNN也可以和BRNN结合，加入双向序列以获得更多的信息。

 

![][10]



## 2. 代码实现

以下代码参考自吴恩达的Deep Learning课程作业，并不是作业的答案，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 



### 2.1 简单RNN模型

#### 2.1.1 RNN cell

RNN的基本模型如下：

![][11]



RNN网络可以被认为是记忆细胞的重复过程，常见的细胞（cell）的结构如下：

![][12]

具体步骤如下：

- 计算隐藏层的激活值 $a^{\langle t \rangle} = \tanh(W_{aa} a^{\langle t-1 \rangle} + W_{ax} x^{\langle t \rangle} + b_a)$
- 利用最新的隐藏层的值来计算预测值 $\hat{y}^{\langle t \rangle} = softmax(W_{ya} a^{\langle t \rangle} + b_y)$
- 储存$(a^{\langle t \rangle}, a^{\langle t-1 \rangle}, x^{\langle t \rangle}, parameters)$在缓存中（cache）
- 返回$a^{\langle t \rangle}$ , $y^{\langle t \rangle}$和缓存（cache）



```python
def rnn_cell_forward(xt, a_prev, parameters):
    # Retrieve parameters from "parameters"
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]
    # compute next activation state
    a_next = np.tanh(np.dot(Waa, a_prev) + np.dot(Wax, xt) + ba)
    # compute output of the current cell 
    yt_pred = softmax(np.dot(Wya, a_next) + by) 
    # store values you need for backward propagation in cache
    cache = (a_next, a_prev, xt, parameters)
    
    return a_next, yt_pred, cache
```



#### 2.2.2 RNN前向传播

当构建了单个细胞的前向传播函数后，便可以建立整个RNN基于时间序列的前向传播：



![][13]

具体步骤如下：

- 创建 $a$ 和 $y_{pred}$ 来储存所有的中间值
- 初始化 $a_0$
- 开始基于时间序列的循环，将每一步的结果存在对应的 $a$、$y_{pred}$和 $caches$ 中
- 返回$a$、$y_{pred}$和 $caches$ 

```python
def rnn_forward(x, a0, parameters):
    # Retrieve dimensions from shapes of x and parameters["Wya"]
    n_x, m, T_x = x.shape
    n_y, n_a = parameters["Wya"].shape
    # initialize "a", "y" and "caches"
    caches = []
    a = np.zeros((n_a, m, T_x))
    y_pred = np.zeros((n_y, m, T_x))
    # Initialize a_next
    a_next = a0
    
    # loop over all time-steps
    for t in range(T_x):
        a_next, yt_pred, cache = rnn_cell_forward(x[:,:,t], a_next, parameters)
        a[:,:,t] = a_next
        y_pred[:,:,t] = yt_pred
        caches.append(cache)
    
    # store values needed for backward propagation in cache
    caches = (caches, x)
    
    return a, y_pred, caches
```



### 2.2 LSTM模块

#### 2.2.1 LSTM cell

LSTM的cell模块如下图所示，包含有 forget gate，update gate 和 output gate，分别决定是否遗忘前置信息，是否利用当前的信息进行更新以及如何输出结果

![][14]



PS：代码中更新门用下标 $i$ 而没有用下标 $u$，用 $f$ 代替了 $\Gamma$ ，$cct$ 代替了 $\tilde{c}^{\langle t \rangle}$ 



> xt -- numpy array of shape (n_x, m)
>
> a_prev -- numpy array of shape (n_a, m)
>
> c_prev -- numpy array of shape (n_a, m)



```python
def lstm_cell_forward(xt, a_prev, c_prev, parameters):
    # Retrieve parameters from "parameters"
    Wf = parameters["Wf"]
    bf = parameters["bf"]
    Wi = parameters["Wi"]
    bi = parameters["bi"]
    Wc = parameters["Wc"]
    bc = parameters["bc"]
    Wo = parameters["Wo"]
    bo = parameters["bo"]
    Wy = parameters["Wy"]
    by = parameters["by"]
    # Retrieve dimensions from shapes of xt and Wy
    n_x, m = xt.shape
    n_y, n_a = Wy.shape
    # Concatenate a_prev and xt
    concat = np.zeros((n_a+n_x, m))
    concat[: n_a, :] = a_prev
    concat[n_a :, :] = xt
    # Compute values for ft, it, cct, c_next, ot, a_next using the formulas given in the figure
    ft = sigmoid(np.dot(Wf, concat) + bf)
    it = sigmoid(np.dot(Wi, concat) + bi)
    cct = np.tanh(np.dot(Wc, concat) + bc)
    c_next = np.multiply(ft, c_prev) + np.multiply(it, cct)
    ot = sigmoid(np.dot(Wo, concat) + bo)
    a_next = np.multiply(ot, np.tanh(c_next))
    # Compute prediction of the LSTM cell
    yt_pred = softmax(np.dot(Wy, a_next) + by)
    # store values needed for backward propagation in cache
    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)

    return a_next, c_next, yt_pred, cache
```



#### 2.2.2 LSTM 前向传播

同RNN的前向传播类似，LSTM的前向传播本质上就是 LSTM cell 在循环的输入新的数据，记录下中间变量并且输出相应的结果，如下图所示：



![][15]



> x -- Input data for every time-step of shape (n_x, m, T_x)
>
> a0 -- Initial hidden state, of shape (n_a, m)
>
> a -- Hidden states for every time-step, of shape (n_a, m, T_x)
>
> y -- Predictions for every time-step, of shape (n_y, m, T_x)



```python
def lstm_forward(x, a0, parameters):
    # Initialize "caches"
    caches = []
    # Retrieve dimensions from shapes of x and parameters['Wy']
    n_x, m, T_x = x.shape
    n_y, n_a = parameters['Wy'].shape
    # initialize "a", "c" and "y" with zeros
    a = np.zeros((n_a, m, T_x))
    c = np.zeros((n_a, m, T_x))
    y = np.zeros((n_y, m, T_x))
    # Initialize a_next and c_next
    a_next = np.zeros((n_a, m))
    c_next = np.zeros((n_a, m))
    # loop over all time-steps
    for t in range(T_x):
        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)
        a[:,:,t] = a_next
        y[:,:,t] = yt
        c[:,:,t]  = c_next
        caches.append(cache)
        
    caches = (caches, x)

    return a, y, c, caches
```



### 2.3 反向传播

#### 2.3.1 RNN的反向传播



![][17]



#### 2.3.2 LSTM的反向传播

**gate derivatives**


$$
\begin{aligned}
& d \Gamma_o^{\langle t \rangle} = da_{next}*\tanh(c_{next}) * \Gamma_o^{\langle t \rangle}*(1-\Gamma_o^{\langle t \rangle}) \\
& d\tilde c^{\langle t \rangle} = dc_{next}*\Gamma_u^{\langle t \rangle}+ \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * i_t * da_{next} * \tilde c^{\langle t \rangle} * (1-\tanh(\tilde c)^2)\\
& d\Gamma_u^{\langle t \rangle} = dc_{next}*\tilde c^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * \tilde c^{\langle t \rangle} * da_{next}*\Gamma_u^{\langle t \rangle}*(1-\Gamma_u^{\langle t \rangle}) \\
& d\Gamma_f^{\langle t \rangle} = dc_{next}*\tilde c_{prev} + \Gamma_o^{\langle t \rangle} (1-\tanh(c_{next})^2) * c_{prev} * da_{next}*\Gamma_f^{\langle t \rangle}*(1-\Gamma_f^{\langle t \rangle})
\end{aligned}
$$


**parameter derivatives**


$$
\begin{aligned}
& dW_f = d\Gamma_f^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \\
& dW_u = d\Gamma_u^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T  \\
& dW_c = d\tilde c^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T  \\
& dW_o = d\Gamma_o^{\langle t \rangle} * \begin{pmatrix} a_{prev} \\ x_t\end{pmatrix}^T \\
\end{aligned}
$$


**previous hidden state renew**


$$
\begin{aligned}
& da_{prev} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c^{\langle t \rangle} + W_o^T * d\Gamma_o^{\langle t \rangle} \\
& dc_{prev} = dc_{next}\Gamma_f^{\langle t \rangle} + \Gamma_o^{\langle t \rangle} * (1- \tanh(c_{next})^2)*\Gamma_f^{\langle t \rangle}*da_{next} \\
& dx^{\langle t \rangle} = W_f^T*d\Gamma_f^{\langle t \rangle} + W_u^T * d\Gamma_u^{\langle t \rangle}+ W_c^T * d\tilde c_t + W_o^T * d\Gamma_o^{\langle t \rangle}
\end{aligned}
$$


### 2.4 Character-Level Language Modeling

恐龙名字生成，用数字来代替每个字母：

```python
char_to_ix = { ch:i for i,ch in enumerate(sorted(chars)) }
ix_to_char = { i:ch for i,ch in enumerate(sorted(chars)) }
print(ix_to_char)
```

{0: '\n', 1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z'}

为了加快收敛速度，我们需要限制每一步梯度更新的步长不超过一定阈值：

```python
def clip(gradients, maxValue):  
    dWaa, dWax, dWya, db, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['db'], gradients['dby']
   
    for gradient in [dWax, dWaa, dWya, db, dby]:
         np.clip(gradient, -maxValue, maxValue, out = gradient)

    gradients = {"dWaa": dWaa, "dWax": dWax, "dWya": dWya, "db": db, "dby": dby}
    
    return gradients
```

当学习完模型后，恐龙的名字生成相当于新序列采样问题：

![][18]



每一步更新如下：


$$
\begin{aligned}
& a^{\langle t+1 \rangle} = \tanh(W_{ax}  x^{\langle t+1 \rangle } + W_{aa} a^{\langle t \rangle } + b) \\
& z^{\langle t + 1 \rangle } = W_{ya}  a^{\langle t + 1 \rangle } + b_y \\
& \hat{y}^{\langle t+1 \rangle } = softmax(z^{\langle t + 1 \rangle })
\end{aligned}
$$


```python
def sample(parameters, char_to_ix, seed):
    # Retrieve parameters and relevant shapes from "parameters" dictionary
    Waa, Wax, Wya, by, b = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['b']
    vocab_size = by.shape[0]
    n_a = Waa.shape[1]
    # Step 1: Create the one-hot vector x, initialize a_prev as zeros and create an empty list of indices
    x = np.zeros((vocab_size, 1))
    a_prev = np.zeros((n_a, 1))
    indices = []
    
    # Idx is a flag to detect a newline character, we initialize it to -1
    idx = -1 
    counter = 0
    newline_character = char_to_ix['\n']
    
    while (idx != newline_character and counter != 50):
        # Step 2: Forward propagate x using the equations
        a = np.tanh(np.dot(Wax, x) + np.dot(Waa, a_prev) + b)
        z = np.dot(Wya, a) + by
        y = softmax(z)
        
        # Step 3: Sample the index of a character within the vocabulary from the probability distribution y
        # np.random.seed(counter+seed) 
        idx = np.random.choice(vocab_size, p = y.ravel())
        # Append the index to "indices"
        indices.append(idx)
        
        # Step 4: Overwrite the input character as the one corresponding to the sampled index.
        x = x = np.zeros((vocab_size, 1))
        x[idx] = 1
        # Update "a_prev" to be "a"
        a_prev = a
        # for grading purposes
        seed += 1
        counter +=1
    if (counter == 50):
        indices.append(char_to_ix['\n'])
    
    return indices
```



```python
def optimize(X, Y, a_prev, parameters, learning_rate = 0.01):
    # Forward propagate through time
    loss, cache = rnn_forward(X, Y, a_prev, parameters)
    # Backpropagate through time
    gradients, a = rnn_backward(X, Y, parameters, cache)
    # Clip your gradients between -5 and 5 
    gradients = clip(gradients, 5)
    # Update parameters
    parameters = update_parameters(parameters, gradients, learning_rate)
    return loss, gradients, a[len(X)-1]
```

```python
def model(data, ix_to_char, char_to_ix, num_iterations = 35000, n_a = 50, dino_names = 7, vocab_size = 27):
    # Retrieve n_x and n_y from vocab_size
    n_x, n_y = vocab_size, vocab_size
    # Initialize parameters
    parameters = initialize_parameters(n_a, n_x, n_y)
    # Initialize loss (this is required because we want to smooth our loss, don't worry about it)
    loss = get_initial_loss(vocab_size, dino_names)
    # Build list of all dinosaur names (training examples).
    with open("dinos.txt") as f:
        examples = f.readlines()
    examples = [x.lower().strip() for x in examples]
    # Shuffle list of all dinosaur names
    np.random.seed(0)
    np.random.shuffle(examples)
    # Initialize the hidden state of your LSTM
    a_prev = np.zeros((n_a, 1))
    
    # Optimization loop
    for j in range(num_iterations):
        # Use the hint above to define one training example (X,Y) (≈ 2 lines)
        index = j % len(examples)
        X = [None] + [char_to_ix[ch] for ch in examples[index]]
        Y = X[1:] + [char_to_ix["\n"]]
        # Perform one optimization step: Forward-prop -> Backward-prop -> Clip -> Update parameters
        # Choose a learning rate of 0.01
        curr_loss, gradients, a_prev = optimize(X, Y, a_prev, parameters, learning_rate = 0.01)
        # Use a latency trick to keep the loss smooth. It happens here to accelerate the training.
        loss = smooth(loss, curr_loss)
        # Every 2000 Iteration, generate "n" characters thanks to sample() to check if the model is learning properly
        if j % 2000 == 0:
            print('Iteration: %d, Loss: %f' % (j, loss) + '\n')
            # The number of dinosaur names to print
            seed = 0
            for name in range(dino_names):
                # Sample indices and print them
                sampled_indices = sample(parameters, char_to_ix, seed)
                print_sample(sampled_indices, ix_to_char)
                seed += 1  # To get the same result for grading purposed, increment the seed by one. 
            print('\n')
    return parameters
```



### 2.5 Jazz Improvisation with LSTM

这里使用Keras内建的LSTM模块来构建音乐自动生成算法

```python
# Required functions
from __future__ import print_function
import IPython
import sys
from music21 import *
import numpy as np
from grammar import *
from qa import *
from preprocess import * 
from music_utils import *
from data_utils import *
from keras.models import load_model, Model
from keras.layers import Dense, Activation, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector
from keras.initializers import glorot_uniform
from keras.utils import to_categorical
from keras.optimizers import Adam
from keras import backend as K
```

```python
# Data load
IPython.display.Audio('./data/30s_seq.mp3')
X, Y, n_values, indices_values = load_music_utils()f
```

shape of X: (60, 30, 78)
number of training examples: 60
Tx (length of sequence): 30
total # of unique values: 78
Shape of Y: (30, 60, 78)

![][19]



```python
n_a = 64 
reshapor = Reshape((1, 78)) 
LSTM_cell = LSTM(n_a, return_state = True)
densor = Dense(n_values, activation='softmax')

def djmodel(Tx, n_a, n_values):
    # Define the input of your model with a shape 
    X = Input(shape=(Tx, n_values))
    # Define s0, initial hidden state for the decoder LSTM
    a0 = Input(shape=(n_a,), name='a0')
    c0 = Input(shape=(n_a,), name='c0')
    a = a0
    c = c0
    # Step 1: Create empty list to append the outputs while you iterate (≈1 line)
    outputs = []
    # Step 2: Loop
    for t in range(Tx):
        # Step 2.A: select the "t"th time step vector from X. 
        x = Lambda(lambda x: X[:,t,:])(X)
        # Step 2.B: Use reshapor to reshape x to be (1, n_values) (≈1 line)
        x = reshapor(x)
        # Step 2.C: Perform one step of the LSTM_cell
        a, _, c = LSTM_cell(x, initial_state=[a, c])
        # Step 2.D: Apply densor to the hidden state output of LSTM_Cell
        out = densor(a)
        # Step 2.E: add the output to "outputs"
        outputs.append(out)
    # Step 3: Create model instance
    model = Model(inputs=[X, a0, c0], outputs=outputs)
    return model
```



```python
# Model generation and training
model = djmodel(Tx = 30 , n_a = 64, n_values = 78)
opt = Adam(lr=0.01, beta_1=0.9, beta_2=0.999, decay=0.01)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
m = 60
a0 = np.zeros((m, n_a))
c0 = np.zeros((m, n_a))
model.fit([X, a0, c0], list(Y), epochs=100)
```

```python
# Music inference model
def music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 100):
    # Define the input of your model with a shape 
    x0 = Input(shape=(1, n_values))
    # Define s0, initial hidden state for the decoder LSTM
    a0 = Input(shape=(n_a,), name='a0')
    c0 = Input(shape=(n_a,), name='c0')
    a = a0
    c = c0
    x = x0
    # Step 1: Create an empty list of "outputs" to later store your predicted values (≈1 line)
    outputs = []
    # Step 2: Loop over Ty and generate a value at every time step
    for t in range(Ty):
        # Step 2.A: Perform one step of LSTM_cell (≈1 line)
        a, _, c = LSTM_cell(x, initial_state=[a, c])
        # Step 2.B: Apply Dense layer to the hidden state output of the LSTM_cell (≈1 line)
        out = densor(a)
        # Step 2.C: Append the prediction "out" to "outputs". out.shape = (None, 78) (≈1 line)
        outputs.append(out)
        # Step 2.D: Select the next value according to "out", and set "x" to be the one-hot representation of the selected value
        x = Lambda(one_hot)(out)
    # Step 3: Create model instance with the correct "inputs" and "outputs" (≈1 line)
    inference_model = Model(inputs=[x0, a0, c0], outputs=outputs)
    return inference_model
  
# Music prediction
def predict_and_sample(inference_model, x_initializer = x_initializer, a_initializer = a_initializer, c_initializer = c_initializer):
    # Step 1: Use your inference model to predict an output sequence given x_initializer, a_initializer and c_initializer.
    pred = inference_model.predict([x_initializer, a_initializer, c_initializer])
    # Step 2: Convert "pred" into an np.array() of indices with the maximum probabilities
    indices = np.argmax(pred, axis=-1)
    # Step 3: Convert indices to one-hot vectors, the shape of the results should be (Ty, n_values)
    results = to_categorical(indices, num_classes=x_initializer.shape[-1])
    return results, indices
```



```python
# Jazz generation
inference_model = music_inference_model(LSTM_cell, densor, n_values = 78, n_a = 64, Ty = 50)
x_initializer = np.zeros((1, 1, 78))
a_initializer = np.zeros((1, n_a))
c_initializer = np.zeros((1, n_a))
results, indices = predict_and_sample(inference_model, x_initializer, a_initializer, c_initializer)
out_stream = generate_music(inference_model # pre-defined function
```




[1]:https://res.cloudinary.com/bxy1994/image/upload/v1560780159/DL_coursera/SequenceData_example.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1561151331/DL_coursera/RNN_StandardNet.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1561151331/DL_coursera/RNN_model.jpg
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1561151762/DL_coursera/RNN_ForwardPropagation.png
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1561155848/DL_coursera/RNN_architecture.jpg
[6]: https://res.cloudinary.com/bxy1994/image/upload/v1561467001/DL_coursera/RNN_LanguageModel.jpg
[7]:https://res.cloudinary.com/bxy1994/image/upload/v1561497015/DL_coursera/RNN_sampling.png
[8]: https://res.cloudinary.com/bxy1994/image/upload/v1566571688/DL_coursera/LSTM.png

[9]: https://res.cloudinary.com/bxy1994/image/upload/v1566744251/DL_coursera/BRNN.jpg
[10]:https://res.cloudinary.com/bxy1994/image/upload/v1566745253/DL_coursera/DeppRNN.png
[11]:https://res.cloudinary.com/bxy1994/image/upload/v1566752348/DL_coursera/RNN_basic.png
[12]:https://res.cloudinary.com/bxy1994/image/upload/v1566752350/DL_coursera/RNN_cell.png
[13]:https://res.cloudinary.com/bxy1994/image/upload/v1566752853/DL_coursera/RNN_ForwardPass.png
[14]: https://res.cloudinary.com/bxy1994/image/upload/v1566850781/DL_coursera/LSTM_scheme.png
[15]:https://res.cloudinary.com/bxy1994/image/upload/v1566857151/DL_coursera/LSTM_ForwardPass.png
[16]:https://res.cloudinary.com/bxy1994/image/upload/v1566857151/DL_coursera/RNN_BackwardPropagation.png

[17]:https://res.cloudinary.com/bxy1994/image/upload/v1567453812/DL_coursera/RNN_backprop.png
[18]:https://res.cloudinary.com/bxy1994/image/upload/v1567188565/DL_coursera/RNN_Sample.png
[19]:https://res.cloudinary.com/bxy1994/image/upload/v1567456127/DL_coursera/LSTM_JazzCreation.png
