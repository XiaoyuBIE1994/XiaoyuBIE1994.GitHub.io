---
layout: post
title: Deep Learning by Andrew (二) 深层神经网络
category: 笔记
tags: 
  - deep learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、 深度神经网络的矩阵维度

DNN是拥有较深网络层级的神经网络，其结构如下图所示：

![](https://res.cloudinary.com/bxy1994/image/upload/v1548714007/DL_coursera/dnn_notation_ztzxi8.jpg)

对于第 $l$ 层的网络，单个样本的各个参数的矩阵维度表示如下：

- $W^{[l]} : (n^{[l]}, n^{[l-1]})$  
- $b^{[l]} : (n^{[l]}, 1)$  
- $dW^{[l]} : (n^{[l]}, n^{[l-1]})$  
- $db^{[l]} : (n^{[l]}, 1)$  
- $Z^{[l]} : (n^{[l]}, 1)$  

在组建深度神经网络时，为了避免代码出错，可以通过检查矩阵的维度关系来检测代码的正确与否


## 二、为什么选择深度神经网络


![](https://res.cloudinary.com/bxy1994/image/upload/v1548722271/DL_coursera/dnn_intuition_wf29wi.jpg)

如上图人脸检测模型所示（实际上是CNN的demo），我们发现第一层网络能够对一些特定的边缘进行检测，到了第二层，网络学习到了一些高级的特征，如眼睛鼻子等，到了第三层，网络已经学习到了人脸的整体。这说明随着网络的加深，模型能够从简单的特征逐渐学习到更为抽象的高级特征，这样一来模型的精确度也就越高。


**电路逻辑理论**告诉我们，在构建N个异或（XOR）逻辑结构时，使用深度的网络，节点的个数为 $O(logN)$，而若是使用只有两层的浅层神经网络，所需的节点个数为 $O(2^N)$。同理，相比于浅层神经网络，使用深层神经网络可以显著的减少所需的节点个数

![](https://res.cloudinary.com/bxy1994/image/upload/v1548722839/DL_coursera/dnn_circuit_theory_hwnefq.jpg) 

## 三、前向与反向传播

深度神经网络的forward propagatgion 和 back propagation 和浅层神经网络并没有区别，具体公式参考上一份笔记。需要注意的是，在浅层网络中，依靠向量化操作，可以在代码中完全避免 for-loop 循环，但对于深层网络，层与层之间的传播还是需要靠 for 循环来书写。

## 四、超参数

在深度学习中，超参数永远是最需要注意的地方，在深层神经网络中，我们通常使用的超参包括：

- 学习率： $\alpha$
- 迭代次数： $N$
- 隐藏层的层数： $L$
- 每一层的节点数： $n[1],n[2],\dots$
- 每一层激活函数的选择：$g(z)^{[l]}$
