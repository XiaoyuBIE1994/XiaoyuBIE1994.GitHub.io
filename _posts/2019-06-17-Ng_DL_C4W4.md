---

layout: post
title: Deep Learning by Andrew (4-4) 人脸识别和风格迁移
category: 笔记(DeepLearning)
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>


## 1. 人脸识别



#### 1.1 验证与识别（verification and recognition）



**人脸验证：**

- 输入：人脸图片，人脸ID
- 输出：True/False，是否是同一个人
- one to oen 问题



**人脸验证：**

- 数据库：K个人的身份及照片信息
- 输入：人脸图片
- 输出：是否在数据库中，验证人的ID
- one to N 问题



人脸识别问题更加困难，系统出错的可能性更高



#### 1.2 one shot learning

对于人脸识别系统，普遍存在的问题是 one shot learning，也就是只见过一张照片（或少量照片）的情况下识别出一个人，若按照分类问题的思路直接针对每个人进行训练不足以训练出一个稳健的网络，而且当增加新的人脸数据时又需要重新训练甚至改变网络结构（增加输出个数）。所以在人脸识别系统中，我们并不是学习一个分类的神经网络，而是去学习一个计算 similarity 的网络：

- $d(img_1, img_2)$ = degree of difference between images
- $d(img_1, img_2) \le \tau$ : same person
-  $d(img_1, img_2) > \tau$ : different person



 #### 1.3 Siamese Network

可以用 Siamese Network 来实现 similarity 函数：



![][1]



如图所示，两幅人像图通过同一神经网络，最后输出一组特征向量（比如图中所示的128维向量），最后计算范数来比较两者的差异：


$$
d(x_1, x_2) = \parallel f(x_1) - f(x_2)) \parallel
$$


#### 1.4 Triplet loss

通过定义 triplet loss 可以用于训练神经网络，如下图所示：

![][2]

- Anchor (A）：目标图片
- Positive (P）：与 Anchor 属于同一人的图片
- Negative (N）：与 Anchor 不属于同一人的图片



我们希望 Anchor 与 Positive 之间的差异要小，且与 Negative 之间的差异要大，并且中间有一定的阈值来保证精度，采用 $L_2$ 范数，目标如下：


$$
\parallel f(A)- f(P)) \parallel^2 - \parallel f(A)- f(N)) \parallel^2 + \alpha \le 0
$$


损失函数定义如下：


$$
L(A, P, N) = max \lbrace \parallel f(A)- f(P)) \parallel^2 - \parallel f(A)- f(N)) \parallel^2 + \alpha, \; 0 \rbrace
$$


**Notation:**

- 在选择数据集进行训练时，若采取随机选择，很容易满足条件，所以我们需要尽量选择 $d(A,P) \approx d(A,N)$ 的样本来训练



#### 1.5 脸部验证与二分类

除了使用 triplet loss 来学习人脸识别网络外，还可以将人脸识别问题转化为一个二分类问题：



![][3]

将最后的sigmoid函数进行如下计算：


$$
\hat{y} = \sigma (\sum_{k=1}^K w_i \vert f(x^{(i)})_k - f(x^{(j)})_k \vert  + b)
$$


最后 $\hat{y} = 1$ 代表是同一个人， $\hat{y} = 0$ 代表是不同的人，训练过程相当于学习每个特征的权重，或者使用DeepFace中的 $\chi^2$ 相似度：


$$
\frac{(f(x^{(i)})_k - f(x^{(j)})_k)^2}{f(x^{(i)})_k + f(x^{(j)})_k}
$$


## 2. 神经风格迁移

#### 2.1 简介

![][4]





神经风格迁移可以理解为使用一张图片上的内容与另外一张图片上的风格，来合成一张新的图片



#### 2.2 深度神经网络的可视化

要想理解神经风格迁移，首先需要了解到深度神经网络到底学习到的是什么：

![][5]



我们希望看到每一层的计算结果，所以我们对与每一层的隐藏单元，遍历训练集找到最大化激活该单元的图片或者图片块，得到如下结果：

![][6]

可以发现，在较浅的层，网络学习到的是颜色或者物体的边，随着层数的增加，网络会学习到更加复杂的形状或者模式，也就是较前的网络学习的是简单的特征，较后的网络学习的是更抽象的特征



### 2.3 代价函数



为了实现风格迁移，我们需要给网络定义一个代价函数：


$$
J(G) = \alpha J_{content} (C, G) + \beta J_{style}(S,G)
$$


其中第一项代表的是内容的相似程度，第二项代表的是风格的相似程度，图像的生成过程如下：

- 随机初始化一张图片
- 基于代价函数 $J(G)$ 进行梯度下降



![][7]



**内容代价函数：**

使用卷积网络的中间层作为内容代价函数的输入，具体步骤如下：

- 使用 pre-trained 的卷积网络（如 VGG19）
- 选取中间层的输出 $a^{\lbrack l \rbrack (C)}$ 和 $a^{\lbrack l \rbrack (G)}$
- 计算两者的范数差


$$
J_{content} (C,G) = \frac{1}{4 \times n_H \times n_W \times n_C} \sum_{entries} (a(C) - a(G))^2
$$


**风格代价函数：**

同样是选择 pre-trained 的卷积网络并使用中间层 $a^{\lbrack l \rbrack (C)}$ 和 $a^{\lbrack l \rbrack (G)}$ 的输出，但计算方式略有不同：



![][8]

![][9]



首先，我们对输出进行展开，得到 $(n_H \times n_W, n_C)$ 的矩阵 $(v_1,v_2,\dots,v_n)$，接着计算矩阵的 correlation，也就是不同管道之间的相关度：


$$
G_{ij} = v_i^T v_j
$$


两张图片在某些卷积层的输出的管道之间的相关度的差定义为 style loss：


$$
J_{style}^{[l]}(S,G) = \frac{1}{4 \times {n_C}^2 \times (n_H \times n_W)^2} \sum _{i=1}^{n_C}\sum_{j=1}^{n_C}(G^{(S)}_{ij} - G^{(G)}_{ij})^2 
$$

$$
J_{style} = \sum_{\lambda} \lambda^{[l]}J_{style}^{[l]}(S,G)
$$


## 3. 1D 和 3D 卷积

在之前的网络中，我们都是对2D的数据进行卷积，但实际上，我们也可以用类似的方法对1D或3D数据进行卷积操作，如下图所示：

![][10]



![][11]



## 4. 代码实现

以下代码参考自吴恩达的Deep Learning课程作业，并不是作业的答案，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 



#### 4.1 人脸识别


```python
# Triplet loss
def triplet_loss(y_true, y_pred, alpha = 0.2):
    anchor, positive, negative = y_pred[0], y_pred[1], y_pred[2]
    # Step 1: Compute the (encoding) distance between the anchor and the positive, sum over axis=-1
    pos_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, positive)), axis=-1)
    # Step 2: Compute the (encoding) distance between the anchor and the negative, sum over axis=-1
    neg_dist = tf.reduce_sum(tf.square(tf.subtract(anchor, negative)), axis=-1)
    # Step 3: subtract the two previous distances and add alpha.
    basic_loss = tf.add(tf.subtract(pos_dist, neg_dist), alpha)
    # Step 4: Take the maximum of basic_loss and 0.0. Sum over the training examples.
    loss = tf.reduce_sum(tf.maximum(basic_loss, 0))
    
    return loss
```




#### 4.2 神经风格迁移



```python
# 计算 content cost
def compute_content_cost(a_C, a_G):
    # Retrieve dimensions from a_G
    m, n_H, n_W, n_C = a_G.get_shape().as_list()
    # Reshape a_C and a_G
    a_C_unrolled = tf.transpose(tf.reshape(a_C, [n_H*n_W, n_C]))
    a_G_unrolled = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))
    # compute the cost with tensorflow 
    J_content = 1./(4 * n_H * n_W * n_C)*tf.reduce_sum(tf.square(tf.subtract(a_C_unrolled, a_G_unrolled)))
    return J_content
```


```python
# 计算 style cost


def gram_matrix(A):
    GA = tf.matmul(a=A, b=A, transpose_b=True)
    return GA
  
def compute_layer_style_cost(a_S, a_G):
    # Retrieve dimensions from a_G
    m, n_H, n_W, n_C = a_G.get_shape().as_list()
    # Reshape the images to have them of shape (n_C, n_H*n_W) (≈2 lines)
    a_S = tf.transpose(tf.reshape(a_S, [n_H*n_W, n_C]))
    a_G = tf.transpose(tf.reshape(a_G, [n_H*n_W, n_C]))
    # Computing gram_matrices for both images S and G (≈2 lines)
    GS = gram_matrix(a_S)
    GG = gram_matrix(a_G)
    # Computing the loss (≈1 line)
    J_style_layer = 1/(4*n_C*n_C*n_H*n_H*n_W*n_W) * tf.reduce_sum(tf.square(tf.subtract(GS, GG)))
    return J_style_layer
  
def compute_style_cost(model, STYLE_LAYERS):
    """
    Arguments:
    model -- our tensorflow model
    STYLE_LAYERS -- A python list, 
    	 e.g.: STYLE_LAYERS = [('conv1_1', 0.2),('conv2_1', 0.2),('conv3_1', 0.2)]
    Returns: 
    J_style
    """
    # initialize the overall style cost
    J_style = 0
    for layer_name, coeff in STYLE_LAYERS:
        # Select the output tensor of the currently selected layer
        out = model[layer_name]
        # Set a_S to be the hidden layer activation from the layer we have selected, by running the session on out
        a_S = sess.run(out)
        a_G = out
        # Compute style_cost for the current layer
        J_style_layer = compute_layer_style_cost(a_S, a_G)
        # Add coeff * J_style_layer of this layer to overall style cost
        J_style += coeff * J_style_layer

    return J_style
```


```python
# Total cost
def total_cost(J_content, J_style, alpha = 10, beta = 40):
    J = alpha * J_content + beta * J_style
    return J
```





[1]:https://res.cloudinary.com/bxy1994/image/upload/v1560764063/DL_coursera/siamese_nn.png
[2]:https://res.cloudinary.com/bxy1994/image/upload/v1560769397/DL_coursera/FaceRecognition_TripletLoss.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1560769861/DL_coursera/FaceRecognition_BinaryClassification.png
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1560771253/DL_coursera/StyleTransfer_example.jpg
[5]:https://res.cloudinary.com/bxy1994/image/upload/v1560771442/DL_coursera/NN_insight.jpg
[6]: https://res.cloudinary.com/bxy1994/image/upload/v1560771443/DL_coursera/NN_visualizing.jpg
[7]: https://res.cloudinary.com/bxy1994/image/upload/v1560771939/DL_coursera/StyleTransfer_process.jpg
[8]:https://res.cloudinary.com/bxy1994/image/upload/v1560772400/DL_coursera/CNN_flatten.png
[9]: https://res.cloudinary.com/bxy1994/image/upload/v1560772400/DL_coursera/CNN_style_cost.png
[10]:https://res.cloudinary.com/bxy1994/image/upload/v1560772998/DL_coursera/1dConv.jpg
[11]:https://res.cloudinary.com/bxy1994/image/upload/v1560772998/DL_coursera/3dConv.jpg