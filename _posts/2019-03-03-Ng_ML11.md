---
layout: post
title: Machine Learning by Andrew (十一) 大数据集的学习 (SGD and MapReduce)
category: Coursera ML 笔记
tags: 
  - machine learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



在机器学习的训练中，如果数据集的规模较大，为了加快学习的速度，通常会引入一些加速技巧，这篇笔记中的SGD和MapReduce就是其中的两例



## 一、SGD (Stochastic Gradient Descent)

批量梯度下降（Batch Gradient Desctent）的核心算法为：


$$
\theta_j := \theta_j - \alpha \frac{1}{m} \sum_{i=1}^{m} (h_{\theta}(x^{(i)})- j^{(i)})x_j^{(i)}
$$


而对于SGD，则是首先讲数据集随机打乱，然后对参数进行依次迭代：


$$
\theta_j := \theta_j - \alpha (h_{\theta}(x^{(i)})- j^{(i)})x_j^{(i)} \quad \text{for i = 1, ..., m}
$$


由于SGD中并没有全局的考虑数据，虽然加快了计算速度，但由于下降的方向可能会产生较大的偏离，反而降低了收敛的速度，在这种情况下，可以采用一种折中的方法，也就是 Mini-batch gradient descent， 这一方法和SGD一样，也会在最开始打乱数据，但不一样的是，SGD对每一份数据都进行梯度下降，BGD是针对所有的数据计算梯队下降，而Mini-BGD则是取一 batch size =  b，然后每b组数据进行梯度下降



## 二、 MapReduce

这一技术实际上是平行计算，将大规模的矩阵运算拆分成若干个来分别计算，然后通过主机通信来汇总结果，和多核计算机的多线程计算类似，大体原理如下：



![](https://res.cloudinary.com/bxy1994/image/upload/v1551653354/ML_coursera/MapReduce.png)