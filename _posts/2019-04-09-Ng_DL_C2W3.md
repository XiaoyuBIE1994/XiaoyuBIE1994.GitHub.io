---
layout: post
title: Deep Learning by Andrew (2-3) 超参调试，BatchNorm, Softmax
category: 笔记(DeepLearning)
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>


## 1. 超参处理

在之前的介绍中，我们会发现深度神经网络的训练中存在着许多的超参，比如：



- 学习率 $\alpha $ （最重要）
- 隐藏层中节点的数量（重要）
- 网络的层数
- Mini-batch 的大小（重要）
- Moment 中的平滑指数 $\beta$（重要）
- 学习率衰减
- Adam中的 $\beta_1, \beta_2, \epsilon$ 



在参数的调试中，常用的方法是在参数空间中进行随机采样，然后计算最佳值，而不是进行均匀采样，如下图所示：



![][1]



当随机选到比较好的参数时，再缩小范围进行更加精细的选取。当然，超参数的采样并不是在同样的尺度也不在均匀的尺度上。这是因为对于较为重要的超参比如学习率 $\alpha$，指数上的搜索会效率更高，或者对于带有动量的优化中的平滑指数 $\beta$ ，平均的样本数等于 $1/(1-\beta)$，所以算法在 $\beta$ 接近1时会非常的敏感，这就要求我们在靠近1的范围内进行更多的采样。故而，我们可以采取以下策略：



```python
learning_rate = 10 ** (-3*np.random.rand()) # 学习率在0.001到1之间指数采样
beta = 1 - 10**(-3*np.random.rand()) # 平滑指数在0.9到0.999之间采样，接近1的范围内采样频率更高
```



在实际的训练过程中，由于计算资的限制，很多时候没有办法并行的计算多个模型来比较优劣（下图右侧，像鱼产子一样一次选取很多样本，最后再进行挑选），这时我可以采取 babysitting 的策略，也即是每间隔一段时间来观察学习曲线的走势，来对参数进行微调（下图左侧，有如养熊猫，一步一步小心调整）



![][2]





## 2. 批量归一化（Batch Normalization）



批量归一化 的算法由Sergey Ioffe 和 Christian Szegedy提出，这一算法可以使超参搜索变得简单，增加神经网络的鲁棒性，让神经网络对于超参数的选择不再敏感，BatchNorm的算法流程如下：


$$
\begin{align}
& \mu = \frac{1}{m} \Sigma_i z^{(i)} \\
& \sigma^2 = \frac{1}{m} \Sigma_i ( z^{(i)} -\mu)^2 \\
& z^{(i)}_{norm}= \frac{z^{(i)} - \mu}{\sqrt{\sigma^2 + \epsilon}} \\
& \tilde{z}^{(i)} = \gamma z^{(i)}_{norm} + \beta
\end{align}
$$


这里的 $z^{(1)} z^{(2)},\dots ,z^{(l)}$ 是某一层上所有训练数据（也就是$z^{[l](1)}, z^{[l](2)},\dots ,z^{[l](l)}$  BatchNorm针对 $z^{(i)}$ 还是 $a^{(i)}$ 在学术上还存在争议，不过通常选择前者），$\gamma, \beta$ 是对归一化后的输出进行再一次的缩放，这两个值作为参数放入模型中进行学习。BatchNorm 之所以有用，主要有着如下几个原因：

- 当输入空间和目标空间的分布不一致时（Internal Covariate Shift， 比如识别猫时，改变了毛发的颜色，猫的坐姿，猫的大小等等），不同的变化会导致决策边界存在着一些线性的映射，当放在一起进行学习时，学习到的决策边界可能在两者之间来回振荡，进行了批归一化后，这一影响将会减弱
- 由于 mini-batch 中每一组数据的 mean/variance 都存在着一定的偏移，所以对模型进行 BatchNorm 时会带来一定的噪声，某种程度上给模型增加了正则化的效果（不是正则化的手段，仍然视为加速训练的手段）
- 可以抑制梯度消失和梯度爆炸（课中没有提到）



**Notation**

在模型训练时，可以在每个 mini-batch 中计算 $\mu, \sigma^2$，但进入预测阶段时，每一组预测数据是单独放入模型进行预测的，这时就没有办法再来计算均值和方差了。所以，我们选择在训练的时候，运用之前介绍的指数加权平均的方法，对均值和方差进行记录和平均，进入预测阶段时便可以使用最后得到的$\mu, \sigma^2$ 了



## 3. Softmax

softmax是针对多分类的问题。对于K个分类，模型会存在K个输出，通常认为输出的值越大，则预测概率越大，区分于SVM中设置边界的做法，softmax采用归一化指数形式来表示概率：


$$
\hat{y}^{(i)} = \frac{e^{y(i)}}{\sum_{j=1}^K e^{y(j)}}
$$
当 K 等于2时便是逻辑回归



## 4. 代码实现

以下代码使用tensorflow来实现简单的神经网络，所用网络结构参考自吴恩达的Deep Learning课程作业，并不是作业的答案，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 

用TensorFlow来训练模型，大体步骤如下：

- Create a graph containing Tensors (Variables, Placeholders ...) and Operations (tf.matmul, tf.add, ...)
- Create a session
- Initialize the session
- Run the session to execute the graph



```python
import math
import numpy as np
import h5py
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.python.framework import ops

# 建立X，Y的占位符
def create_placeholders(n_x, n_y):
    X = tf.placeholder(tf.float32, shape = [n_x, None])
    Y = tf.placeholder(tf.float32, shape = [n_y, None])
    return X, Y

# 参数初始化，具体大小取决于网络结构
def initialize_parameters():
    W1 = tf.get_variable("W1", [25, 12288], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
    b1 = tf.get_variable("b1", [25, 1], initializer = tf.zeros_initializer())
    W2 = tf.get_variable("W2", [12, 25], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
    b2 = tf.get_variable("b2", [12, 1], initializer = tf.zeros_initializer())
    W3 = tf.get_variable("W3", [6, 12], initializer = tf.contrib.layers.xavier_initializer(seed = 1))
    b3 = tf.get_variable("b3", [6, 1], initializer = tf.zeros_initializer())
    parameters = {"W1": W1, "b1": b1, "W2": W2, "b2": b2, "W3": W3, "b3": b3}
    return parameters

# 前向传播
def forward_propagation(X, parameters):
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    W3 = parameters['W3']
    b3 = parameters['b3']
    
    Z1 = tf.add(tf.matmul(W1, X), b1)            # Z1 = np.dot(W1, X) + b1
    A1 = tf.nn.relu(Z1)                          # A1 = relu(Z1)
    Z2 = tf.add(tf.matmul(W2, A1), b2)           # Z2 = np.dot(W2, a1) + b2
    A2 = tf.nn.relu(Z2)                          # A2 = relu(Z2)
    Z3 = tf.add(tf.matmul(W3, A2), b3)           # Z3 = np.dot(W3,Z2) + b3
    return Z3

# 定义loss函数
def compute_cost(Z3, Y):
    logits = tf.transpose(Z3)
    labels = tf.transpose(Y)
    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels = labels))
    return cost

# 反向传播
def model(X_train, Y_train, X_test, Y_test, learning_rate = 0.0001,
          num_epochs = 1500, minibatch_size = 32, print_cost = True):
    ops.reset_default_graph()  # 重新初始化 
    (n_x, m) = X_train.shape 
    n_y = Y_train.shape[0]   
    costs = []           
    
    X, Y = create_placeholders(n_x, n_y)
    parameters = initialize_parameters()
    Z3 = forward_propagation(X, parameters)
    cost = compute_cost(Z3, Y)
    optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)
    init = tf.global_variables_initializer()

    # Start the session to compute the tensorflow graph
    with tf.Session() as sess:
        sess.run(init)
        for epoch in range(num_epochs):
            epoch_cost = 0.       
            num_minibatches = int(m / minibatch_size)
            minibatches = random_mini_batches(X_train, Y_train, minibatch_size)
            for minibatch in minibatches:
                (minibatch_X, minibatch_Y) = minibatch
                _ , minibatch_cost = sess.run([optimizer, cost], feed_dict = {X: minibatch_X, Y : minibatch_Y})
                epoch_cost += minibatch_cost / num_minibatches

            # Print the cost every epoch
            if print_cost == True and epoch % 100 == 0:
                print ("Cost after epoch %i: %f" % (epoch, epoch_cost))
            if print_cost == True and epoch % 5 == 0:
                costs.append(epoch_cost)
                
        # plot the cost
        plt.plot(np.squeeze(costs))
        plt.ylabel('cost')
        plt.xlabel('iterations (per tens)')
        plt.title("Learning rate =" + str(learning_rate))
        plt.show()

        # lets save the parameters in a variable
        parameters = sess.run(parameters)
        print ("Parameters have been trained!")

        # Calculate the correct predictions
        correct_prediction = tf.equal(tf.argmax(Z3), tf.argmax(Y))

        # Calculate accuracy on the test set
        accuracy = tf.reduce_mean(tf.cast(correct_prediction, "float"))

        print ("Train Accuracy:", accuracy.eval({X: X_train, Y: Y_train}))
        print ("Test Accuracy:", accuracy.eval({X: X_test, Y: Y_test}))
        
        return parameters
```



[1]: <https://res.cloudinary.com/bxy1994/image/upload/v1554799887/DL_coursera/HyParams_random.jpg>
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1554802318/DL_coursera/HyParams_practice.jpg