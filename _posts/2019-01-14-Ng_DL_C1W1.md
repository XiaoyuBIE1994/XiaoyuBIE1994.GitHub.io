---
layout: post
title: Deep Learning by Andrew (一) 浅层神经网络
category: 笔记
tags: 
  - deep learning
description: 
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、 神经网络的表现


![](https://res.cloudinary.com/bxy1994/image/upload/v1548603214/nn_representation_xnozmv.png) 

神经网络的基本结构如上所示，需要注意的是，在深度学习的课程中，网络的层级用上标方括号来表示，$W^{[l]}$ 表示第 $l$ 层之前的参数，且输入层为第0层，和机器学习课程中有所区别。同时，逻辑回归和神经网络中的参数表达会不一样：

> 逻辑回归，参数矩阵大小为 $(n_{in}, n_{out})$，计算公式为 $y = W^Tx + b$
> 神经网络，参数矩阵大小为 $(n_{out}, n_{in})$，计算公式为 $y = Wx + b$

## 二、神经网络的输出

![](https://res.cloudinary.com/bxy1994/image/upload/v1548604517/nn_output_i9gbcd.jpg) 

神经网络的输出计算基本上遵循`线性计算 -> 激活函数 -> 线性计算 -> 激活函数 ...`，Python代码可以很轻易的实现如上计算

## 三、向量化实现

![](https://res.cloudinary.com/bxy1994/image/upload/v1548605006/nn_vec_pt9bo3.jpg) 

在m个训练样本中，每次的计算过程都相同，所以我们可以利用向量化来降低计算的耗时，将样本合并到一个矩阵之中，大小为 $(x_n, m)$，其中 $x_n$ 是输入层的神经元个数， $m$ 是训练样本的个数，同时Python的广播机制可以很好对每一组样本加上偏置项 $b$

## 四、激活函数

![](https://res.cloudinary.com/bxy1994/image/upload/v1548606156/nn_activation_k6ofis.jpg) 

常用的四个激活函数如下：

- **sigmoid函数** $a = \frac{1}{1+e^{-z}}$ 一般用于二分类问题，即网络的输出范围为0到1
- **tanh函数** $a = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$ 函数的输出在-1到1之间，数学上可以看做一个位移后的sigmoid函数，但训练效果常常要好于sigmoid，这是因为训练数据常常经过了中心化，而tanh函数会让隐藏层中的输出更加逼近0，而不是0.5。但是和Sigmoid一样，在 $|z|$ 很大时导数会变得非常小，这样在参数更新的后期，训练速度会变得非常慢
- **Relu函数** $a = max \{ 0， z\}$ 实践中通常运用的激活函数，唯一的缺点是z小于0时导数为0（实际问题中通常不是问题）。学习效率要高于sigmoid函数和tanh函数
- **Leaky Relu函数** $a = max \{ 0.01z， z\}$ Relu函数的变形，效果通常要好于Relu，但实际应用不多

PS：在实际应用中，很难说哪种激活函数会取得更好的效果，通常我们可以依次去实验，并用交叉验证集或开发验证集去验证效果


## 五、梯度下降

- **sigmoid函数** 

$$
g(z) = \frac{1}{1+e^{-z}} \\
\frac{d}{dz} g(z) = g(z) (1-g(z))
$$

 - **tanh函数** 
 
$$
g(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\
\frac{d}{dz} g(z) = 1 - g(z)^2
$$ 

- **Relu函数** 

$$
g(z) = max \{ 0， z\} \\
\frac{d}{dz} g(z) = \begin{cases} 1; \; z \ge 0 \\ 0; \; z <0 \end{cases}
$$

- **Leaky Relu函数** 

$$
g(z) = max \{0.01z， z\} \\
\frac{d}{dz} g(z) = \begin{cases} 1; \; z \ge 0 \\ 0.01; \; z <0 \end{cases}
$$


以sigmoid函数为例，下图所示为一个二层神经网络的反向传播算法的推导，左侧为公式，右侧为代码实现，具体的公式推导可参见[吴恩达机器学习笔记（四）神经网络](http://www.biexiaoyu1994.com/%E7%AC%94%E8%AE%B0/2019/01/04/Ng_ML4/) 

![](https://res.cloudinary.com/bxy1994/image/upload/v1548622922/nn_bp_vchzai.jpg) 


## 六、随机初始化

由于神经网络的特殊属性，若两个隐藏神经元的参数设置为相同大小，那么两个隐藏神经元对输出单元的影响是相同的，那么通过反向传播来计算梯度时，得到的梯度大小也是一样的，那么经过了多次迭代后参数也会完全相同，那么多个隐藏的神经元也就失去了意义，而偏置项则不存在这个问题。通常，我们会对初始的参数进行随机化：

```python
W = np.random.rand((n_out, n_int) * 0.01
b = np.zeros((n_out, 1))
```

乘以0.01是为了尽可能使得权重初始化较小的值，这是因为在使用sigmoid或者tanh时，W较小则Z较小，导数较大，可以提高算法的更新速度，但Relu和Leaky Relu不存在。