---

layout: post
title: Deep Learning by Andrew (一) 浅层神经网络
category: 笔记
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>




## 1.  神经网络的表现


![](https://res.cloudinary.com/bxy1994/image/upload/v1548603214/DL_coursera/nn_representation_xnozmv.png) 

神经网络的基本结构如上所示，需要注意的是，在深度学习的课程中，网络的层级用上标方括号来表示，$W^{[l]}$ 表示第 $l​$ 层之前的参数，且输入层为第0层，和机器学习课程中有所区别。同时，逻辑回归和神经网络中的参数表达会不一样：

> 逻辑回归，参数矩阵大小为 $(n_{in}, n_{out})$，计算公式为 $y = W^Tx + b$
> 神经网络，参数矩阵大小为 $(n_{out}, n_{in})$，计算公式为 $y = Wx + b$



## 2. 神经网络的输出

![](https://res.cloudinary.com/bxy1994/image/upload/v1548604517/DL_coursera/nn_output_i9gbcd.jpg) 

神经网络的输出计算基本上遵循`线性计算 -> 激活函数 -> 线性计算 -> 激活函数 ...`，Python代码可以很轻易的实现如上计算



## 3. 向量化实现

![](https://res.cloudinary.com/bxy1994/image/upload/v1548605006/DL_coursera/nn_vec_pt9bo3.jpg) 

在m个训练样本中，每次的计算过程都相同，所以我们可以利用向量化来降低计算的耗时，将样本合并到一个矩阵之中，大小为 $(x_n, m)$，其中 $x_n$ 是输入层的神经元个数， $m$ 是训练样本的个数，同时Python的广播机制可以很好对每一组样本加上偏置项 $b$

## 4. 激活函数

![](https://res.cloudinary.com/bxy1994/image/upload/v1548606156/DL_coursera/nn_activation_k6ofis.jpg) 

常用的四个激活函数如下：

- **sigmoid函数** $a = \frac{1}{1+e^{-z}}​$ 一般用于二分类问题，即网络的输出范围为0到1  
- **tanh函数** $a = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}}$ 函数的输出在-1到1之间，数学上可以看做一个位移后的sigmoid函数，但训练效果常常要好于sigmoid，这是因为训练数据常常经过了中心化，而tanh函数会让隐藏层中的输出更加逼近0，而不是0.5。但是和Sigmoid一样，在 $\vert z \vert$ 很大时导数会变得非常小，这样在参数更新的后期，训练速度会变得非常慢  
- **Relu函数** $a = max \lbrace 0， z \rbrace​$ 实践中通常运用的激活函数，唯一的缺点是z小于0时导数为0（实际问题中通常不是问题）。学习效率要高于sigmoid函数和tanh函数  
- **Leaky Relu函数** $a = max \lbrace 0.01z， z \rbrace$ Relu函数的变形，效果通常要好于Relu，但实际应用不多  

PS：在实际应用中，很难说哪种激活函数会取得更好的效果，通常我们可以依次去实验，并用交叉验证集或开发验证集去验证效果




## 5. 梯度下降

- **sigmoid函数** 

$$
g(z) = \frac{1}{1+e^{-z}} \\
\frac{d}{dz} g(z) = g(z) (1-g(z))
$$

 - **tanh函数** 

$$
g(z) = \frac{e^{z} - e^{-z}}{e^{z} + e^{-z}} \\
\frac{d}{dz} g(z) = 1 - g(z)^2
$$

- **Relu函数** 

$$
g(z) = max \{ 0， z\} \\
\frac{d}{dz} g(z) = \begin{cases} 1; \; z \ge 0 \\ 0; \; z <0 \end{cases}
$$

- **Leaky Relu函数** 

$$
g(z) = max \{0.01z， z\} \\
\frac{d}{dz} g(z) = \begin{cases} 1; \; z \ge 0 \\ 0.01; \; z <0 \end{cases}
$$


以sigmoid函数为例，下图所示为一个二层神经网络的反向传播算法的推导，左侧为公式，右侧为代码实现，具体的公式推导可参见[吴恩达机器学习笔记（四）神经网络](http://www.biexiaoyu1994.com/%E7%AC%94%E8%AE%B0/2019/01/04/Ng_ML4/) 

![](https://res.cloudinary.com/bxy1994/image/upload/v1548622922/DL_coursera/nn_bp_vchzai.jpg) 


## 6. 随机初始化

由于神经网络的特殊属性，若两个隐藏神经元的参数设置为相同大小，那么两个隐藏神经元对输出单元的影响是相同的，那么通过反向传播来计算梯度时，得到的梯度大小也是一样的，那么经过了多次迭代后参数也会完全相同，那么多个隐藏的神经元也就失去了意义，而偏置项则不存在这个问题。通常，我们会对初始的参数进行随机化：

```python
W = np.random.rand((n_out, n_int) * 0.01
b = np.zeros((n_out, 1))
```

乘以0.01是为了尽可能使得权重初始化较小的值，这是因为在使用sigmoid或者tanh时，W较小则Z较小，导数较大，可以提高算法的更新速度，但Relu和Leaky Relu不存在。

## 7. 代码实现

以下代码为下图所示的两层神经网络的实现，所用网络结构参考自吴恩达的Deep Learning课程作业，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 

![](https://res.cloudinary.com/bxy1994/image/upload/v1548721119/DL_coursera/nn_model_ieej3i.png) 

```python
# import dependencies
import numpy as np
```

```python
# Function: find layer size
"""
X -- input dataset of shape (input size, number of examples)
Y -- labels of shape (output size, number of examples)
n_x -- the size of the input layer
n_h -- the size of the hidden layer
n_y -- the size of the output layer
"""

def layer_sizes(X, Y): 

    n_x = X.shape[0] # size of input layer
    n_h = 4
    n_y = Y.shape[0] # size of output layer
    
    return (n_x, n_h, n_y)
```

```python
# Function: iniitialize parameters (random for w and zero for b)
"""
n_x -- size of the input layer
n_h -- size of the hidden layer
n_y -- size of the output layer
params -- python dictionary containing your parameters:
            W1 -- weight matrix of shape (n_h, n_x)
            b1 -- bias vector of shape (n_h, 1)
            W2 -- weight matrix of shape (n_y, n_h)
            b2 -- bias vector of shape (n_y, 1)
"""

def initialize_parameters(n_x, n_h, n_y):

    W1 = np.random.randn(n_h, n_x) * 0.01
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))

    
    assert (W1.shape == (n_h, n_x))
    assert (b1.shape == (n_h, 1))
    assert (W2.shape == (n_y, n_h))
    assert (b2.shape == (n_y, 1))
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
```

```python
# Function: forward propagation
"""
X -- input data of size (n_x, m)
parameters -- neuro network parameters (output of initialization function)
A2 -- The second activation
cache -- a dictionary containing "Z1", "A1", "Z2" and "A2", used for back propagation
"""

def forward_propagation(X, parameters):

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    # Forward Propagation 
    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)
    
    assert(A2.shape == (1, X.shape[1]))
    
    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}
    
    return A2, cache
```

```python
# Function: compute cost
"""
A2 -- The sigmoid output of the second activation, of shape (1, number of examples)
Y -- "true" labels vector of shape (1, number of examples)
parameters -- python dictionary containing your parameters W1, b1, W2 and b2
cost -- cross-entropy cost given equation (13)
"""

def compute_cost(A2, Y, parameters):
    
    m = Y.shape[1] # number of example

    # Compute the cross-entropy cost
    cost = - (np.dot(Y, np.log(A2).T) + np.dot((1-Y), np.log(1 - A2).T)) / m
    cost = float(np.squeeze(cost))     # keep dimension
    
    assert(isinstance(cost, float))
    
    return cost
```

```python
# backward propagation
"""
parameters -- python dictionary containing our parameters 
cache -- a dictionary containing "Z1", "A1", "Z2" and "A2".
X -- input data of shape (2, number of examples)
Y -- "true" labels vector of shape (1, number of examples)
grads -- python dictionary containing gradients with respect to different parameters
"""

def backward_propagation(parameters, cache, X, Y):

    m = X.shape[1]
    W1 = parameters['W1']
    W2 = parameters['W2']    
    A1 = cache['A1']
    A2 = cache['A2']

    dZ2 = A2 - Y
    dW2 = 1 / m * np.dot(dZ2, A1.T)
    db2 = 1 / m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(W2.T, dZ2) * (1 - A1 * A1)
    dW1 = 1 / m * np.dot(dZ1, X.T)
    db1 = 1 / m * np.sum(dZ1, axis=1, keepdims=True)
    
    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}
    
    return grads
```

```python
# Function: params update
"""
parameters -- python dictionary containing your parameters 
grads -- python dictionary containing your gradients 
parameters -- python dictionary containing your updated parameters 
"""

def update_parameters(parameters, grads, learning_rate = 1.2):

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2
    
    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}
    
    return parameters
```

```python
# Function: model integraion
"""
X -- dataset of shape (2, number of examples)
Y -- labels of shape (1, number of examples)
n_h -- size of the hidden layer
num_iterations -- Number of iterations in gradient descent loop
print_cost -- if True, print the cost every 1000 iterations
parameters -- parameters learnt by the model. They can then be used to predict.
"""

def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):

    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]
    
    # Initialize parameters
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
    
    # Loop (gradient descent)

    for i in range(0, num_iterations):
        A2, cache = forward_propagation(X, parameters)
        cost = compute_cost(A2, Y, parameters)
        grads = backward_propagation(parameters, cache, X, Y)
        parameters = update_parameters(parameters, grads)
        
        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" %(i, cost))

    return parameters
```



