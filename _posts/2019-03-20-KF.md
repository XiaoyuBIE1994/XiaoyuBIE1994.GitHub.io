---
layout: post
title: 基于滤波的状态估计（一）线性高斯系统与卡尔曼滤波（Kalman Filter, KF）
category: 知识整理
tags: 
  - SLAM
---

## 1. 问题描述

在SLAM问题中，我们通常假设机器人携带着传感器在一定的未知环境中运动，同时进行着定位与建图的任务。定位可以帮助机器人识别到自己所处的位置，进而对后续任务的执行起到引导的作用，而建图中机器人将记录下一系列的路标，可以作为观测（obsevation）辅助定位，也可以用于减少漂移（loop closure）或是让机器人在下一次执行任务时得到同样坐标系下的坐标（relocalization）。传感器通常是在一定的时间节点进行数据采集，我们也只关心这些时刻的状态和地图，所以SLAM的问题通常简化为离散时刻的任务 $t = 1, \dots ,K$， 用 $x$ 表示机器人的位置状态，于是各个时刻的位置标记为 $x_1, \dots, x_K$， 这些位置组成了机器人的轨迹。同时，我们假设地图是由一个个路标（landmark）组成，每个时刻，传感器都能够观测到一部分路标，得到观测数据，设每个时刻观测到的数据为 $y_1, \dots y_K$。实际上，SLAM中的定位是通过**运动**和**观测**来来描述的，如果这两者发生在线性高斯系统（Linear-Gaussian, LG）下，则我们可以建立如下的方程：


$$
\begin{align}
\text{Motion Model:} \quad & x_k = A_{k-1} x_{k-1} + v_k + w_k, \quad k = 1, \dots, K \\
\text{Observation Model: } \quad & y_k = C_k x_k + n_k, \quad k = 0, \dots, K
\end{align}
$$


其中 $A_{k-1}​$ 成为转移矩阵（transition matrix），代表状态从 $k-1​$ 时刻到 $k​$ 时刻的变化，$v_k​$ 是系统的输入，$w_k​$ 和 $n_k​$ 分别是运动模型和观测模型的噪声，各个变量的性质如下：


$$
\begin{align}
\text{system state:} \quad & x_k \in \mathbb{R}^N \\
\text{initial state:} \quad & x_0  \in \mathbb{R}^N \sim \mathcal{N}(\check{x}_0, \check{P}_0) \\
\text{input: } \quad & v_k  \in \mathbb{R}^N  \\
\text{process noise: } \quad & w_k  \in \mathbb{R}^N \sim \mathcal{N}(0, Q_k) \\
\text{measurement: } \quad & y_k  \in \mathbb{R}^N \\
\text{measurement noise: } \quad & n_k  \in \mathbb{R}^N \sim \mathcal{N}(0, R_k)
\end{align}
$$

若在求解系统某一时刻的状态 $k$ 时，使用了系统的全部时刻的数据（$v_1, \dots, v_K, y_1, \dots , y_K$），则称为离散时间的批量估计，这里暂不展开。批量估计综合了系统中所有时刻的数据，显然更为准确，但对于时刻 $k$ 来说，由于使用了未来的数据；所以批量估计的方法是无法做到实时的。更为常见的是，我们通过时刻 $1$ 到时刻 $k-1$ 的状态和时刻 $1$ 到时刻 $k$ 的观测来估计时刻 $k$ 的状态，称为离散时间的滤波算法。滤波算法没有利用未来的信息，所以可以做到实时估计，但由于每一步都用到了前面所有的信息，所以会导致效率低下，若我们假设系统具有一阶马尔科夫性质，即 $k$ 时刻的状态只取决于 $k-1$ 时刻的状态和 $k$ 时刻的输入，且 $k$ 时刻的观测仅取决于 $k$ 时刻的状态。在这一假设下，我们可以利用 $k-1$ 时刻的估计以及 $k$ 时刻的输出和观测来估计 $k$ 时刻的状态，这就是离散时间的迭代滤波，也叫做卡尔曼滤波（Kalman Filter），如下图所示：

![][1]





问题的求解通常有着两种思路：

1. 最大后验估计（Maximum a posteriori, MAP）：通过优化的理论，来寻找给定信息下的最大后验概率，从而求得极值处系统的状态
2. 贝叶斯推断（Bayesian inference）：从状态的先验概率密度函数出发，通过初始状态、输入、运动方程和观测推导出后验概率密度函数，从函数的形式上求得系统的状态

无论是哪种方法，我们都需要求解时刻 $k$ 的后验概率分布，如下所示：



![][2]

这里 $\lbrace  \hat{x}_{k-1}, \hat{P}_{k-1} \rbrace$ 代表了 $k-1$ 时刻的状态估计，$\lbrace  \hat{x}’_{k-1}, \hat{P}’_{k-1} \rbrace$ 代表用 $0$ 到 $k$ 时刻的数据计算 $k-1$ 时刻的状态估计，其中 $x$ 是状态， $P$ 是对于这个状态估计的偏差。Kalman Filter 属于迭代滤波，目的是从 $k-1$ 时刻的状态估计推导出 $k$ 时刻的状态估计：


$$
\lbrace  \hat{x}_{k-1}, \hat{P}_{k-1} \rbrace \to \lbrace  \hat{x}_{k}, \hat{P}_{k} \rbrace
$$



**Notation**

- 最大后验估计关注的是后验概率极值点，而贝叶斯推断关注的时后验概率的期望值点
- 在线性高斯系统中，这两者是一致的




## 2. 从最大后验估计推导卡尔曼滤波



在推导卡尔曼滤波之前，我们先用贝叶斯法则来重写 $x_k​$ 的后验概率表达式：


$$
\begin{align}
p(x_k \mid \hat{x}_{k-1}, v_k, y_k) & = \eta p (y_k \mid x_k, \hat{x}_{k-1}, v_k) p(x_k \mid \hat{x}_{k-1}, v_k) \\
& = \eta  p (y_k \mid x_k) p(x_k \mid {x}'_{k-1}, v_k) p({x}'_{k-1} \mid \hat{x}_{k-1})
\end{align}
$$


这里的 $\eta$ 是归一化常数， $x'_{k-1}$ 是指 $0$ 至 $k$ 时刻数据对 $k-1$ 时刻状态的估计，这里的重写实际上用到了马尔科夫性质（Markov property），在LG系统中，这一猜想认为是满足的。MAP是求其后验概率的极值，为了简化运算，我们将后验概率写成对数形式：


$$
ln p(x_k \mid \hat{x}_{k-1}, v_k, y_k) = ln ( p (y_k \mid x_k)) + ln( p(x_k \mid x'_{k-1}, v_k)) + ln ( p(x'_{k-1} \mid \hat{x}_{k-1})) + C
$$


这里及之后的 $C$ 代表与 $x$ 无关的常数。由之前的定义可知：


$$
\begin{align}
& ln ( p (y_k \mid x_k)) = -\frac{1}{2} (y_k - C_k {x}_k)^T R_k^{-1} (y_k - C_k {x}_k) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det R_k )}_{\text{independent of x}} \\
& ln ( p(x_k \mid {x}'_{k-1}, v_k)) = -\frac{1}{2} (x_k - A_{k-1} {x}'_{k-1})^T Q_k^{-1} (x_k - A_{k-1} {x}'_{k-1}) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det Q_k )}_{\text{independent of x}} \\
& ln ( p({x}'_{k-1} \mid \hat{x}_{k-1})) = -\frac{1}{2} (x'_{k-1} - \hat{x}_{k-1})^T \hat{P}_{k-1}^{-1} (x'_{k-1} - \hat{x}_{k-1}) + \underbrace{ -\frac{1}{2} ln((2 \pi)^N det \hat{P}_{k-1} )}_{\text{independent of x}} \\
\end{align}
$$


这些都是平方马氏距离（Mahalanobis distance），忽略和 $x​$ 无关的项，我们可以构建整体的目标函数：


$$
J(x) =  (y_k - C_k {x}_k)^T R_k^{-1} (y_k - C_k {x}_k) + (x_k - A_{k-1} {x}'_{k-1})^T Q_k^{-1} (x_k - A_{k-1} {x}'_{k-1}) + (x'_{k-1} - \hat{x}_{k-1})^T \hat{P}_{k-1}^{-1} (x'_{k-1} - \hat{x}_{k-1})
$$


后验概率最大处也就是目标函数 $J(x)$ 的最小值处。接着，定义如下变量：


$$
z = \left\lbrack \begin{matrix} \hat{x}_{k-1} \\ v_k \\ y_k \end{matrix} \right\rbrack , \quad H =  \left\lbrack \begin{matrix} 1 &  \\ -A_{k-1} & 1 \\  & C_k \end{matrix} \right\rbrack, \quad W = \left\lbrack \begin{matrix} \hat{P}_{k-1} & & \\  & Q_k &  \\ & & R_k \end{matrix} \right\rbrack, \quad {x} = \left\lbrack \begin{matrix} {x}'_{k-1} \\ {x}_k   \end{matrix} \right\rbrack
$$


则目标函数可以写成如下形式：



$$
J(x) = (z - Hx)^T W^{-1} (z - Hx) \\
\hat{x}_k = \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack = \mathop{\arg\min}_{x_k} J(x) 
$$


$J(x)​$ 是一个抛物面，最小值是其偏导数为0处的值：


$$
\left. \frac{\partial J(x)}{\partial x^T} \right\vert_{\hat{x}} = -H^TW^{-1}(z - H\hat{x}) = 0 \\
(H^T W^{-1} H) \hat{x} = H^T W^{-1} z
$$


代入 $z,H,W$，可得:


$$
\left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1} & -A^T_{k-1}Q^{-1}_k  \\ -Q^{-1}_k A_{k-1} & Q^{-1}_k + C^T_k R^{-1}_k C_k   \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k   \\ Q^{-1}_kv_k + C^T_k R^{-1}_k y_k  \end{matrix} \right\rbrack
$$


在求解的过程中，我们其实并不关心 $\hat{x}'_{k-1}$ 的解，利用公式 (6.1.1)，左乘如下矩阵：


$$
\left\lbrack \begin{matrix} I_p & 0 \\ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} & I_p \end{matrix} \right\rbrack
$$


方程变为：


$$
\left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1} & -A^T_{k-1}Q^{-1}_k  \\ 0 & Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k  \end{matrix} \right\rbrack \left\lbrack \begin{matrix} \hat{x}'_{k-1} \\ \hat{x}_k   \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} \hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k   \\ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k  \end{matrix} \right\rbrack
$$


将矩阵乘法的第二行提出来，便是 $\hat{x}_k$ 的解：


$$
\underbrace{(Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k) }_{\text{待化简}} \hat{x}_k = Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k
$$


利用公式 (6.2.2)，可以化简等式的左侧：


$$
\begin{align}
Q^{-1}_k + C^T_k R^{-1}_k C_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k & =  Q^{-1}_k - Q^{-1}_k A_{k-1} (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1} A^T_{k-1}Q^{-1}_k +  C^T_k R^{-1}_k C_k \\
& = \underbrace{ (Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T)^{-1} + C^T_k R^{-1}_k C_k }_{\hat{P}_k^{-1},\text{ 定义见下} }
\end{align}
$$


此时，我们定义如下变量：


$$
\begin{align}
& \check{P}_k = Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T \\
& \hat{P}_k = (\check{P}_k^{-1} + C^T_k R^{-1}_k C_k)^{-1}
\end{align}
$$


代入 $\hat{x}_k​$ 的解：


$$
\begin{align}
\hat{P}_k^{-1} \hat{x}_k & = Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}(\hat{P}^{-1}_{k-1}\hat{x}_{k-1} -A^T_{k-1}Q^{-1}_k v_k) + Q^{-1}_kv_k + C^T_k R^{-1}_k y_k \\
& = \underbrace{ Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}\hat{P}^{-1}_{k-1} }_{\check{P}^{-1}_k A_{k-1}, \text{证明见下}} \hat{x}_{k-1} + \underbrace{( -Q^{-1}_k A_{k-1}(\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}A^T_{k-1}Q^{-1}_k + Q^{-1}_k)}_{\check{P}_k^{-1}, \text{ 代入公式（6.2.2）可直接得到定义}} v_k + C^T_k R^{-1}_k y_k \\
& = \check{P}_k^{-1}\underbrace{(A_{k-1}\hat{x}_{k-1} + v_k)}_{\check{x}_k} + C^T_k R^{-1}_k y_k 
\end{align}
$$


上式中第一步化简的证明如下：


$$
\begin{align}
Q^{-1}_k A_{k-1} \underbrace{ (\hat{P}^{-1}_{k-1}+A^T_{k-1} Q^{-1}_k A_{k-1})^{-1}}_{\text{代入公式 （6.2.2）}} \hat{P}^{-1}_{k-1} & = Q^{-1}_k A_{k-1}(\hat{P}_{k-1} - \hat{P}_{k-1}A^T_{k-1}\underbrace{(Q_k + A_{k-1}\hat{P}_{k-1}A_{k-1}^T)^{-1}}_{\check{P}_k^{-1}} A_{k-1}\hat{P}_{k-1} )\hat{P}^{-1}_{k-1} \\
& =  Q^{-1}_k A_{k-1}(\hat{P}_{k-1} - \hat{P}_{k-1}A^T_{k-1}\check{P}_{k-1}^{-1} A_{k-1} \hat{P}_{k-1} )\check{P}_{k-1}^{-1} \\
& =  (Q^{-1}_k - Q^{-1}_k \underbrace{A_{k-1} \hat{P}_{k-1}A^T_{k-1}}_{\check{P}_k - Q_k, \text{ 定义}}\check{P}_{k-1}^{-1}) A_{k-1}   \\
& = (Q^{-1}_k - Q^{-1}_k + \check{P}^{-1}_k)A_{k-1} \\
& = \check{P}^{-1}_k A_{k-1}
\end{align}
$$


至此，我们已推出了基于MAP的迭代滤波的解：


$$
\begin{matrix}
\text{Prediction:} & \begin{matrix} \check{P}_k = A_{k-1}\hat{P}_{k-1}A_{k-1}^T + Q_k \\ \check{x}_k = A_{k-1}\hat{x}_{k-1} + v_k \end{matrix} \\
\text{Correction} & \begin{matrix} \hat{P}_k^{-1} =\check{P}_k^{-1} + C^T_k R^{-1}_k C_k \\ \hat{P}_k^{-1}\hat{x}_k = \check{P}_k^{-1}\check{x}_k + C^T_k R^{-1}_k y_k  \end{matrix} 
\end{matrix}
$$


定义卡尔曼增益：


$$
K_k = \hat{P}_k C_k^T R_k^{-1}
$$


此时，卡尔曼增益中含有 $\hat{P}_k$ 项，我们进行如下变换：


$$
\begin{align}
1 & = \hat{P}_k (\check{P}_k^{-1} + C^T_k R^{-1}_k C_k) \\
& = \hat{P}_k \check{P}_k^{-1} + K_k C_k \\
\hat{P}_k  & = (1 - K_k C_k) \check{P}_k \\
\underbrace{\hat{P}_k C_k^T R_k^{-1}}_{K_k} & = (1 - K_k C_k) \check{P}_k C_k^T R_k^{-1} \\
K_k (1 + C_k \check{P}_k C_k^T R_k^{-1} ) & = \check{P}_k C_k^T R_k^{-1}\\
K_k &= \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k )^{-1}
\end{align}
$$


将其代入迭代滤波的解：


$$
\begin{align}
\hat{P}_k^{-1} &=\check{P}_k^{-1} + C^T_k R^{-1}_k C_k \\
1 & = \hat{P}_k \check{P}_k^{-1} + \underbrace{\hat{P}_k C^T_k R^{-1}_k}_{K_k} C_k \\
\hat{P}_k \check{P}_k^{-1} & = 1 - K_k C_k \\
\hat{P}_k & = (1 - K_k C_k) \check{P}_k^{-1}
\end{align}
$$

$$
\begin{align}
\hat{P}_k^{-1}\hat{x}_k & = \check{P}_k^{-1}\check{x}_k + C^T_k R^{-1}_k y_k \\
\hat{x}_k & = \underbrace{\hat{P}_k \check{P}_k^{-1}}_{1-K_kC_k}\check{x}_k + \underbrace{\hat{P}_k C^T_k R^{-1}_k}_{K_k} y_k \\
\hat{x}_k & = \check{x}_k + K_k(y_k - C_k \check{x}_k)
\end{align}
$$




便得到了经典的卡尔曼滤波：


$$
\begin{matrix}
\text{Prediction:} & \begin{matrix} \check{P}_k = A_{k-1}\hat{P}_{k-1}A_{k-1}^T + Q_k \\ \check{x}_k = A_{k-1}\hat{x}_{k-1} + v_k \end{matrix} \\
\text{Kalman gain:} & K_k = \check{P}_k C_k^T (C_k \check{P}_k C_k^T + R_k )^{-1} \\
\text{Correction} & \begin{matrix} \hat{P}_k =  (1-K_k C_k)\check{P}_k^{-1} \\ \hat{x}_k = \check{x}_k + K_k(y_k - C_k \check{x}_k)  \end{matrix} 
\end{matrix}
$$






## 3. 从贝叶斯推断推导卡尔曼滤波









## 4. 从增益的角度看卡尔曼增益 





## 5. 卡尔曼滤波的性质讨论



## 6. 数学补充

#### 6.1 舒尔补（Schur complement）

对于一块矩阵 $M$  :


$$
M = \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack
$$


若 $A​$ 是可逆矩阵，则 $M​$ 对 $A​$ 的舒尔补为： $M/A = D - CA^{-1}B​$

若 $D​$ 是可逆矩阵，则 $M​$ 对 $D​$ 的舒尔补为： $M/D = A - BD^{-1}C​$  

舒尔补可以用于构造上三角矩阵：


$$
\begin{align}
\text{左乘构造上三角} \quad & \left\lbrack \begin{matrix} I_p &  \\ -CA^{-1} & I_p \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} A & B \\ 0 & D - CA^{-1}B \end{matrix} \right\rbrack \tag{6.1.1}\\
\text{右乘构造上三角} \quad & \left\lbrack \begin{matrix} A & B \\ C & D \end{matrix} \right\rbrack \left\lbrack \begin{matrix} I_p &  \\ -D^{-1}C & I_p \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} A - BD^{-1}C & B \\ 0 & D \end{matrix} \right\rbrack \tag{6.1.2}
\end{align}
$$


#### 6.2 SMW等价（Sherman-Morrison-Woodbury Identity）

对如下矩阵做LDU分解（lower-diagonal-upper）和UDL分解（upper-diagonal-lower）：


$$
\begin{align}
\left\lbrack \begin{matrix} A^{-1} & -B \\ C & D \end{matrix} \right\rbrack & = \left\lbrack \begin{matrix} 1 & 0 \\ CA & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A^{-1} & 0 \\ 0 & D + CAB \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & -AB \\ 0 & 1 \end{matrix} \right\rbrack \quad (LDU) \\
& = \left\lbrack \begin{matrix} 1 & -BD^{-1} \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A^{-1} + BD^{-1}C & 0 \\ 0 & D \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & 0 \\ D^{-1}C & 1 \end{matrix} \right\rbrack \quad (UDL)
\end{align}
$$


针对LDU和UDL分别进行矩阵求导，可得：


$$
\begin{align}
\left\lbrack \begin{matrix} A^{-1} & -B \\ C & D \end{matrix} \right\rbrack^{-1} & = \left\lbrack \begin{matrix} 1 & AB \\ 0 & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix} A & 0 \\ 0 & (D + CAB)^{-1} \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & 0 \\ -CA & 1 \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} A-AB(D+CAB)^{-1}CA & AB(D+CAB)^{-1} \\ -(D+CAB)^{-1}CA & (D+CAB)^{-1} \end{matrix} \right\rbrack \quad (LDU) \\
& = \left\lbrack \begin{matrix} 1 & 0 \\ -D^{-1}C & 1 \end{matrix} \right\rbrack \left\lbrack \begin{matrix}(A^{-1} + BD^{-1}C)^{-1} & 0 \\ 0 & D^{-1} \end{matrix} \right\rbrack \left\lbrack \begin{matrix} 1 & BD^{-1} \\ 0 & 1 \end{matrix} \right\rbrack = \left\lbrack \begin{matrix} (A^{-1} + BD^{-1}C)^{-1} & (A^{-1} + BD^{-1}C)^{-1}BD^{-1} \\ -D^{-1}C(A^{-1} + BD^{-1}C)^{-1} & D^{-1} - D^{-1}C(A^{-1} + BD^{-1}C)^{-1}BD^{-1} \end{matrix} \right\rbrack \quad (UDL)
\end{align}
$$


实际上两个求逆后的矩阵应该是相等的，只是由于分解的顺序不同，所以以不同的形式呈现了出来，因此可以得到非常重要的SMW等价：


$$
\begin{align}
(A^{-1} + BD^{-1}C)^{-1} & \equiv A-AB(D+CAB)^{-1}CA \tag{6.2.1} \\
(D+CAB)^{-1} & \equiv D^{-1} - D^{-1}C(A^{-1} + BD^{-1}C)^{-1}BD^{-1} \tag{6.2.2} \\
AB(D+CAB)^{-1} & \equiv (A^{-1} + BD^{-1}C)^{-1}BD^{-1} \tag{6.2.3} \\
(D+CAB)^{-1}CA & \equiv D^{-1}C(A^{-1} + BD^{-1}C)^{-1} \tag{6.2.4}
\end{align}
$$






## 参考

[1] State Estimation for Robotics, Timothy D. BARFOOT

[2] 视觉SLAM十四讲, 高翔 



[1]:https://res.cloudinary.com/bxy1994/image/upload/v1553094985/SLAM/KF_scheme.png
[2]:https://res.cloudinary.com/bxy1994/image/upload/v1553001904/SLAM/KF.png