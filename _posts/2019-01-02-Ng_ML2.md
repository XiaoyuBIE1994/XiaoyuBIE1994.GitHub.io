---
layout: post
title: Machine Learning by Andrew (二) 逻辑回归
category: 笔记
tags: 
  - machine learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、逻辑回归的模型
在机器学习中，除了回归问题，我们也会经常碰到分类的问题，比如判断一封邮件是否是垃圾邮件，某一笔线上交易是否是骗人的，肿瘤是恶性还是良性等等。与回归问题不同，分类问题的输出往往是有限的个数。当只有两个输出是，就是最为经典的二分类模型:

$$
y \in \{0,1\}
$$

在分类问题中，若我们仍然使用回归模型:

$$
h_{\theta}(x) = \theta ^T x
$$

如下图所示:
![](https://res.cloudinary.com/bxy1994/image/upload/v1546466745/ML_coursera/logic_vs_linear_lall1c.png)
在判断是否为恶性肿瘤的问题中，我们通过设定某一阈值来进行分类，比如:
> $h_{\theta} \ge 0.5$， predict "y=1"
> $h_{\theta} < 0.5$， predict "y=0"

但是，当某一些样本具有一些极端值的时候，如图中右上方的红色叉叉所示，会让我们的预测模型曲线产生较大的变化，而这显然不是我们希望看到的。同时，当我们学习好模型$h_{\theta}(x)$之后，对于某一输入x，可能会发生$h_{\theta}(x)>1$或者$h_{\theta}(x)<0$，为了避免此类问题的出现，在分类问题中，通常会使用另外一个模型，也就是**逻辑回归(Logistic Regression)**:

$$
h_{\theta}(x) = \frac{1}{1+e^{-\theta^Tx}}
$$

如下图所示:
![Logistic Regression](https://res.cloudinary.com/bxy1994/image/upload/v1546542503/ML_coursera/logistic_regression_qn5ape.png)

在这一模型中，我们会发现，对于任意的输入x，函数的输出都在0到1之间，且极值对于模型的影响力有限。对于逻辑回归，我们使用**决策边界(Decision Boundary)**，也就是前面的阈值，同理:
> $h_{\theta} \ge 0.5$， predict "y=1"  
> $h_{\theta} < 0.5$， predict "y=0"
同样，我们发现，逻辑回归对边界处的数据十分敏感（导数最大）

## 二、损失函数

对于逻辑回归的模型，若我们同线性回归一样使用欧式距离，会发现损失函数是一个非凸函数，显然这不适用于我们的梯度下降算法，所以，我们使用了新的损失函数定义：

$$
J(\theta) = - \left[\frac{1}{m} \sum_{i=1}^{m} y^{(i)}log\; h_{\theta}(x^{(i)}) + (1-y^{(i)})log (1-h_{\theta}(x^{(i)})) \right] 
$$

此时，
>若$y^{(i)} = 0$，$J(\theta)^{(i)} = -log (1-h_{\theta}(x^{(i)}))$  
> $\qquad\qquad$ $h_{\theta}(x) \to 0$, $Cost \to \infty$  
> $\qquad\qquad$ $h_{\theta}(x) \to 1$, $Cost \to 0$  
>若$y^{(i)} = 1$，$J(\theta)^{(i)} = -log (h_{\theta}(x^{(i)}))$  
> $\qquad\qquad$ $h_{\theta}(x) \to 0$, $Cost \to 0$  
> $\qquad\qquad$ $h_{\theta}(x) \to 1$, $Cost \to 1$  


## 三、梯度下降

对于逻辑回归，我们会发现，其梯度和线性回归的梯度在形式上是一致的：

$$
\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)}_j 
$$

证明如下:
>对于$f(x) = \frac{1}{1+e^{-x}}$，有$f(x)' = f(x)(1-f(x))$  
> 故 :
$$
\frac{\partial J}{\partial \theta_j} = -\frac{1}{m} \sum_{i = 1} ^ m \left[ y^{(i)} \frac{h_{\theta}(x)(1-h_{\theta}(x))}{h_{\theta}(x)} + (1-y^{(i)})\frac{-h_{\theta}(x)(1-h_{\theta}(x))}{1-h_{\theta}(x)} \right] \cdot x_j^{(i)}\\
= -\frac{1}{m} \sum_{i = 1} ^ m \frac{y^{(i)}h_{\theta}(x)-y^{(i)}h^2_{\theta}(x)-h^2_{\theta}(x)+h^3_{\theta}(x)}{h_{\theta}(x)(1-h_{\theta}(x))} \cdot x_j^{(i)} \\
= \frac{1}{m} \sum_{i = 1} ^ m  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)}_j 
$$

## 四、高级优化
虽然梯度下降通过迭代的方式有效的降低了计算量，但当样本量m和特征n非常大时，依然需要很大的计算量，同时，我们还需要手动的选择学习速率$\alpha$，所以在实际的应用中我们会使用更加复杂的优化方法，如：
> Conjugate gradient  
> BFGS  
> L-BFGS  

## 五、多分类问题 one vs all
在上述问题中，$y \in \{0,1\}$，分类的结果只有两种，称为二分类问题。而对于多分类问题，我们可以采用 One vs All 的策略。当需要分类K个类时，我们分别训练K个模型，每个模型$h_{\theta}^i(x)$中，$y = i$为positive，$y \neq i$为negative。当进行分类是，分别计算每一个$h_{\theta}^i(x)$，选取最大值对应的类别作为输出：

$$
h_{\theta}(x) = \mathop{max}_i h_{\theta}^i(x)
$$


