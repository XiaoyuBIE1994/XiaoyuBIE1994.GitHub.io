---

layout: post
title: Deep Learning by Andrew (5-3) 序列模型（sequential model）与注意力机制（attention mechanism）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 1. 基础模型

**sequence to sequence**

sequence to sequence 模型是指输入一段序列，网络再输出一段序列，由编码-解码两部分组成，最为常见的应用之一便是机器翻译，如法语翻译英语：

> 输入为：Jane visite l'Afrique en septembre（分别标记为 $x^{<1>}, x^{<2>},x^{<3>},x^{<4>},x^{<5>}$）
>
> 输出为：Jane is visiting Africa in Septembre（分别标记为 $y^{<1>}, y^{<2>},y^{<3>},y^{<4>},y^{<5>}, y^{<6>}$）



用 RNN 搭建一个序列模型，在编码部分，将法语句子作为输入依次喂入模型作为编码，接着建立一个解码网络，以编码器的输出作为解码器的输入，解码器部分和之前的语言文本生成模型类似，当解码器停止时就代表着翻译结束。和之前一样，在解码部分，我们可以将每一步的输出作为下一步的输入

相关论文：

- Sutskever et al., Sequence to sequence learning with neural networks, 2014
- Cho et al., Learning phrase representation using RNN encoder-decoder for statistical machine translation, 2014



**image to sequence**

和 sequence to sequence 模型类似，image to sequence 模型也是有编码和解码部分构成，不一样的是输入为图像，而不是序列，所以编码网络从 RNN 变成了 CNN



相关论文：

- Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks
- Vinyals et. al., 2014. Show and tell: Neural image caption generator；
- Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions









## 2. 挑选最有可能的句子

机器翻译模型和语言生成模型有一定类似的地方，但也有着一些显著的差异。例如：

- 语言生成模型的最开始的输入往往是一个全零向量，而机器翻译模型在生成部分的输入是前置编码网络的输出，可以认为机器翻译是一个有条件的语言模型，而不是以所有的句子的概率去建模
- 语言模型中，我们每一步的输出是各个单词的概率，输出则是基于这些概率的随机采样，而机器翻译中我们需要的是条件概率最大化，所以我们需要挑选出使得整体概率最大的输出，也就是 $output = \mathop{\arg\max} P( y^{<1>} ,\dots, y^{<T>} \mid x)$



**greedy search vs beam search**

既然机器翻译中我们需要最大化条件概率，那么我们则需要记录下每一步中每个输出的概率，最后求得总概率的最大化，而又因为：



$$
\begin{align}
& \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<1>}, y^{<2>} | x) \right\rbrace \\
= & \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<2>} \mid y^{<1>}, x) * P (y^{<1>} \mid x) \right\rbrace \\
\neq & \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<2>} \mid \hat{y}^{<1>}, x) \right\rbrace, \quad \text{where } \hat{y}^{<1>} = \mathop{\arg \max} P (y^{<1>} \mid x)
\end{align}
$$



所以我们不能单独的在每一步都选取最大概率的单词作为翻译的输出和下一步的输入。严格意义上我们需要把每一步中每一个单词输出的概率都记录下来，然后分别计算对应下一步中每个输出单词的概率，以此递推，最后选取概率最大的一组输出，这就是 **greedy search**。然而，若是每一步都记录下每个单词的概率，那么随着翻译语句长度的增加，计算量将以指数级增长，这在实际的系统中是无法接受的，常见的解决办法是做出一个近似的搜索，尝试找出使条件概率最大化的输出，但不保证找出来的 $y$ 使概率最大化，以牺牲一定的精确度换来了计算速度极大地提升。而**集束搜索（beam serach）**就是一种常见的近似方法。



## 3. 集束搜索（Beam search）

#### 3.1 Beam search 算法

beam search 算法可以有效的降低 sequence to sequence 模型的计算复杂度，和 greedy search 中每一步都计算所有单词的概率不同，beam search 中我们定义一个变量 $B$，代表集束宽度，假设 $B=3$，词汇库的总量为 $m = 10000$ ，则使用 beam search 算法的的 seq2seq 模型步骤如下：

- 编码网络对输入进行编码，得到解码网络的初始输入 $a^{<0>}$
- 将  $a^{<0>}$ 输入到解码的 RNN 网络，第一步的输出为词汇库中10000个单词每个出现在第一位的概率，选取出其中概率最大的3个单词，记为 $y^{<1>}_1, y^{<1>}_2, y^{<1>}_3$，其概率分别为 $P_1(y^{<1>} \mid x), P_2(y^{<1>} \mid x), P_3(y^{<1>} \mid x)$
- 将这3个输出分别作为下一步的输入，得到不同输入下的 softmax 结果，此时可以得到30000 个输出，由贝叶斯公式 $P(y^{<1>}, y^{<2>} \mid x) = P(y^{<2>} \mid y^{<1>}, x) * P(y^{<1>} \mid x)$ 可以得到30000个不同输出的联合概率，从这30000个输出组合中再选出概率最大的3个组合，记为 $(y^{<1>}y^{<2>})_1, (y^{<1>}y^{<2>})_2, (y^{<1>}y^{<2>})_3$ ，同样记录下其概率 $P_1(y^{<1>}, y^{<2>} \mid x), P_2(y^{<1>}, y^{<2>} \mid x), P_3(y^{<1>}, y^{<2>} \mid x)$ 
- 以此循环，每次都只取前一层概率最大的3种组合作为下一层的输入，直到翻译结束



![][1]

ps: [图片来源](https://d2l.ai/chapter_recurrent-neural-networks/beam-search.html)



#### 3.2 Beam search 的改进

**长度归一化：**

对于 beam search 算法，我们的目标在于最大化如下概率：


$$
\mathop{\arg\max}_y \prod _{t=1}^{T_y} P(y^{<t>} \mid x, y^{<1>},\dots, y^{<t-1>}) 
$$


通常，累乘项中的每一项都非常的小（远小于1），乘以很多个远小于1的数字，会导致最后得到的结果非常非常的小，以至于计算机的浮点运算无法保证结果的精确性，因此，在实际应用过程中，我们不会将这一数值最大化，而是进行log转换：


$$
\mathop{\arg\max}_y \sum _{t=1}^{T_y} log \, P(y^{<t>} \mid x, y^{<1>},\dots, y^{<t-1>})
$$


由于对数函数的单调性质，这两个式子是等价的，通过这一步转换，我们可以得到一个更加数值稳定的算法。另外，我们需要注意到，无论是直接的概率相乘，还是取对数后求和，项越多，其值就会越小（因为概率总是小于1），此时进行语句翻译时，系统就会不自然的趋向于选择语句较短的结果，因此我们需要针对项数进行一定的**归一化**。实际操作中，没有直接的除以 $T_y$ 来进行归一，而是用一种更加柔和的方法，除以 $T_y^{\alpha}$ ：


$$
\frac{1}{T_y^{\alpha}}\mathop{\arg\max}_y \sum _{t=1}^{T_y} log \, P(y^{<t>} \mid x, y^{<1>},\dots, y^{<t-1>})
$$


在这里，$\alpha$ 是一个可调的参数。



**集束宽度的选择：**

在实际的应用中，集束宽度越大，搜索的结果越准确，但更大的 $B$ 往往计算更复杂，因而：

- 在实际生产系统中，$B$ 往往取值在10左右，100是非常大的值
- 在研究系统中，为了取得更好的实验效果，$B$ 的取值经常在1000或者3000左右
- 同广度优先搜索（BFS, Breadth First Search）和深度优先搜索（DFS, Depth First Search）不同，集束搜索并不保证找到精确的最大值，但可以极大的提高计算速度



#### 3.3 Beam search 的误差分析

集束搜索是一种近似搜索，也被称为启发式搜索，它并不总是输出最有可能的结果，所以我们需要一定的方法来对错误的输出结果进行分析，来定位问题是来自于 RNN 网络还是搜索算法，以法语翻译英语为例：



> 原句： Jane visite l'Afrique en septembre
>
> Human: Jane visits Africa in September, 记为 $y^{\star}$
>
> Algorithm: Jane visited Africa las September, 记为 $\hat{y}$



- $ P(y^{\star} \mid x) > P(\hat{y} \mid x)$ ，说明虽然人类翻译的概率更高，但算法仍然选择了 $\hat{y}$，说明 beam search 并没有找到最大化 $P(y \mid x)$ 的 $y$ 值，也就是 beam search 出现了问题

- $ P(y^{\star} \mid x) \leq P(\hat{y} \mid x)$，说明虽然 $y^{\star}$ 虽然是更好的翻译结果，但 RNN 网络却给了 $\hat{y}$ 更高的预测概率，说明 RNN 网络出现了问题

- 这里没有考虑长度归一化的现象

- 在开发集上，我们可以针对每个句子进行如上的检测，得到每个句子的出错情况，进行针对性的修改

  

## 4. Bleu Score

对于机器翻译系统来说，一种语言对于另外一种语言的翻译常常会有很多种正确且合适的结果，我们无法得到像图像识别一样的标准答案，所以针对不同的翻译结果，往往很难评估哪一个结果是更好的，所以我们需要针对设计一种合理的评价体系，比如 bleu score (bleu, bilingual evaluation understudy)，同样以法语-英语翻译为例：

>  French: Le chat est sur le tapis.
>
> Reference 1: The cat is on the mat.
>
> Reference 2: There is a cat on the mat.
>
> MT (machine translation) output: the the  the the the the the.



- 精确度：观察输出结果的每一个词是否出现在参考翻译之（一般忽略大小写），如上述例子中， 7个 the 都出现在参考中，此时的精确度就是 $\frac{7}{7}$
- 改良的精确度：将每个单词设置一个得分上限（比如单个参考句子中出现的最大次数，上述例子中 the 在 ref 1 中出现了2次，在 ref 2 中出现了 1 次，那么 the 的得分上限就是2），此时改良后的精确度便是 $\frac{2}{7}$



**二元词组的 bleu score:**

与单个词的评估方式类似，这里以相邻的两个单词组成的词组来进行 bleu score 评估，为了便于展示（之前的结果中二元词组全是 the the，且在参考中无法找到）这里将机器的翻译结果进行更改：

> MT (machine translation) output: The cat the cat on the mat

评分机制如下：

|         | $Count$ | $Count_{clip}$ |
| ------- | ------- | -------------- |
| the cat | 2       | 1              |
| cat the | 1       | 0              |
| cat on  | 1       | 1              |
| on the  | 1       | 1              |
| the mat | 1       | 1              |

此时的精确度为 $\frac{4}{6}$



**多元词组的一般形式：**

多元词组成为 n-gram，由上述的公式进行类推，得到多元词组评分的一般形式：


$$
\begin{align}
& P_1 = \frac{\sum_{unigram \in \hat{y}} Count_{clip}(unigram)}{\sum_{unigram \in \hat{y}} Count(unigram)} \\
& \\
& P_n = \frac{\sum_{n-gram \in \hat{y}} Count_{clip}(n-gram)}{\sum_{n-gram \in \hat{y}} Count(n-gram)}
\end{align}
$$


这里的精确度都是修正后的精确度，若 MT-output 和参考中的句子完全一样，则精确度的得分为1



**Bleu 指数：**

将多元词组的得分组合起来，就形成了最终的 bleu 指数，以 $p_n$ 代表 n 元词组的得分，一般有 $p_1, p_2, p_3, p_4$，那么总的 bleu 指数便是这些得分的平均数取幂，并且加入 BP 惩罚：


$$
Bleu \; score = BP \; exp(\frac{1}{4} \sum_{n=1}^4 p_n)
$$


BP (brevity penalty) 惩罚的意义在于，如果输出的结果非常的简短，那么很容易得到高精确度，因此需要有一个 adjustment factor 来惩罚较短的输出


$$
BP = \begin{cases} 1\text{, if MT_output_lenght > reference_out_put_lenght}, \\
 exp(1 - \text{reference_out_put_lenght/MT_output_lenght} ) \text{, otherwise} \end{cases}
$$


当翻译的结果长度小于参考结果的长度时，翻译结果越短，BP 值越小，bleu score 也就越低，起到了一定的补偿作用



## 5. 注意力模型（Attention model）



机器翻译的模型本质上是一个用一个 RNN 做 encoder，再用另外一个 RNN 做 decoder，如下所示。而注意力模型（attention model）就是对于这个模型（尤其是针对长句子）的一个改进

![][2]

当所需要翻译的语句很长的时候，人类通常不会先把整个句子读完，并且记住整个句子，而是先读一部分，翻译出一部分，再看，再翻译，因为记住整个句子太难了。这一点对于机器翻译模型来说也是一样的，使用以上的模型，对于简短句子的翻译效果是很好的，bleu score 分数很高，但随着句子长度的增加，翻译效果会逐渐下降。当然，过于简短的句子也是非常难以翻译的，所以句子过短时 bleu score 的分数也会较低，如下所示：

![][3]

注意力模型的引入使得机器翻译模型更像正常人类进行翻译时采用的方法，注意力模型在某个时间点只看句子的一部分，如下图所示：

> 参考论文：
>  [Neural machine translation by jointly learning to align and translate, Bahdanau et. al., 2014](https://arxiv.org/abs/1409.0473)
>  [Show, attend and tell: Neural image caption generation with visual attention, Kelvin Xu, et. al., 2015](http://www.jmlr.org/proceedings/papers/v37/xuc15.pdf)



![][4]

通过一个双向 LSTM 进行编码，然后在输出网络与编码之间，加入一层全连接网络，经过 softmax 层后计算每一个编码的输出在当前步骤的翻译结果中占有的权重大小，利用这些权重，计算出一个基于整个语句编码输出的加权平均，作为当前解码步骤的一个输入，记为 $context^{<t>}$，中间量 $s^{<t-1>}$ 既作为计算 Attention 的输入，也作为解码网络 LSTM的记忆细胞的输入，原理示意图如下：



![][5]



**注意力模型的例子：**

- 将不标准的时间格式转化为统一的时间格式
- 对注意力权重进行可视化（画出每一个 $\alpha^{<i,j>}$的大小,其中 $i$ 代表输出结果的编号，$j$ 代表输入的编号）



## 6. 语音识别

语音识别是将一段音频信号转化为相应的文本信息，人耳的机制是将音频信号转化为声谱图，也就是基于时间和不同频率振幅的二维图像。在过去的语音识别问题中，语言学家会构造音素来进行识别，尝试将语言拆分为基本的声音单元（如 "The"有一个"de"音一个"e"音），但是在基于深度学习的 end-to-end 系统中音素的分离已经不再重要。

**Notation:**

- 一般的语音识别系统，300个小时的数据集是合理的选择
- 学术界中，一般会使用到3000小时左右的数据集
- 最好的工业界系统数据集，达到了1w小时甚至10w小时的数量级



对于语音识别系统的构建，较好的办法是构建一个 RNN 模型，然后利用注意力机制，直接输出文字。另一个很好的办法是使用 CTC 损失函数来做语音识别

**CTC (connectionist temporal classification):**

例如，我们拥有 10s 的音频输入，特征是 100 Hz，那么就会有1000个输入向量，但实际上我们没有1000的对应的输出，那么 CTC 损失函数允许 RNN 输出连续序列，例如 $the \; quick$ 的音频会输出 $ttt\_h\_eee\_ \_ \_\sqcup qqq \dots$

的输出，这里 $\_$ 代表空白字符，用于区分不同的单词，而 $\sqcup$ 代表空格，用于区分不同的单词，CTC 的基本原理是折叠哪些没有被空白字符分隔开的重复字符

> 参考论文：
> [Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks, A Graves, et. al., 2006](https://dl.acm.org/citation.cfm?id=1143891)



## 7. 触发字检测

触发字（trigger word）普遍存在于各类语音识别系统，用于唤醒机器。简单的触发字应用算法，是使用 RNN 模型，将音频信号转化为声谱图作为输入，而输出为 0/1，代表是否存在唤醒词，为了数据集0,1标签的平衡，一般将触发词后的多个目标都标记为1，如下所示：

![][6]



## 8 代码实现

以下代码参考自吴恩达的Deep Learning课程作业，并不是作业的答案，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 

#### 8.1 Machine translation

基于注意力机制的机器翻译系统，原理图见 Section 5 ，此处的例子是将不规则的时间写法转化为标准的时间写法：

- 9 may 1998 -> 1998-05-09
- 10.09.70 -> 1970-09-10
- 4/28/90 -> 1990-04-28
- 1 jan 1981 -> 1981-01-01



```python
# 必要的库函数
from keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply
from keras.layers import RepeatVector, Dense, Activation, Lambda
from keras.optimizers import Adam
from keras.utils import to_categorical
from keras.models import load_model, Model
import keras.backend as K
import numpy as np

from faker import Faker
import random
from tqdm import tqdm
from babel.dates import format_date
from nmt_utils import *
import matplotlib.pyplot as plt
%matplotlib inline
```

```python
# 数据集准备
m = 10000
dataset, human_vocab, machine_vocab, inv_machine_vocab = load_dataset(m)
Tx = 30 # 假设输入向量最长为30
Ty = 10 # 输出向量长度固定为10
X, Y, Xoh, Yoh = preprocess_data(dataset, human_vocab, machine_vocab, Tx, Ty)
```

```python
# 定义一部分通用的网络层
repeator = RepeatVector(Tx)
concatenator = Concatenate(axis=-1)
densor1 = Dense(10, activation = "tanh")
densor2 = Dense(1, activation = "relu")
activator = Activation(softmax, name='attention_weights')
dotor = Dot(axes = 1)
```

```python
# 模型构建
def one_step_attention(a, s_prev):
    s_prev = repeator(s_prev)
    concat = concatenator([a, s_prev])
    e = densor1(concat)
    energies = densor2(e)
    alphas = activator(energies)
    context = dotor([alphas, a])
    return context
  
def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):
    """
    Arguments:
    Tx -- length of the input sequence
    Ty -- length of the output sequence
    n_a -- hidden state size of the Bi-LSTM
    n_s -- hidden state size of the post-attention LSTM
    human_vocab_size -- size of the python dictionary "human_vocab"
    machine_vocab_size -- size of the python dictionary "machine_vocab"

    Returns:
    model -- Keras model instance
    """
    
    X = Input(shape=(Tx, human_vocab_size))
    s0 = Input(shape=(n_s,), name='s0')
    c0 = Input(shape=(n_s,), name='c0')
    s = s0
    c = c0
  
    outputs = []
    
    a = Bidirectional(LSTM(n_a, return_sequences = True))(X)
    
    for t in range(Ty):
        context = one_step_attention(a, s)
        s, _, c = post_activation_LSTM_cell(context, initial_state= [s, c])
        out = output_layer(s)
        outputs.append(out)
        
    model = Model(input=[X, s0, c0], output=outputs)
    
    return model
```



```python
# 模型的编译，训练和预测
model = model(Tx, Ty, n_a, n_s, len(human_vocab), len(machine_vocab))
opt = Adam(lr=0.005, beta_1=0.9, beta_2=0.999, decay=0.01)
model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])
model.fit([Xoh, s0, c0], outputs, epochs=1, batch_size=100)
model.load_weights('models/model.h5')
prediction = model.predict([source, s0, c0])
```



#### 8.2 Trigger word detection

```python
# 必要的库函数
import numpy as np
from pydub import AudioSegment
import random
import sys
import io
import os
import glob
import IPython
from td_utils import *
%matplotlib inline

from keras.callbacks import ModelCheckpoint
from keras.models import Model, load_model, Sequential
from keras.layers import Dense, Activation, Dropout, Input, Masking, TimeDistributed, LSTM, Conv1D
from keras.layers import GRU, Bidirectional, BatchNormalization, Reshape
from keras.optimizers import Adam
```

```python
_, data = wavfile.read("audio_examples/example_train.wav")
Tx = 5511 # The number of time steps input to the model from the spectrogram
n_freq = 101 # Number of frequencies input to the model at each time step of the spectrogram
Ty = 1375 # The number of time steps in the output of our model
activates, negatives, backgrounds = load_raw_audio()

```

为了进行数据增广，通常的做法是利用背景声音和相应的词语（是唤醒词和不是唤醒词）进行人工合成，这时候需要人为的插入片段，在插入的时候需要检测片段是否重复

```python
def get_random_time_segment(segment_ms):
    segment_start = np.random.randint(low=0, high=10000-segment_ms)  
    segment_end = segment_start + segment_ms - 1
    
    return (segment_start, segment_end)
  
def is_overlapping(segment_time, previous_segments):
    segment_start, segment_end = segment_time
    overlap = False
    for previous_start, previous_end in previous_segments:
        if segment_start<=previous_end and segment_end>=previous_end:
            overlap = True
    return overlap
  
def insert_audio_clip(background, audio_clip, previous_segments):
    segment_ms = len(audio_clip)
    segment_time = get_random_time_segment(segment_ms)
    while is_overlapping(segment_time, previous_segments):
        segment_time = get_random_time_segment(segment_ms)

    previous_segments.append(segment_time)
    new_background = background.overlay(audio_clip, position = segment_time[0])
    
    return new_background, segment_time
  
def insert_ones(y, segment_end_ms):
    segment_end_y = int(segment_end_ms * Ty / 10000.0)
    for i in range(segment_end_y+1, segment_end_y+51):
        if i < y.shape[1]:
            y[0, i] = 1
    return y
```

触发词检测的模型如下所示：

![][7]

```python
# 建立模型并训练
def model(input_shape):
    
    X_input = Input(shape = input_shape)
    
    X = Conv1D(filters=196, kernel_size=15, strides=4)(X_input)        
    X = BatchNormalization()(X)      
    X = Activation('relu')(X)      
    X = Dropout(0.8)(X)  
    X = GRU(units=128, return_sequences=True)(X)   
    X = Dropout(0.8)(X) 
    X = BatchNormalization()(X)  
    X = GRU(units=128, return_sequences=True)(X)
    X = Dropout(0.8)(X)     
    X = BatchNormalization()(X)
    X = Dropout(0.8)(X)        
    X = TimeDistributed(Dense(1, activation = "sigmoid"))(X) 

    model = Model(inputs = X_input, outputs = X)
    
    return model  
  
model = model(input_shape = (Tx, n_freq))
model = load_model('./models/tr_model.h5')
opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, decay=0.01)
model.compile(loss='binary_crossentropy', optimizer=opt, metrics=["accuracy"])
model.fit(X, Y, batch_size = 5, epochs=1)
loss, acc = model.evaluate(X_dev, Y_dev)

# 预测
def detect_triggerword(filename):
    plt.subplot(2, 1, 1)

    x = graph_spectrogram(filename)
    # the spectogram outputs (freqs, Tx) and we want (Tx, freqs) to input into the model
    x  = x.swapaxes(0,1)
    x = np.expand_dims(x, axis=0)
    predictions = model.predict(x)
    
    plt.subplot(2, 1, 2)
    plt.plot(predictions[0,:,0])
    plt.ylabel('probability')
    plt.show()
    return predictions

chime_file = "audio_examples/chime.wav"
def chime_on_activate(filename, predictions, threshold):
    audio_clip = AudioSegment.from_wav(filename)
    chime = AudioSegment.from_wav(chime_file)
    Ty = predictions.shape[1]
    consecutive_timesteps = 0
    for i in range(Ty):
        consecutive_timesteps += 1
        if predictions[0,i,0] > threshold and consecutive_timesteps > 75:
            audio_clip = audio_clip.overlay(chime, position = ((i / Ty) * audio_clip.duration_seconds)*1000)
            consecutive_timesteps = 0
        
    audio_clip.export("chime_output.wav", format='wav')
```



[1]: https://res.cloudinary.com/bxy1994/image/upload/v1570635529/DL_coursera/BeamSearch.png
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1571318375/DL_coursera/MT_model.png
[3]:https://res.cloudinary.com/bxy1994/image/upload/v1571318702/DL_coursera/BleuScore_intuition.png
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1571319515/DL_coursera/Attention_model.png
[5]:https://res.cloudinary.com/bxy1994/image/upload/v1571319515/DL_coursera/Attention_mechanism.png

[6]:https://res.cloudinary.com/bxy1994/image/upload/v1571323107/DL_coursera/TriggerWord_LabelDiagram.jpg
[7]: https://res.cloudinary.com/bxy1994/image/upload/v1571325165/DL_coursera/TriggerWord_Model.png