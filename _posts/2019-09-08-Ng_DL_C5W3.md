---

layout: post
title: Deep Learning by Andrew (5-3) 序列模型（sequential model）与注意力机制（attention mechanism）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 1. 基础模型

**sequence to sequence**

sequence to sequence 模型是指输入一段序列，网络再输出一段序列，由编码-解码两部分组成，最为常见的应用之一便是机器翻译，如法语翻译英语：

> 输入为：Jane visite l'Afrique en septembre（分别标记为 $x^{<1>}, x^{<2>},x^{<3>},x^{<4>},x^{<5>}$）
>
> 输出为：Jane is visiting Africa in Septembre（分别标记为 $y^{<1>}, y^{<2>},y^{<3>},y^{<4>},y^{<5>}, y^{<6>}$）



用 RNN 搭建一个序列模型，在编码部分，将法语句子作为输入依次喂入模型作为编码，接着建立一个解码网络，以编码器的输出作为解码器的输入，解码器部分和之前的语言文本生成模型类似，当解码器停止时就代表着翻译结束。和之前一样，在解码部分，我们可以将每一步的输出作为下一步的输入

相关论文：

- Sutskever et al., Sequence to sequence learning with neural networks, 2014
- Cho et al., Learning phrase representation using RNN encoder-decoder for statistical machine translation, 2014



**image to sequence**

和 sequence to sequence 模型类似，image to sequence 模型也是有编码和解码部分构成，不一样的是输入为图像，而不是序列，所以编码网络从 RNN 变成了 CNN



相关论文：

- Mao et. al., 2014. Deep captioning with multimodal recurrent neural networks
- Vinyals et. al., 2014. Show and tell: Neural image caption generator；
- Karpathy and Li, 2015. Deep visual-semantic alignments for generating image descriptions









## 2. 挑选最有可能的句子

机器翻译模型和语言生成模型有一定类似的地方，但也有着一些显著的差异。例如：

- 语言生成模型的最开始的输入往往是一个全零向量，而机器翻译模型在生成部分的输入是前置编码网络的输出，可以认为机器翻译是一个有条件的语言模型，而不是以所有的句子的概率去建模
- 语言模型中，我们每一步的输出是各个单词的概率，输出则是基于这些概率的随机采样，而机器翻译中我们需要的是条件概率最大化，所以我们需要挑选出使得整体概率最大的输出，也就是 $output = \mathop{\arg\max} P( y^{<1>} ,\dots, y^{<T>} \mid x)$



**greedy search vs beam search**

既然机器翻译中我们需要最大化条件概率，那么我们则需要记录下每一步中每个输出的概率，最后求得总概率的最大化，而又因为：



$$
\begin{align}
& \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<1>}, y^{<2>} | x) \right\rbrace \\
= & \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<2>} \mid y^{<1>}, x) * P (y^{<1>} \mid x) \right\rbrace \\
\neq & \mathop{\arg\max}_{y^{<1>}, y^{<2>}} \left\lbrace P(y^{<2>} \mid \hat{y}^{<1>}, x) \right\rbrace, \quad \text{where } \hat{y}^{<1>} = \mathop{\arg \max} P (y^{<1>} \mid x)
\end{align}
$$



所以我们不能单独的在每一步都选取最大概率的单词作为翻译的输出和下一步的输入。严格意义上我们需要把每一步中每一个单词输出的概率都记录下来，然后分别计算对应下一步中每个输出单词的概率，以此递推，最后选取概率最大的一组输出，这就是 **greedy search**。然而，若是每一步都记录下每个单词的概率，那么随着翻译语句长度的增加，计算量将以指数级增长，这在实际的系统中是无法接受的，常见的解决办法是做出一个近似的搜索，尝试找出使条件概率最大化的输出，但不保证找出来的 $y$ 使概率最大化，以牺牲一定的精确度换来了计算速度极大地提升。而**集束搜索（beam serach）**就是一种常见的近似方法。



## 3. 集束搜索（Beam search）

#### 3.1 Beam search 算法

beam search 算法可以有效的降低 sequence to sequence 模型的计算复杂度，和 greedy search 中每一步都计算所有单词的概率不同，beam search 中我们定义一个变量 $B$，代表集束宽度，假设 $B=3$，词汇库的总量为 $m = 10000$ ，则使用 beam search 算法的的 seq2seq 模型步骤如下：

- 编码网络对输入进行编码，得到解码网络的初始输入 $a^{<0>}$
- 将  $a^{<0>}$ 输入到解码的 RNN 网络，第一步的输出为词汇库中10000个单词每个出现在第一位的概率，选取出其中概率最大的3个单词，记为 $y^{<1>}_1, y^{<1>}_2, y^{<1>}_3$，其概率分别为 $P_1(y^{<1>} \mid x), P_2(y^{<1>} \mid x), P_3(y^{<1>} \mid x)$
- 将这3个输出分别作为下一步的输入，得到不同输入下的 softmax 结果，此时可以得到30000 个输出，由贝叶斯公式 $P(y^{<1>}, y^{<2>} \mid x) = P(y^{<2>} \mid y^{<1>}, x) * P(y^{<1>} \mid x)$ 可以得到30000个不同输出的联合概率，从这30000个输出组合中再选出概率最大的3个组合，记为 $(y^{<1>}y^{<2>})_1, (y^{<1>}y^{<2>})_2, (y^{<1>}y^{<2>})_3$ ，同样记录下其概率 $P_1(y^{<1>}, y^{<2>} \mid x), P_2(y^{<1>}, y^{<2>} \mid x), P_3(y^{<1>}, y^{<2>} \mid x)$ 
- 以此循环，每次都只取前一层概率最大的3种组合作为下一层的输入，直到翻译结束



![][1]

ps: [图片来源](https://d2l.ai/chapter_recurrent-neural-networks/beam-search.html)



#### 3.2 Beam search 的改进



#### 3.3 Beam search 的误差分析



## 4. Bleu Score

 



## 5. 注意力模型（Attention model）



## 6. 语音识别









[1]: https://res.cloudinary.com/bxy1994/image/upload/v1570635529/DL_coursera/BeamSearch.png