---
layout: post
title: Machine Learning by Andrew (四) 神经网络
category: Coursera ML 笔记
tags: 
  - machine learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、神经网络模型

在分类问题中，逻辑回归能够很好的去解决一些线性可分的模型，但对于非线性的模型，或者是一些难以进行特征提取的任务，例如对一张图片中的物体进行类别识别，逻辑回归就难以完成任务了。此时，收到人脑神经元细胞工作原理的启发，人们发明了额神经网络模型，能够针对此类问题进行有效的分类。神经网络模型也为后续的卷积神经网络（CNN），递归神经网络（RNN）等深度学习的应用做好了铺垫。其模型结构如下：

![](https://res.cloudinary.com/bxy1994/image/upload/v1546604179/ML_coursera/NN_model_krcmvy.png) 

如上图所示，第一层为输入层（Input Layer），最后一层为输出层（Output Layer），中间为隐藏层（Hidden Layer）。层与层之间，加入偏执量后，先通过一次线性运算，然后在导入激活函数进行非线性运算，其中各个变量定义如下：
> $x_i$： 第 $i$ 个输入  
> $z_i^{(l)}$： 第 $l$ 层的第 $i$ 个输出（线性运算的结果）  
> $a_i^{(l)}$ ：第 $l$ 层的第 $i$ 个激活值（非线性运算的结果）  
> $\Theta_{j,i}^{(l)}$： 从第 $l$ 层到第 $l+1$ 层的线性运算中，上一层的第 $i$ 个节点值到下一层中第 $j$ 个节点值的参数  
> $g(z)$： 激活函数

综上，神经网络的运算如下：

$$
a_1^{(2)} = g(\Theta_{1,0}^{(1)} x_0 + \Theta_{1,1}^{(1)} x_1  + \Theta_{1,2}^{(1)} x_2  + \Theta_{1,3}^{(1)} x_3)   \\
a_2^{(2)} = g(\Theta_{2,0}^{(1)} x_0 + \Theta_{2,1}^{(1)} x_1  + \Theta_{2,2}^{(1)} x_2  + \Theta_{2,3}^{(1)} x_3 )   \\
a_3^{(2)} = g(\Theta_{3,0}^{(1)} x_0 + \Theta_{3,1}^{(1)} x_1  + \Theta_{3,2}^{(1)} x_2  + \Theta_{3,3}^{(1)} x_3 )
$$

矢量形式如下：

$$
z^{(l+1)} = \Theta^{(l)} a^{(l)} \\
a^{(l+1)} = g(z^{(l+1)})
$$

与传统的机器学习算法不同，神经网络算法不需要先行寻找特征，而是将其交给网络自己来学习，当网络结构达到一定的复杂度，数据量足够且计算机的运算能力能够支持此类运算时，神经网络可以取得意想不到的效果。而对于多分类问题，神经网络不需要像逻辑回归一样去计算 $k$ 个模型，而是增加输出层的结点，通常，输出最接近1的节点所对应的类别便是分类的结果。

## 二、损失函数

对于神经网络模型，我们增加如下定义：
> 训练集为 $\{ (x^{(1)}, y^{(1)}), \;(x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)}) \}$  
> $L$是网络的总层数， $s_l$是第$l$层的节点个数  
> 对于二分类问题 $y = 0 \; or \; 1$， 对于多分类问题 $y \in \mathbb{R}^K$ (e.g. $y = [0 \; 1 \; 0 \; 0 ]^T$)  
> $\delta_j^{(l)}$：第$l$层第$j$个结点的误差

对于神经网络的损失函数，我们可以讲输出层看成是$K$个逻辑回归，所以损失是这$K$个输出的损失的和，并加上正则项：

$$
J(\Theta) = -\frac{1}{m}  \sum_{i=1}^m\sum_{k=1}^K \Big[ y_k^{(i)}log(h_{\Theta}(x^{(i)}))_k+ (1-y_k^{(i)})log(1-h_{\Theta}(x^{(i)}))_k  \Big] \\
+ \frac{\lambda}{2m}\sum_{l=1}^{L-1} \sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\Theta_{ji}^{(l)})^2
$$

## 三、反向传播

在神经网络中，我们通过反向传播来计算每个参数的梯度：

令 $\Delta_{ij}^{(l)} = 0$ for all $i,j,l$   
For i = 1 to m :  
&emsp;&emsp; 令 $a^{(1)} = x^{(1)}$  
&emsp;&emsp;　Forward Propagation:  
&emsp;&emsp;&emsp;&emsp;　$z_{j+1} = \Theta^{(j)}  a^{(j)}, \quad a_{j+1} = g(z_{j+1})$  
&emsp;&emsp;　Back Propagation;  
&emsp;&emsp;&emsp;&emsp; $\delta^{(l)} = ((\Theta^{(l)})^T \delta^{(l+1)}) \times a^{(l)} \times (1-a^{(l)})) \; for \; (i = L-1, L-2, ..., 2) $，这里的 $\times$ 为向量的数乘，即元素依次相乘  
&emsp;&emsp;&emsp;&emsp; $\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l+1)} \quad (\Delta^{(l)} := \Delta^{(l)} + \delta^{(l+1)}(a^{(l)})^T ) $  

 这里的 $\Delta_{ij}^l$ 可以看作是参数 $\Theta_{ij}^l$ 关于所有样本的总cost，代入梯度下降算法时，需要取其均值并加上正则项：

$$
 \frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)  := \frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)}, \quad j \neq 0 \\
 \frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta)  := \frac{1}{m} \Delta_{ij}^{(l)}, \;\qquad\qquad j = 0 
$$

## 四、公式推导
在此模型中，我们默认的损失函数是`sigmoid`函数，其函数表达式及导数的表达式为：

$$
g(x) = \frac{1}{1+e^{-x}} \\
g(x)' = \frac{e^{-x}}{(1+e^{-x})^2} = g(x) * (1-g(x))
$$

定义损失函数 $cost(t)$ 以及其对于每一层的输出值 $z_j$ 的导数 $\delta_j^{(l)}$：

$$
cost(t) = - \sum_{k=1}^K \Big[ y_k^{(t)}log(h_{\Theta}(x^{(t)}))_k+ (1-y_k^{(t)})log(1-h_{\Theta}(x^{(t)}))_k \Big] \\
\delta_j^{(l)} = \frac{\partial}{\partial z_j^{(l)}}cost(t)
$$

代入`sigmoid`函数的求导公式，可以得出最后一层的导数：

$$
\delta_j^{(L)} = \frac{\partial}{\partial z_j^{(L)}}cost(t) = a_j^{(L)} - y_j^{(t)} \quad (last \; layer)
$$

再利用链式法则，可以依次求出每一层的导数，这就是反向传播：

$$
\begin{aligned}
\delta^{(l-1)} &= \frac{\partial}{\partial z^{(l-1)}}J(\Theta) \quad (without \; regularization) \\
&= \frac{\partial z^{(l)}}{\partial z^{(l-1)}} \frac{\partial}{\partial z^{(l)}}J(\Theta) \\
&= \frac{\partial (\Theta^{(l-1)}g( z^{l-1})) }{\partial z^{(l-1)}} \delta^{(l)} \\
&= (\Theta^{(l-1)})^T \delta^{(l)} .* a^{(l-1)} .* (1-a^{(l-1)})
\end{aligned}
$$

$\delta_j^{(l)}$ 是损失函数针对于每一层的输出值的导数，通过输出值和参数 $\Theta$ 的关系，可以求出损失函数对于模型参数的导数：

$$
\begin{aligned}
\Delta_{ij}^{(l)} &= \sum_{t=1}^m \frac{\partial}{\partial\Theta_{ij}^{(l)}} cost(t) \quad (without \; regularization) \\
&= \sum_{t=1}^m \frac{\partial z_i^{(l+1)} }{\partial \Theta_{ij}^{(l)}} \frac{\partial}{\partial z_i^{(l+1)}} cost(t) \\
&= \sum_{t=1}^m \frac{\partial (\Theta_{ij}^{(l)}a_j^{(l)}) }{\partial \Theta_{ij}^{(l)}} \delta_i^{(l+1)} \\
&= \sum_{t=1}^m a_j^{(l)}\delta_i^{(l+1)}
\end{aligned}
$$

这里的 $a_j^{(l)},\delta_i^{(l)}$ 对于不同的训练对象有着不同的值， $m$ 为训练对象的数目，由于损失函数的定义里加入了1/m，所以最终的在求梯度的时候也需要除以m

最终：

$$
\frac{\partial}{\partial\Theta_{ij}^{(l)}}J(\Theta) = D_{ij}^{(l)} = 
\begin{cases}
\frac{1}{m} \Delta_{ij}^{(l)} + \lambda \Theta_{ij}^{(l)}, \quad j \neq 0 \\
\frac{1}{m} \Delta_{ij}^{(l)} , \quad j = 0
\end{cases}
$$

在实际运用中，若初始的参数均为0，则隐藏层中每一个神经元所得到的值都是一样，无法得到差异化的体现。通常的做法是采取随机采样来初始化参数：

$$
\Theta = rand(L_{out}, L_{in} + 1) *2 * \epsilon_{init} - \epsilon_{init}
$$

对于随机采样的方差，一种经验的取法是(by Andrew Ng)：

$$
\epsilon_{init} = \frac{\sqrt{6}}{\sqrt{L_{out} + L_{in} }}
$$

