---
layout: post
title: Machine Learning by Andrew (四) 神经网络
category: 笔记
tags: 
  - machine learning
description: 
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

####神经网络模型

在分类问题中，逻辑回归能够很好的去解决一些线性可分的模型，但对于非线性的模型，或者是一些难以进行特征提取的任务，例如对一张图片中的物体进行类别识别，逻辑回归就难以完成任务了。此时，收到人脑神经元细胞工作原理的启发，人们发明了额神经网络模型，能够针对此类问题进行有效的分类。神经网络模型也为后续的卷积神经网络（CNN），递归神经网络（RNN）等深度学习的应用做好了铺垫。其模型结构如下：

![](https://res.cloudinary.com/bxy1994/image/upload/v1546604179/NN_model_krcmvy.png) 

如上图所示，第一层为输入层（Input Layer），最后一层为输出层（Output Layer），中间为隐藏层（Hidden Layer）。层与层之间，加入偏执量后，先通过一次线性运算，然后在导入激活函数进行非线性运算，其中各个变量定义如下：
> $x_i$ 第 $i$ 个输入
> $z_i^{(l)}$ 第 $l$ 层的第 $i$ 个输出（线性运算的结果）
> $a_i^{(l)}$ 第 $l$ 层的第 $i$ 个激活值（非线性运算的结果）
> $\Theta_{j,i}^{(l)}$ 从第 $l$ 层到第 $l+1$ 层的线性运算中，上一层的第 $i$ 个节点值到下一层中第 $j$ 个节点值的参数
> $g(z)$ 激活函数

综上，神经网络的运算如下：
$$
a_1^{(2)} = g(\Theta_{1,0}^{(1)} x_0 + \Theta_{1,1}^{(1)} x_0  + \Theta_{1,2}^{(1)} x_0  + \Theta_{1,3}^{(1)} x_0 )   \\
a_2^{(2)} = g(\Theta_{2,0}^{(1)} x_0 + \Theta_{2,1}^{(1)} x_0  + \Theta_{2,2}^{(1)} x_0  + \Theta_{2,3}^{(1)} x_0 )   \\
a_3^{(2)} = g(\Theta_{3,0}^{(1)} x_0 + \Theta_{3,1}^{(1)} x_0  + \Theta_{3,2}^{(1)} x_0  + \Theta_{3,3}^{(1)} x_0 )  
$$
