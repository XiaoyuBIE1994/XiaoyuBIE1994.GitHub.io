---
layout: post
title: 神经网络基础概念
category: 知识整理
tags:
  - deep learning
description: 
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 检验(Evaluation)

交叉验证
训练集，验证集，测试集

# 特征标准化(Feature Normalization)

避免某些特征所占比重太大
minimax normalization:(0,1)
std normalization:(mean=0,std=1)

# 特征选择(Good Feature)
避免无意义信息
避免重复信息
避免复杂信息

# 激励函数(Activation Function)
神经网络中Layers之间的传递为线性传递，所以需要添加Activation function来解决非线性问题

> **常用激励函数：**
>* 线性整流函数，relu: \\(f(x) = max(0,x)\\) 
>* S函数，sigmoid: \\(f(x) = \frac{1}{1+e^{-x}} \\)
>* 双曲函数，tanh:\\(f(x) = tanh(x) \\)

神经网络层数过多时需要谨慎选择激励函数，需注意梯度消失，梯度爆炸等问题

# 过拟合(Overfitting)

产生原因：参数过多
解决方法：增加数据量，正规化(对过多参数进行惩罚)，Dropout（用于神经网络，以一定概率断掉神经网络）

# 优化器(Optimization)

### 1、随机梯度下降(SGD)
每一次迭代计算mini-batch的梯度，然后进行参数更新

$$
g_t = \bigtriangledown_{\theta_{t-1}} f(\theta_{t-1})
$$

$$
\Delta_{\theta_t} = -\eta * g_t
$$
其中\\(\eta\\)是学习速率，\\(g_t\\)是当前batch的梯度

> **优点：**
>* 结构简单
> **缺点：**
>* Learning rate 比较难选择
>* 容易收敛至局部最优

### 2、Momentum
Momentum为模拟物理中动量的概念，通过动量的积累来代替梯度

$$
m_t = \mu * m_{t-1} + g_t
$$

$$
\Delta_{\theta_t} = - \eta * m_t
$$

其中，\\(\mu\\)是动量因子

> **优点：**
>* 下降初期，选取较大的\\(\mu\\)，可获得较大的加速度
>* 下降后期，在局部最小值处来回震荡时，\\(\mu\\)可以使更新幅度增大，跳出陷阱
>* 梯度方向改变时，\\(\mu\\)可以减小更新

### 3、Nesterov
Momentum中，\\(\Delta_{\theta_t} = - \eta * \mu *  m_{t-1} - \eta * g_t\\)，动量并没有影响到梯度。而Nesterov则是在求梯度做一个矫正:

$$
g_t = \bigtriangledown_{\theta_{t-1}} f(\theta_{t-1}- \eta * \mu *  m_{t-1})
$$

$$
m_t = \mu * m_{t-1} + g_t
$$

$$
\Delta_{\theta_t} = - \eta * m_t
$$


### 4、AdaGrad
AdaGrad是对学习速率进行了一定的约束，属于自适应学习率的方法

$$
n_t = n_{t-1} + g_t^2
$$

$$
\Delta_{\theta_t} = -\frac{\eta}{\sqrt{n_t+\epsilon}} *g_t
$$

此处对\\(g_t\\)有一个约束项\\(n_t\\)，\\(\epsilon\\)为一个很小的常量用以保证分母非0。

> **优点：**
>* 前期\\(g_t\\)较小时可以放大梯度
>* 后期\\(g_t\\)较大时可以约束梯度
>* 适合处理稀疏梯度
> **缺点：**
>* 比较依赖人工设置的一个全局学习速率\\(\eta\\)，设置的过大时对梯度的调节太大
>* 中后期\\(n_t\\)过大，容易使训练提前结束

### 5、Adadelta
Adadelta是对AdaGrad的一种扩展，AdaGrad会累加所有的梯度平方，而Adadelta只累加固定大小的项，而且并不直接储存储存，而是近似计算平均值:

$$
n_t = \nu*n_{t-1} + (1-\nu)*g_t^2
$$

$$
\Delta_{\theta_t} = -\frac{\eta}{\sqrt{n_t+\epsilon}} *g_t
$$

> **优点：**
>* 训练初中期，加速效果很快
> **缺点：**
>* 训练中后期容易在局部最小附近反复抖动

### 6、RMSProp

RMSProp属于Adadelta的特例，取\\(\nu=0.5\\)

$$
E|g^2|_t = 0.5*E|g^2|_{t-1} + 0.5*g_t^2
$$

$$
RMS|t|_t = \sqrt{E|g^2|_t+\epsilon}
$$

$$
\Delta_{\theta_t} = -\frac{\eta}{RMS|t|_t} * g_t
$$

> **优点：**
>* 适合处理非平稳目标，对RNN效果很好

### 7、Adam
Adam的本质是带有动量项的RMSProp

$$
m_t = \mu * m_{t-1} + (1-\mu)*g_t
$$

$$
n_t = \nu * n_{t-1} + (1-\nu) * g_t^2
$$

$$
\hat{m}_t = \frac{m_t}{1-\mu^t}
$$

$$
\hat{n}_t = \frac{n_t}{1-\nu^t}
$$

$$
\Delta \theta_t = -\frac{\hat{m}_t}{\sqrt{\hat{n}_t}+\epsilon} * \eta
$$

其中，\\(m_t\\),\\(n_t\\)可看做是对期望\\(E|g_t|\\)，\\(E|g_t|^2\\)的估计，\\(\hat{m}_t\\),\\(\hat{n}_t\\)是对\\(m_t\\),\\(n_t\\)的无偏矫正

> **优点：**
>* 结合了Adagrad善于处理稀疏梯度的优点和RMSProp善于处理飞平稳目标的优点
>* 对内存需求较小
>* 为不同的参数计算不同的自适应学习率
>* 适用于大多数非凸优化，大数据集和高维空间

### 8、Adamax
Adamax对Adam的学习率上线提供了一个更简单的范围：

$$
n_t = max(\nu * n_{t-1},|g_t|)
$$

$$
\Delta \theta_t = -\frac{\hat{m}_t}{\hat{n}_t+\epsilon} * \eta
$$

> **优点：**
>* Adamax的学习率边界范围更简单

### 8、Nadam
Nadam类似于带有Nesterov动量项的Adam：

$$
\hat{g}_t = \frac{g_t}{1-\prod \mu_i}
$$

$$
m_t = \mu_t * m_{t-1} + (1-\mu_t)*g_t
$$

$$
n_t = \nu * n_{t-1} + (1-\nu) * g_t^2
$$

$$
\hat{m}_t = \frac{m_t}{1-\prod \mu_i}
$$

$$
\hat{n}_t = \frac{n_t}{1-\nu^t} \bar{m}_t = (1-\mu_t) * \hat{g}_t + \mu_{t+1}\hat{m}_t
$$

$$
\Delta \theta_t = -\eta * \frac{\bar{m}_t}{\sqrt{\hat{n}_t}+\epsilon}
$$


# 不均衡数据(Imbalanced Data)

获取更多数据
更换评测方法：Confusion matrix => Precision & Recall => F1 Score
重组数据
修改算法

# 批标准化(Batch Normalization)
在层与层之间进行Normalization，增加数据传递的效率

# 正规化手段(Regularization)

$$
J(\theta) + [y_{\theta}(x)-y]^2+\sum f(\theta_i)
$$

$$
L1: f(\theta_i) = | \theta_i|
$$

$$
L2: f(\theta_i) = \theta_i^2
$$

# Reference
[1] [深度学习最全优化方法总结比较（SGD，Adagrad，Adadelta，Adam，Adamax，Nadam)](https://zhuanlan.zhihu.com/p/22252270) 
