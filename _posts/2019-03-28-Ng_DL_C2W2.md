---

layout: post
title: Deep Learning by Andrew (2-2) 优化算法
category: Coursera DL 笔记
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



机器学习的应用高度的依赖经验，需要不断地重复实验来优化模型，如果能够找到合适的优化算法，会大大的加快学习的过程，提高工作效率。



## 1. Mini-batch gradient descent

在对数据集进行训练的时候，如果采用批量梯度下降的方式，每一次梯度下降都要计算所有的训练数据的损失，这种情况下计算的效率会很低，所以人们提出了 Stochastic gradient descent(SGD)和 Mini-batch gradient descent。前者是将数据集打乱，然后每一份数据都计算一次梯度下降，这样迭代的速度将会大幅度提升，但由于没有考虑到整体的数据，所以实际上模型的梯度下降方向会发生偏移，导致收敛变慢。Mini-batch gradient descent则是两个权衡后的结果，将数据分成 N 组，每一组批量计算损失并进行梯度下降，这样既提升了迭代的速度，也一定程度上考虑到了下降方向的准确性，具体的步骤为：

- 先划分好N个 mini-batch

  `for t in range(N)` ，循环迭代

- 每一小组进行前向传播

- 计算损失函数

- 反向传播计算梯度

- 参数更新



由于Mini-batch的训练过程中，每一次epoch都训练的是不一样的数据组，所以他的 cost 曲线会如下图所示，相比于批量梯度下降，Mini-batch的方法会存在一定的噪声，但由于是随机采样得到的 mini-batch，所以可以认为每一个batch都拥有同样的分布，所以 cost 在整体上是呈现下降趋势的

![][1]



|                             | 缺点                     | Mini-batch 的改进 |
| --------------------------- | ------------------------ | ----------------|
| Batch gradient descent      | 训练速度太慢             | 分成若干组加速迭代   |
| Stochastic gradient descent | 方向缺失，没有利用向量化 | 向量化加速，下降方向更贴近真实梯度|



**Notation**

1. 数据量较小（小于2000）时，直接利用批量梯度下降比较合适
2. Mini-batch 的 size 一般选在64到512之间



## 2. 指数加权滑动平均

为了理解后续提出的几种优化算法，这这一节解释了指数加权滑动平均的原理，本质上是以较小的计算开销来对数据进行平滑处理。如下图所示：



![][2]

蓝点显示的是每一天的气温，可以看到数据分布的抖动非常大，如果我们想要将他进行平均，如下图所示，黄、红、绿分别代表2天、10天和50天的平均

![][3]



我们可以直接依次计算平均值，得到如上的曲线，但当数据的维度非常大是，直接计算会很浪费储存空间，于是就有了**指数加权滑动平均**：


$$
V_0 = 0 \\
V_1 = \beta V_0 + (1-\beta)\theta_1 \\
\vdots \\
V_t =  \beta V_{t-1} + (1-\beta)\theta_t
$$


实际上，这里有：


$$
V_t = (1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2(1-\beta)\theta_{t-1} + \cdots
$$


我们可以近似的认为 $V_t$ 是前面  $({1}/{1-\beta})$ 组数据的平均，粗略的证明如下：


$$
\begin{align}
& \epsilon = 1 - \beta \\
\Rightarrow \quad & ln(1-\epsilon) \approx - \epsilon  \quad (\text{等价无穷小}) \\
\Rightarrow \quad & (1-\epsilon)^{1/\epsilon} \approx 1 \\
\Rightarrow \quad & \beta^{1 / (1 - \beta)} \approx \frac{1}{e}
\end{align}
$$


也就是在  $({1}/{1-\beta})$ 项之前的数据，由于 $\beta$ 的累乘导致系数过小，而被忽略了，所以近似于前   $({1}/{1-\beta})$  项的平均

**Bias 修正**

在指数加权平均算法中，由于前几项的估计中并没有前置的数据来进入平均，最最开始的几项会和直接计算平均值产生一些偏差，如下图所示，绿色曲线是直接平均所得，紫色曲线则是采用指数加权平均得到的结果



![][4]



最开始的这一块偏差称为 **bias**， 这一阶段也称为**预热阶段** ，为了消除 **bias**  的影响，我们可以引入修正系数：


$$
V_t = \frac{V_t}{1-\beta^t}
$$


- t 较小时，会将结果放大，消除 bias
- t 较大时，$1-\beta^t \approx 1$，对结果几乎不产生影响





## 3. Momentum



Mini-batch SGD 虽然避免了过于庞大的数据维度，又利用了矢量算法进行加速，但由于数据的随机选择，下降的过程中任免不了来回的振荡，降低了效率，如下图所示

![][5]



Momenta 是一种带有动量的优化算法，如下所示。其形式非常类似上节所介绍的指数加权滑动平均，可以理解成给参数一个加速度，让其更快速的下降



$$
\begin{align}
& V_{dw} = \beta V_{dw} + (1 -\beta) dW \\
& V_{db} = \beta V_{db} + (1 -\beta) db \\
& dW = W - \alpha V_{dW} \\
& db = b - \alpha V_{db}
\end{align}
$$



**Notation**

1. 一般来说 $\beta = 0.9 $
2. 下降初期，使用上一次参数更新，下降方向一致，取较大的 $\beta$ 可以进行很好的加速
3. 下降中后期，由于下降的幅度是之前几步的平均，所以即使在梯度为零处也不会立马停下来，这样可以让优化算法有概率冲出鞍点，避免收敛到局部极小值
4. 当梯度改变方向时， 动量的存在能够减小更新，避免过多的振荡



### 4. RMSprop

RMSprop也被称为均方根传递，如下公式所示：


$$
\begin{align}
& s_{dW} = \beta s_{dW} + ( 1 - \beta) dW^2 \\
& s_{db} = \beta s_{db} + ( 1 - \beta) db^2 \\
& W = W - \alpha \frac{dW}{\sqrt{s_{dW}}+ \epsilon} \\
& b = b - \alpha \frac{db}{\sqrt{s_{db}}+ \epsilon}
\end{align}
$$
**Notation**

1. $\epsilon$ 是一个非常小的值（比如 $10^{-8}$），用于避免零除
2. RMSprop能够加速的原理是， 当梯度振动过大时，$s$ 会变得很大，减少了梯度，从而对振荡产生了一定的抑制效果
3. RMSprop 依赖学习率 $\alpha$ 的选择
4. RMSprop 适合处理非平稳目标，对RNN效果很好



## 5. Adam

Adam 算法是 Momentum 和 RMSprop 的结合，是目前运用最广泛的优化算法，算法如下


$$
\begin{matrix}
& V_{dw} = \beta_1 V_{dw} + (1 -\beta_1) dW & V_{db} = \beta_1 V_{db} + (1 -\beta_1) db \\
& s_{dW} = \beta_2 s_{dW} + ( 1 - \beta_2) dW^2 & s_{db} = \beta_2 s_{db} + (1 - \beta_2) db^2 \\
& V_{dw}^{corr} = V_{dw} / (1-\beta_1^t) & V_{db}^{corr} = V_{db} / (1-\beta_1^t) \\
& s_{dw}^{corr} = s_{dW} / (1-\beta_2^t) & s_{db}^{corr} = s_{db} / (1-\beta_2^t) \\
& W = W - \alpha \frac{V_{dW}^{corr}}{\sqrt{s_{dw}^{corr}}+ \epsilon} &  b = b - \alpha \frac{V_{db}^{corr}}{\sqrt{s_{db}^{corr}}+ \epsilon}
\end{matrix}
$$


**Notation**

1. 除以 $1 - \beta^2$ 是为了做 Bias 的修正
2. 同时引入了动量和均方根修正，
3. 学习率 $\alpha$ 依然是非常重要的超参，需要时常去调整
4. 通常推荐 $\beta_1 = 0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$



## 6. 学习率衰减

另外一种加速学习效率的方法是对学习速率进行衰减，尤其是使用了mini-batch的方法时，随着损失逐渐的降低，由于采样的随机性，若采用固定的学习速率，算法会在极小值处随机的浮动而不会收敛，所以为了加速算法的收敛，我们需要随着算法的学习逐渐降低学习速率，以下是若干种学习率衰减的例子：



$$
\alpha = \frac{1}{1 + \text{decay-rate} * \text{epoch-rate}} \alpha_0 \\
\alpha = 0.95^{\text{epoch-num}}\alpha_0  \\
\alpha = \frac{k}{\sqrt{\text{epoch-num}}} \alpha_0 \\
\vdots
$$



## 7. 局部最小值与鞍点

神经网络的参数非常的庞大，其参数的优化，通常不属于凸优化过程，也就是会存在着许多的局部最小值和鞍点，在高维数据中尤为常见，如下图所示



![][6]



局部最小值的存在会让只考虑梯度的算法很容易收敛到局部极值点。而鞍点的存在，会因为算法处于临近鞍点时，由于坡非常的平缓，所以算法需要很长的时间来到达鞍点，然后继续下降，甚至有可能会收敛在鞍点处。这也就是为什么我们要使用例如 Adam 的优化算法的原因










## 参考

1. [深度学习最全优化方法总结比较](<https://zhuanlan.zhihu.com/p/22252270>)





[1]: <https://res.cloudinary.com/bxy1994/image/upload/v1553723536/DL_coursera/Mini-batch.jpg>
[2]: <https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average1.png>
[3]: <https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average2.png>
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average3.png
[5]: <https://res.cloudinary.com/bxy1994/image/upload/v1553808986/DL_coursera/gradient_descent.png>
[6]: <https://res.cloudinary.com/bxy1994/image/upload/v1553811886/DL_coursera/local_optima.png>