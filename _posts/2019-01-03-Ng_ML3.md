---
layout: post
title: Machine Learning by Andrew(三) 过拟合问题
category: 笔记
tags: 
  - machine learning
description: 
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、过拟合问题
在分类和回归问题中，如果我们选取了太多的特征，学习到的模型可能会非常好的符合训练集上的数据，但在测试集上却变现较差，也就是泛化能力较差，这种问题称之为**过拟合**，反之若特征选取的太少，在训练集和测试集上都无法取得很好的效果，则称之为**欠拟合**，如下图所示：
![](https://res.cloudinary.com/bxy1994/image/upload/v1546549648/lr_overfitting_ywwygg.png) 

图一中我们只选取了2个特征，无法很好的拟合数据，属于欠拟合，图三中我们选取了5个特征，虽然非常好的吻合了训练数据，但很显然在测试集上会表现较差，反之图二中的拟合情况正是我们需要的。当我们遇到过拟合的情况时，我们通常会：
> 减少特征的数量
> 加入正则项（保留所有的特征但减少特征的参数值，在模型拥有大量特征的时候能很好的work）

## 二、线性回归中的正则项
对于线性回归，我们加入如下正则项：

$$
J(\theta) = - \frac{1}{m} \sum_{i=1}^{m} (y^{(i)}-h_{\theta}(x^{(i)}))^2 + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
$$
  
梯度下降算法变为：

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x_0^{(i)} \\
\theta_j := \theta_j - \alpha\left[ \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)} + \frac{\lambda}{m}\theta_j \right]
$$

对于解析解：  

$$
\theta = (X^TX + \lambda \cdot L)^{-1} X^T y \\
where \; L = \left[ \begin{matrix} 0 & & & & \\ & 1 & & & \\  & & 1 & & \\   & & & \ddots & \\  & & & & 1\\ \end{matrix} \right]
$$



## 三、逻辑回归中的正则项
同样，对于逻辑回归，我们也加入正则项：  

$$
J(\theta) = - \left[\frac{1}{m} \sum_{i=1}^{m} y^{(i)}log\; h_{\theta}(x^{(i)}) + (1-y^{(i)})log (1-h_{\theta}(x^{(i)})) \right] + \frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
$$

梯度下降算法变为：  

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x_0^{(i)} \\
\theta_j := \theta_j - \alpha\left[ \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)} + \frac{\lambda}{m}\theta_j \right]
$$
