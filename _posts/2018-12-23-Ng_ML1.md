---
layout: post
title: Machine Learning by Andrew (一) 线性回归
category: 笔记
tags: 
  - machine learning
description: 
---

对于机器学习，我的理解是我们拥有一组数据，我们希望对这一组数据进行分析，提取出我们感兴趣的信息。对于不同的任务，我们会去建立不同的模型，每个模型有对应的参数，这些参数在最开始是未知的。通过对这一组数据进行学习，我们可以得到一个我们认为最优化的模型参数。在优化的过程中，每一个数据可以表示为 **x**，这是一个向量，向量中的不同变量称之为数据的特征。当每一个数据，或者每一组特征都有一个标签(label)时，我们称为**监督学习**，而没有输出时，我们称为无监督学习。对于监督学习，如果模型的输出是离散的，有限的，我们可以认为每一个不同的输出代表这不同的类别，此时的机器学习问题称之为**分类问题**，而当数据的输出是连续的，无线的时候，通常我们是希望通过已知的数据及其输出去预测未知的数据的输出，此时我们称之为**回归问题**。

线性回归可以理解成用一条直线去拟合数据的分布，属于回归问题，用一组二维的点进行直线拟合就是一种线性回归。

## 一、单变量的线性回归
首先，我们考虑最简单的线性回归问题，输入只有一个变量x，label是y，假设有m组训练数据，则我们的训练集合是:

$$
(x_1, y_1),\; (x_2, y_2),\; \dots (x_m, y_m) 
$$

此时的模型为:

$$
h_{\theta} = \theta_0 + \theta_1x
$$

既然我们需要找到最优化的模型参数，那我们就需要定义模型好坏的衡量标准，也就是损失函数。损失函数的值越大，则说明模型越差，反之则说明模型越好。在直线拟合的问题中，我们将每一个训练集中的点到模型直线的y方向上的距离的平方和定义为**损失函数**:

$$
J(\theta_0,\theta_1) = \frac{1}{2m} \sum^{m}_{i=1} (h_{\theta}(x^{(i)})-y^{(i)})^2 
$$

此时，我们优化的目标，便是最小化损失函数:

$$
Goal: \quad {minimize}\; J(\theta_0, \theta_1)
$$

在机器学习的问题中，我们通常不会去定义问题的解析解，因为这样往往需要巨大的计算量，或者根本就不存在解析解。反而，我们会通过迭代的方法，每一次迭代都去减少一部分我们的损失函数，在若干次的迭代后，我们就可以得到一个拥有足够小的
损失函数的模型 显然，当我们希望通过迭代的方法来最小化损失函数的时候，我们沿着损失函数的梯度的反方向，能够最好的达到我们的目标，这一算法称为**梯度下降**:

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial\theta_j}J(\theta_0,\theta_1) \quad j=0,1
$$

在单变量线性回归的模型中，梯度求解为:

$$
j=0:  \frac{\partial}{\partial\theta_0}J(\theta_0,\theta_1)  = \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \\
j=1:  \frac{\partial}{\partial\theta_1}J(\theta_0,\theta_1)  = \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)})\cdot x^{(i)} 
$$

梯度下降算法为:

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \\
\theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)}
$$

在这里，我们每一次迭代都使用了所有的训练数据，所以这一梯度下降算法称之为**批量梯度下降(Batch Gradient Descent)**

## 二、多变量的线性回归
当我们的每一组样本，都有多个输入，也就是多个特征时，问题就变成了多变量的线性回归，此时我们有如下的约定:
> n 每一组样本中特征的数量
> m 在学习问题中样本的总数
> $x^{(i)}$ 第i个训练样本
> $x^{(i)}_j$ 第i个训练样本中的第j个特征

此时，我们的模型变成了：

$$
h_{\theta} = \sum_{j=0}^m\theta_jx_j = \theta^Tx \; (x_0 = 1)
$$

损失函数的定义为:

$$
J(\theta) = \frac{1}{2m} \sum^m_{i=1} (h_{\theta}(x^{(i)})-y^{(i)})^2
$$

梯度下降算法为:

$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)})
$$

$$
\theta_j := \theta_j - \alpha \frac{1}{m}\sum_{i=1}^{m}  (h_{\theta}(x^{(i)})-y^{(i)}) \cdot x^{(i)}_j \quad j \ge 1
$$

当我们面对多变量的线性回归问题时，我们有时会遇到某几个特征的值远大于其他的特征，这样在执行梯度下降算法时，这几个特征所在的方向占据了太大的权重，为了时梯度下降算法更好的执行，我们通常会对数据进行一定的缩放:

$$
x_j = \frac{x_j - mean}{var}
$$

在机器学习中，我们还遇到另外一个问题，也就是超参$\alpha$的设置，当$\alpha$过小时，损失函数收敛的过慢，当$\alpha$过大时损失函数$J(\theta)$可能不会随着迭代次数的增加而减小，这个时候，我们通常会选取多个$\alpha$
来进行尝试，比如:

$$
0.001, 0.003,0.01,0.03,\dots
$$

对于一些样本，可能并没有良好的线性性质。在这种情况下，我们可以通过增加一些特征，比如$x^2$, $\sqrt{x}$来构建非线性的回归，但这种方法需要手动选择特征，实际操作难度较大

## 三、 线性回归的解析解

对于线性回归，我们其实可以得到最佳的解析解，我们构建如下方程:

$$
X = \left[
\begin{matrix}
—— (x^{(1)})^T —— \\
—— (x^{(2)})^T —— \\
\vdots \\
—— (x^{(m)})^T —— \\
\end{matrix}
\right]
$$

$$
\theta = \left[
\begin{matrix}
\theta_1 \\
\theta_2 \\
         \vdots \\
\theta_m \\
\end{matrix}
\right] \quad 
Y = \left[
\begin{matrix}
y^{(1)} \\
y^{(2)} \\
\vdots \\
y^{(m)} \\
\end{matrix}
\right] 
$$

$$
\theta = arg\; min (X\theta - Y) ^2
$$

矩阵求导可得:

$$
\theta = (X^TX)^{-1}X^TY
$$

针对梯度下降法和解析法，我们可以进行如下对比：
 **Gradient Descent(梯度下降法)**
> 需要选择学习速率$\alpha$  
> 需要多次迭代  
> 当n非常大的时候依然能够很好的工作  

 **Normal Function(解析法)**
 > 不需要选择学习速率$\alpha$  
 > 不需要迭代，一次即可求解成功  
 > 需要计算$ (X^TX)^{-1} $，计算复杂度为$ O(n^3) $  
 > 当n非常大的时候，计算耗时比梯度下降法要长很多  
 
 **总结**
 对于线性回归问题，当n比较小的时候，我们使用解析法一次求解，当n很大的时候(e.g. n = 1,000,000)，使用梯度下降法能够更好的work