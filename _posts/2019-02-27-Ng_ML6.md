---
layout: post
title: Machine Learning by Andrew (六) 支持向量机
category: 笔记
tags: 
  - machine learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>

## 一、SVM与LR的关系



![][1]



上图所示是logistic regression的原理图，以0作为阈值，$\theta^Tx$ 大于0则判断为1，否则判断为0，而不论预测的正确与否，都会得到一个大于0的cost值。与之不同的是，对于SVM，希望的是:


$$
y=1, \theta^Tx \ge 0 \\ 
y=0, \theta^Tx \le 0
$$


所以SVM的损失函数定义为：


$$
J = \mathop{\min}_{\theta} C \sum_{i=1}^{m}[y^{(i)}cost_1(\theta^T x^{(i)}) + (1 - y^{(i)})cost_0(\theta^T x^{(i)})] + \frac{1}{2} \sum_{i=1}^n \theta_j^2
$$


如下图所示，SVM的损失函数相当于是 logistic regression 的损失函数的线性化



![][2]



## 二、 间隔最大化

SVM会使分界超平面离数据的距离最远，下图所示是二维情况下的一种形象化理解，当间隔最大时，由于 $\theta$ 和平面垂直，所以数据投影在 $\theta$ 上的距离最大，也就是 $p^{(i)}$ 最大，而算法需要满足分类条件，所以所需要的 $\parallel \theta \parallel $  就可以最小，对应的 $\frac{1}{2}\mathop{\min}_{\theta}\sum_{i=1}^m \theta_j^2 $

![][3]





## 三、 核技巧

当数据属于线性不可分时，我们除了添加特征的多项式组合之外，在SVM中还可以使用核技巧，计算相似度：


$$
f_1 = similarity(x,l^{(1)}) = exp\left( -\frac{\parallel x-l^{(1)}\parallel ^2}{2\sigma^2} \right)
$$


然后用 $\theta^T f^{(i)}$ 代替 $\theta^T x^{(i)}$ 进行后续的计算

![][4]





PS：Andrew 的课中，对于SVM只做了简单的介绍，很多内容都没有涉及，比如严格的数学推导，软间隔，核技巧的真正意义等等，详细的知识整理等有时间再做



[1]:https://res.cloudinary.com/bxy1994/image/upload/v1551306736/ML_coursera/svm_lr.png
[2]:https://res.cloudinary.com/bxy1994/image/upload/v1551306736/ML_coursera/svm_loss.png
[3]:https://res.cloudinary.com/bxy1994/image/upload/v1551306736/ML_coursera/svm_LargeMargin.png
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1551306999/ML_coursera/svm_kernel.png

