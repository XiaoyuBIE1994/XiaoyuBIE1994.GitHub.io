---
layout: post
title: Machine Learning by Andrew (八) 主成分分析（PCA算法）
category: 笔记
tags: 
  - machine learning
---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 一、目的

主成分分析算法，能将数据的维度降低，而尽可能的保留更多的信息，所以PCA算法主要用于：

- 数据压缩（压缩便是要降低数据的复杂度却尽可能的保留更多的信息）
- 数据可视化（高维的数据没有办法直观的展示，所以可利用PCA将数据降低到三维甚至二维，用以展示）



## 二、算法公式

PCA既要降低更多的维度，又要保留更多的信息，类比于矩阵的变基，我们也可以对数据进行变基操作，分解成若干个正交基，数据在有些基底上分布较为分散，方差较大，说明这一维度包含了较多的信息，反之则表明包含了较少的信息，甚至于这一维度只是数据的噪声。所以， PCA算法的整体思路便是：

- 数据变基
- 计算方差
- 根据需求，保留相应方差较大的基底，删去方差较小的基底

具体的算法如下：

- 训练集为 $x^{(1)},x^{(2)}, \dots , x^{(m)} $
- 特征归一化（避免特征间尺度差异的影响）：$x_j^{(i)} = \frac{x_j^{(i)} - \mu_j}{s_j}$ （$\mu_j$是数据的均值，$s_j$ 可以使数据的最大最小值的差，也可以是数据的方差）
- 计算 covariance matrix：$\Sigma  = \frac{1}{m} \sum_{i=1}^m (x^{(i)}) (x^{(i)})^T$
- 进行奇异值分解： $[U, S, V] = svd(\Sigma)$
- 数据压缩：$Ureduce = U(:, 1:k)$, $z = Ureduce' * x$ 
- 数据还原：$x_{approx} =Ureduce * z $



PS: $S$ 矩阵中所代表的特征值便是代表此维度包含信息的多少，若要保留 99% 的信息，则有 $\frac{\sum_{i=1}^k}{\sum_{i=1}^m}  \ge 0.99$

PS：PCA其实有着更严格的数学证明，留待以后有时间再整理

