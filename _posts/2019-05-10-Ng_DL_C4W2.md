---
layout: post
title: Deep Learning by Andrew (九) 卷积神经网络案例学习
category: 笔记
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>





## 1. 经典的卷积神经网络



 #### 1.1 LeNet-5

LeNet-5的任务最初被用于识别手写数字，在1998年由Yann Le Cun提出，这一网络中并没有用到padding来保持图片大小，并且训练于 $32 \times 32 \times 1$ 的灰度图上

![][1]

网络的主要结构为:

Conv -- Avg pool -- Conv -- Avg pool -- Fc -- Fc -- Softmax

在今天看来这只是一个小型的网络，**参数大概60,000左右**，最后的softmax输出了识别0到9的概率。原始的论文中，激活函数采用了 sigmoid 和 tanh，而不是现在主流的 ReLu， 而且原文在池化层后也进行了非线性的处理。由于当时计算机算力的限制,原始的LeNet-5让每个滤波器处理不同的通道 (现在网络中一个滤波器会处理所有的通道)。同时，原文中还介绍了其他的内容 (如现在已经不常用的图转换网络GTN)，关于LeNet-5的介绍主要集中在第二章和第三章



原论文: [LeCun Y, Bottou L, Bengio Y, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE, 1998, 86(11): 2278-2324.](http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf)



#### 1.2 AlexNet

AlexNet直接对彩色图进行处理,如下:

![][2]

网络的特点如下：

- 与LeNet相似，但网络结构更大，参数更多，表现更加出色，拥有大概**六千万个参数**
- 使用了Relu
- 使用了多个GPUs，每个GPU负责训练一部分训练，GPU之间有通讯（受限于当时GPU的计算力）
- LRN（Local Response Normalization，在某一个层中，对一个像素点的所有通道进行归一化处理后来发现用处不大，丢弃了）



原文比较容易懂，推荐阅读 =， =

原文: [Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[C]//Advances in neural information processing systems. 2012: 1097-1105.](http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf)



#### 1.3 VGG-16

VGG网络的特点是简单，网络中没有设置过多的超参，一直使用大小 $3\times 3$的filter (stride=1, same pidding)，池化层均选用 Max pooling (size $2 \times 2$, stride=2)，VGG-16中的16指的是网络中包含着16个带有参数权重的层 ()$2 \times 2 + 3 \times 3 + 3 = 16$ )



![][3]

网络的特点如下：

- 结构非常简单
- 参数非常多，为**1亿3千8百万**



原文： [Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.](https://arxiv.org/pdf/1409.1556.pdf%20http://arxiv.org/abs/1409.1556.pdf)



## 2. ResNet

深度神经网络非常难以训练，因为会遇到梯度消失和梯度爆炸的问题，而 ResNet 中使用了跳跃连接 (skip connection)，它能让你从一层中得到激活，并且突然传递到下一层，甚至更深的神经网络层。

普通的神经网络模块的前向传播过程为： $a^{[l+1]} = g(w^{[l+1]} a^{[l]} + b^{[l+1]})$

而 ResNet 则在传播过程中增加了一个从 $a^{[l]} $ 到 $z^{[l+2]}$ 的链接，也就是 "short cut/skip connection"，此时的传播函数变为：$a^{[l+2]} = g(z^{[l+2]} + a^{[l]} )$

![][4]

这里传统的神经网络就变成了一个**残差块**，多个残差块堆积就够了**残差网络 (ResNet)**

![][5]

没有残差模块时，由于梯度消失/爆炸 (vanishing/exploding)，当网络层数过高时，训练误差不仅没有继续下降，然而升高了，但加入了残差模块后，即使网络再深，训练误差都能继续减小：

![][6]



[1]: https://res.cloudinary.com/bxy1994/image/upload/v1557498138/DL_coursera/LeNet-5.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1557498138/DL_coursera/AlexNet.jpg
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1557498138/DL_coursera/VGG16.jpg
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1557503349/DL_coursera/ResNet_example.jpg
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1557503064/DL_coursera/ResNet.jpg
[6]: https://res.cloudinary.com/bxy1994/image/upload/v1557503064/DL_coursera/ResNet_compare.jpg