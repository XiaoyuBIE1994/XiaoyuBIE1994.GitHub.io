---

layout: post
title: Deep Learning by Andrew (四) 优化算法
category: 笔记
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



机器学习的应用高度的依赖经验，需要不断地重复实验来优化模型，如果能够找到合适的优化算法，会大大的加快学习的过程，提高工作效率。



## 1. Mini-batch gradient descent

在对数据集进行训练的时候，如果采用批量梯度下降的方式，每一次梯度下降都要计算所有的训练数据的损失，这种情况下计算的效率会很低，所以人们提出了 Stochastic gradient descent(SGD)和 Mini-batch gradient descent。前者是将数据集打乱，然后每一份数据都计算一次梯度下降，这样迭代的速度将会大幅度提升，但由于没有考虑到整体的数据，所以实际上模型的梯度下降方向会发生偏移，导致收敛变慢。Mini-batch gradient descent则是两个权衡后的结果，将数据分成 N 组，每一组批量计算损失并进行梯度下降，这样既提升了迭代的速度，也一定程度上考虑到了下降方向的准确性，具体的步骤为：

- 先划分好N个 mini-batch

  `for t in range(N)` ，循环迭代

- 每一小组进行前向传播

- 计算损失函数

- 反向传播计算梯度

- 参数更新



由于Mini-batch的训练过程中，每一次epoch都训练的是不一样的数据组，所以他的 cost 曲线会如下图所示，相比于批量梯度下降，Mini-batch的方法会存在一定的噪声，但由于是随机采样得到的 mini-batch，所以可以认为每一个batch都拥有同样的分布，所以 cost 在整体上是呈现下降趋势的

![][1]



|                             | 缺点                     | Mini-batch 的改进 |
| --------------------------- | ------------------------ | ----------------|
| Batch gradient descent      | 训练速度太慢             | 分成若干组加速迭代   |
| Stochastic gradient descent | 方向缺失，没有利用向量化 | 向量化加速，下降方向更贴近真实梯度|



**Notation**

1. 数据量较小（小于2000）时，直接利用批量梯度下降比较合适
2. Mini-batch 的 size 一般选在64到512之间



## 2. 指数加权滑动平均

为了理解后续提出的几种优化算法，这这一节解释了指数加权滑动平均的原理，本质上是以较小的计算开销来对数据进行平滑处理。如下图所示：



![][2]

蓝点显示的是每一天的气温，可以看到数据分布的抖动非常大，如果我们想要将他进行平均，如下图所示，黄、红、绿分别代表2天、10天和50天的平均

![][3]



我们可以直接依次计算平均值，得到如上的曲线，但当数据的维度非常大是，直接计算会很浪费储存空间，于是就有了**指数加权滑动平均**：


$$
V_0 = 0 \\
V_1 = \beta V_0 + (1-\beta)\theta_1 \\
\vdots \\
V_t =  \beta V_{t-1} + (1-\beta)\theta_t
$$


实际上，这里有：


$$
V_t = (1-\beta)\theta_t + \beta(1-\beta)\theta_{t-1} + \beta^2(1-\beta)\theta_{t-1} + \cdots
$$


我们可以近似的认为 $V_t​$ 是前面  $({1}/{1-\beta})​$ 组数据的平均，粗略的证明如下：


$$
\begin{align}
& \epsilon = 1 - \beta \\
\Rightarrow \quad & ln(1-\epsilon) \approx - \epsilon  \quad (\text{等价无穷小}) \\
\Rightarrow \quad & (1-\epsilon)^{1/\epsilon} \approx 1 \\
\Rightarrow \quad & \beta^{1 / (1 - \beta)} \approx \frac{1}{e}
\end{align}
$$


也就是在  $({1}/{1-\beta})$ 项之前的数据，由于 $\beta$ 的累乘导致系数过小，而被忽略了，所以近似于前   $({1}/{1-\beta})$  项的平均

**Bias 修正**

在指数加权平均算法中，由于前几项的估计中并没有前置的数据来进入平均，最最开始的几项会和直接计算平均值产生一些偏差，如下图所示，绿色曲线是直接平均所得，紫色曲线则是采用指数加权平均得到的结果



![][4]



最开始的这一块偏差称为 **bias**， 这一阶段也称为**预热阶段** ，为了消除 **bias**  的影响，我们可以引入修正系数：


$$
V_t = \frac{V_t}{1-\beta^t}
$$


- t 较小时，会将结果放大，消除 bias
- t 较大时，$1-\beta^t \approx 1$，对结果几乎不产生影响





[1]: <https://res.cloudinary.com/bxy1994/image/upload/v1553723536/DL_coursera/Mini-batch.jpg>
[2]: <https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average1.png>
[3]: <https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average2.png>
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1553724782/DL_coursera/exp_weight_average3.png