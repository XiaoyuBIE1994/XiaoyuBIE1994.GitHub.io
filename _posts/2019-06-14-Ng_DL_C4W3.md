---

layout: post
title: Deep Learning by Andrew (4-3) 目标检测算法
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>





## 1. 目标检测问题



#### 1.1 定位与分类



图片检测问题可以分为三类：

- 分类问题：判断途中是否为汽车
- 定位分类：判断是否为汽车，并给出具体位置
- 目标检测：检测不同物体并定位



![][1]



对于定位分类问题，卷积网络可能有如下结构：

![][2]

最后的输出不只一个，而是包含：

- 是否有检测对象，$P_c$ (0 or 1)
- 行人，$c_1$ (0 or 1)
- 车，$c_2$ (0 or 1)
- 摩托，$c_3$ (0 or 1)
- 定位框，$b_x, b_y, b_w, b_h$



 在实际的目标定位问题中，我们通常使用如下损失函数：

- 对目标的分类 （$c_1,c_2,c_3$）的 softmax 输出，使用 log likelihood 损失
- 对于定位框（$b_x, b_y, b_w, b_h$）使用均方差
- 对于 $P_c$， 使用逻辑回归（交叉熵），均方差也效果不错



#### 1.2 特征点检测



![][3]

特征点检测并不直接输出 bounding box 和 classification，而是输出一系列特征点的位置。当我们在识别一些特定的目标时，目标总会带有一些特定标签的特征点，比如人脸检测中人的眼角，人体姿态识别中人的关节部位等等，我们可以通过检测这些特征点来实现目标检测



#### 1.3 滑动窗口的目标检测



目标检测算法多基于滑动窗口，训练模型如下:

![][4]

在训练集中对于目标进行适当的裁剪，形成目标（如汽车）基本占据所有空间的小图，并对小图进行标注，目标存在为1，不存在为0。在检测中，选取不同大小的滑动窗口，以固定的步幅遍历整张图像：

![][5]

**缺点：**计算成本巨大，每一个窗口都要放入网络进行卷积计算

**改进：**使用卷积运算来代替窗口的滑动



**卷积层代替全连接层**



![][6]

在之前的课程中讲过，$1 \times 1$ 卷积相当于在一个三维图像的切片上应用了一个全连接的神经网络，所以同样，全连接的神经网络也可以由$1 \times 1$ 的卷积代替，卷积核的数量等于FC层神经元的个数



**滑动窗口的卷积实现**

![][7]



当我们用卷积代替全连接之后，就可以在此基础上实现滑动窗口了。假设我们预先训练好的模型针对的是 $14 \times 14 \times 3$ 的窗口，将此模型应用于 $16 \times 16 \times 3$ 的图片，并且将第一层卷积的步长改为2，在第二层便相当于得到了4副 $10 \times 10 \times 16$ 的特征图，最后得到的 $2 \times 2 \times 4$ 可以认为是4个滑动窗口的输出。同理对于 $28 \times 28 \times 3$ 的图片我们可以得到64个滑动窗口的输出。



**Notation:**

- 卷积避免了重复的运算，大大提升了滑动窗口算法的运算效率
- 此方法的窗口边框定位并不精确，需要进一步优化



## 2. YOLO



#### 2.1 YOLO 算法简介



![][8]



YOLO全称是 You only look once，是由UW的 Joseph Redmon 提出的 one-stage 的目标检测算法，YOLO算法可以快速的让滑动窗口找到 bounding box，具体策略如下：

- 在整幅图像上划分网格 $n \times n$ （图中为了方便展示划分为 $ 3 \times 3$，但实际操作中可以划分的更加精细，比如 $19 \times 19$ ）
- 定义训练标签 $y_i = \lbrack P_c \; b_x \; b_y \; b_h \; b_w \; c_1 \; c_2 \; c_3 \rbrack$ ，标签含义与前面相同
- 采用 Classification and Localization 的方法，分别应用于图像中的每一个网格
- 将子标签合并，最后得到 $n \times n \times 8$ 的输出，通过观测 $n \times n$ 不同位置的输出值，就可以得到目标准确的 bounding box



**Notation:**

- 将目标对象分配到格子的过程中，主要考虑对象的中心点所在的位置，及时对象横跨多个格子，最终只分配到中心点所在的格子
- YOLO算法与1.1节中介绍的定位与分类算法类似，显示的输出边界框且可以有任意的长宽比，并提供精确的坐标，不受滑动窗口算法步幅大小的限制
- YOLO算法是一次卷积实现，所以算法运行效率高，可以实现实时识别
- 此处的YOLO算法仅仅针对网格中只存在一个目标的情况（更加精细的网格可以避免这一问题）



**Boundingbox 的细节:**

![][9]

- 对于每个网格，以左上角为 (0,0)，右下角为 (1,1)
- 中点 $b_x, b_y$ 表示坐标比例，在 $0 \sim 1$ 之间
- 宽高 $b_w, b_h$ 表示 bounding box 同网格的比例，可以大于1
- 此处的做法仅用于理解，实际应用中会更加复杂，比如采用 sigmoid 保证结果在 $0 \sim 1$ 之间，或者使用指数参数来保证结果非负



#### 2.2 交并比 （Intersection over Union）

交并比函数，也就是 Intersection over Union (IoU)，可用于评价目标检测算法的性能，同时也可以用于在目标检测算法中加入其他特征部分

![]][10]

如上图所示，IoU的定义如下：


$$
IoU = \frac{S_{Detect} \cap S_{GroundTruth}}{S_{Detect} \cup S_{GroundTruth}}
$$


实际使用中，阈值可设置为0.5或者更高，一般不会低于0.5





#### 2.3 非极大抑制（Non-max suppresion）

目标检测算法可能会对目标产生多次响应，如下图所示：



![][11]

将问题简化为只做汽车检测，NMS算法的步骤如下：

- 将输出中关于分类的部分去掉，剩下的为 $\lbrack P_c \; b_x \; b_y \; b_h \; b_w  \rbrack$
- 去掉低概率的探测，也就是 $P_c \le 0.6$ 的框
- 当还存在预测出的框时，进行如下循环：
  - 选择 $P_c$ 最大的一个框，设定为 prediction
  - 对于剩下的框，抑制所有和 prediction 相比 $IoU \ge 0.5$ 的框



**Notation:**

- 当进行多分类问题时，可运行多次独立的非极大抑制算法



#### 2.4 锚框（Anchor boxes）

到这里，目标检测的最大问题是一个网格只能检测一个目标，如果想要检测多个，则可以使用**锚框**（Anchor boxes）来解决

![][12]



对于重叠的目标，这些目标的中点有可能会落在同一个网格中，使用我们之前定义的输出:


$$
y_i = \lbrack P_c \; b_x \; b_y \; b_h \; b_w \; c_1 \; c_2 \; c_3 \rbrack
$$



只能指定一个目标，而 Anchor box 是预先定义多个不同形状的 box，我们需要把预测的目标和各个 Anchor box 关联起来，重新定义目标向量：


$$
y_i = \lbrack P_c \; b_x \; b_y \; b_h \; b_w \; c_1 \; c_2 \; c_3 \; P_c \; b_x \; b_y \; b_h \; b_w \; c_1 \; c_2 \; c_3 \rbrack
$$


**Notation:**

- 不使用 Anchor boxes，训练图片时根据对象的中点分配到对应的格子，输出大小为 $n \times n \times 8$
- 使用 Anchor boxes，不仅根据对象中点分配到对应的格子，还根据 IoU 的大小分配到相应的 Anchor box中
- Anchor boxes 无法处理同一格子里目标的数量大于 Anchor 数量的情况，需要额外写挑选机制
- Anchor boxes 无法处理一个格子中多个目标的 Anchor box 相同的情况，需要额外写挑选机制
- 网格划分的比较精细时可以降低以上两种情况出现的概率



#### 2.5 YOLO Implementation

假设进行行人，摩托车和汽车的目标检测，使用了两个 Anchor boxes，步骤如下：

- 对于每一个网格，我们都会得到预测输出的2个bounding boxes，其中一个的 $P_c$ 比较高
- 抛弃低概率 $P_c$ 预测的 bounding box
- 对每个对象（行人，摩托车和汽车）分别使用非极大抑制算法，得到最终的预测边界框

![][13]

## 3.  候选区域算法（region proposals）

**R-CNN：**

R-CNN (Regions with CNN)  会先使用分割算法在图片中选出一些目标区域，在目标区域上放置窗口，从而避免了传统滑动窗口在大量无对象区域进行无用的运算：

![][14]

**更快的算法：**

- R-CNN：给出候选区域，**不使用滑动窗口**，对每个候选区域进行分类识别，输出对象的标签和 bounding box，速度非常慢
- Fast R-CNN：给出候选区域，**使用滑动窗口的卷积**实现去分类所有候选区域，速度比 R-CNN 快，但仍然很慢
- Faster R-CNN：使用**卷积网络**给出候选区域，而不是传统的分割算法，速度更快，但比 YOLO 慢






[1]:https://res.cloudinary.com/bxy1994/image/upload/v1560515452/DL_coursera/Loc_detect.png
[2]:https://res.cloudinary.com/bxy1994/image/upload/v1560515581/DL_coursera/Classification_loc.jpg
[3]:https://res.cloudinary.com/bxy1994/image/upload/v1560516820/DL_coursera/Landmark_detect.jpg
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1560519002/DL_coursera/ObjectDetection_model.jpg
[5]:https://res.cloudinary.com/bxy1994/image/upload/v1560519002/DL_coursera/ObjectDetection_sliding_window.jpg

[6]:https://res.cloudinary.com/bxy1994/image/upload/v1560521278/DL_coursera/ObjectDetection_FCtoConv.jpg
[7]:https://res.cloudinary.com/bxy1994/image/upload/v1560521286/DL_coursera/ObjectDetection_conv.jpg
[8]: https://res.cloudinary.com/bxy1994/image/upload/v1560524066/DL_coursera/YOLO.jpg
[9]:https://res.cloudinary.com/bxy1994/image/upload/v1560524066/DL_coursera/YOLO_details.jpg
[10]:https://res.cloudinary.com/bxy1994/image/upload/v1560526620/DL_coursera/Interaction_Union.jpg
[11]:https://res.cloudinary.com/bxy1994/image/upload/v1560527095/DL_coursera/Non-max_suppresion.png
[12]:https://res.cloudinary.com/bxy1994/image/upload/v1560529963/DL_coursera/Anchor_box.jpg
[13]:https://res.cloudinary.com/bxy1994/image/upload/v1560531430/DL_coursera/YOLO_Implementation.png
[14]: https://res.cloudinary.com/bxy1994/image/upload/v1560531955/DL_coursera/RCNN.jpg