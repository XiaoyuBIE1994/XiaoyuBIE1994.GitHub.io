---


layout: post
title: Deep Learning by Andrew (5-2) 词嵌入（Word Embeddings）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>





## 1. 词嵌入（Word Embeddings）简介

#### 1.1 word repressentation

**one-hot repressentation**

在之前的文本分析中，我们使用 one-hot 表征来表示对应模型中的字典，对应单词的位置用1表示，其余位置为0，这样的表征方式简单明了，易于代入计算，但这一方法只考虑了单个词在向量中的作用，没有考虑词与词之间的联系。由于不同的 one-hot vector 之间的乘积为0，在文本分析时无法有效的利用单词本身的各种属性，所以在NLP问题中，我们考虑使用各类特征来表示单词



![][1]



**featurized repressentation**

不同的单词由不同的特征来描述，从逻辑上也更加符合人类对于语言的认知，同时也降低了大规模语言任务中的计算难度

![][2]



#### 1.2 Word embeddings 的使用

Word Embeddings对不同单词进行了实现了特征化的表示，如下图所示

![][3]

当我们的数据集很小时，可能并不包含 durian（榴莲）和 cultivator（培育家）这两个单词，但如果我们提前做好  word embeddings，我们就可以知道”榴莲“和”苹果“都属于水果，”培育家“和”农民“相似，那我们便不难推断出 durian cultivator 的意思了，这就让我们有了从少量训练集中归纳出没有出现过的实体的能力，也就是泛化能力。

有了词嵌入，我们便有了迁移学习的能力：

- 从大量的文本中进行词迁移学习（1-100B words），或者从网上下载预先训练好的词嵌入模型
- 将词嵌入模型嵌入到我们所需要的小训练集上
- 可选，使用新的标记数据集对模型进行微调（finetune）



**词嵌入和人脸编码**

这两者之间有着有趣的联系。对于人脸识别，我们会训练一个 Siamese 网络来学习，用一个128维的向量来表示不同的面孔，然后比较这些编码用于识别。区别在于，对于人脸识别，我们通过训练一个网络来应对任意的人脸输入，而对于词嵌入，我们则会有一个单独的词汇库

![][4]



#### 1.3 Word embeddings 的性质

通过词嵌入，我们可以将单词转化为更小维度上的 feature vector，除了能够降低表征复杂度外，词嵌入还有一个重要的特性，它还能够帮助实现类比推理，比如 man - women 和 king - queen 之间就存在着相同的内在联系，同为男女对应，表现在 word vertors 中两两vectors之间的向量差是相似的

![][5]



我们使用余弦相似度函数（Cosine Similarity）来定义这一相似程度：


$$
\begin{aligned}
& sim(u,v) = \frac{u^T v}{\parallel u \parallel_2 \parallel v \parallel_2} \\
& u = e_w \\
& v = e_{king} - e_{man} + e_{women} 
\end{aligned}
$$


当然，我们也可以用欧几里得距离:


$$
\parallel u - v \parallel^2 
$$


技术上，这是对**非相似度**的计算，而不是**相似度**的计算。这两者的区别主要在于如何规范化 $u$ 和 $v$ 的长度。因此，嵌入词的一个显著特征是可以学习到类比关系的普遍性，比如说它可以学习男人对应女人之于男孩对应女孩，也能学到当日元是日本的货币时，那么日元对应日本正如卢布对应俄罗斯。

日元是日本的货币，那么日元对应日本正如卢布对应俄罗斯



#### 1.4  Embeddings matrix

当我们要学习一个词嵌入模型时，我们实际上是需要对词汇表学习一个嵌入矩阵 $E$ ，学习完毕后通过嵌入矩阵与对应词的 one-hot 向量相乘，得到词汇的 embeddings：


$$
e_j = E \cdot o_j
$$




## 2 词嵌入学习：Word2Vec & GloVe

词嵌入的学习随着研究的进行变得越来越简单，但这里先从早期比较复杂的算法开始，能够让我们更好的看到词嵌入学习的机理。



#### 2.1 早期的学习算法

以 *I want a glass of orange juice* 为例，其中前面的6个单词为输入，*juice* 为输出，步骤如下：

- 将每个单词的 one-hot 向量与嵌入矩阵相乘，得到相应的 embeddings 向量
- 通过相应的窗口控制影响预测结果的单词数量，如这里我们只选择 *a glass of orange* ，也就是4个输入
- 通过隐藏层和 softmax 层，输入最后的单词的预测，其中模型的参数包括嵌入矩阵 $E$，隐藏从以及 softmax 层的参数 $w^{[1]}, b^{[1]},w^{[2]}, b^{[2]}$
- 可以利用反向传播来进行梯度下降，最大化模型的似然概率
- 当我们进行文本分析的时候，也可以结合上下语境来进行分析，这个时候窗口的选择可以扩展到前后若干个词或者某个、某几个单词



![][6]



#### 2.2 Word2Vec

Word2Vec算法是一种简单高效词嵌入的学习方式，以 *I want a glass of orange juice to go along with my cereal* 为例

**skip-gram 模型**

在这一模型中，我们需要找出一些语境词（Content）和目标词（Target）配对，从而建立监督学习的问题。其中，我们随机选择一个单词作为目标词（例如 *orange*），再以一定的窗口随机选择目标词（例如以前后五个单词为窗口，选中了 *juice* 或者 *glass*）所以我们建立的监督学习模型，就需要预测在给定语境词下，目标词会出现什么，以下是模型的细节：

- 使用一个具有大量词汇的词汇表，如Vocab size = 10000k
- 构建监督学习问题，语境词（C）到目标词（T）的映射 （$x \rightarrow y$）
- 网络的结构为 $o_c(one-hot) \rightarrow E(\text{词嵌入矩阵}) \rightarrow e_c \rightarrow Softmax \text{层} \rightarrow \hat{y} $
- Softmax: $p(t \mid c) = \frac{e^{\Theta_t^T e_c}}{\sum_{j=1}^{1000} e^{\Theta_j^T e_c}}$
- Loss: $L(\hat{y}, y) = - \sum_{i=1}^{1000} y_i log \hat{y_i}$
- 通过反向传播更新模型的参数



**缺点**

- softmax层需要对所有的单词进行指数运算，当词汇表很大时计算量太大，此时可以采用分级（hierarchical）softmax （一种树状结构）来简化运算
- 在随机采样时，一些常用词如 *the*，*of* 会经常出现，而 *durian* 这样的词会很少出现，实际应用中 $P(c)$ 并不会从文本中随机均匀采样，而是采用不同的启发模式来平衡常见词汇和罕见的词汇



#### 2.3 负采样 （Negative Sampling）







## 3 词嵌入的应用







[1]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_onehot.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_feature.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1567517280/DL_coursera/WordEmbeddings_implementation.jpg
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1567517720/DL_coursera/WordEmbeddings_RelationFaceEncoding.jpg
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1567519740/DL_coursera/WordEmbeddings_analogies.jpg
[6]:https://res.cloudinary.com/bxy1994/image/upload/v1567676126/DL_coursera/WordEmbeddings_learn.png