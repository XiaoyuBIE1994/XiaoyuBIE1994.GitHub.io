---


layout: post
title: Deep Learning by Andrew (5-2) 词嵌入（Word Embeddings）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>





## 1. 词嵌入（Word Embeddings）简介

#### 1.1 word repressentation

**one-hot repressentation**

在之前的文本分析中，我们使用 one-hot 表征来表示对应模型中的字典，对应单词的位置用1表示，其余位置为0，这样的表征方式简单明了，易于代入计算，但这一方法只考虑了单个词在向量中的作用，没有考虑词与词之间的联系。由于不同的 one-hot vector 之间的乘积为0，在文本分析时无法有效的利用单词本身的各种属性，所以在NLP问题中，我们考虑使用各类特征来表示单词



![][1]



**featurized repressentation**

不同的单词由不同的特征来描述，从逻辑上也更加符合人类对于语言的认知，同时也降低了大规模语言任务中的计算难度

![][2]



#### 1.2 Word embeddings 的使用

Word Embeddings对不同单词进行了实现了特征化的表示，如下图所示

![][3]

当我们的数据集很小时，可能并不包含 durian（榴莲）和 cultivator（培育家）这两个单词，但如果我们提前做好  word embeddings，我们就可以知道”榴莲“和”苹果“都属于水果，”培育家“和”农民“相似，那我们便不难推断出 durian cultivator 的意思了，这就让我们有了从少量训练集中归纳出没有出现过的实体的能力，也就是泛化能力。

有了词嵌入，我们便有了迁移学习的能力：

- 从大量的文本中进行词迁移学习（1-100B words），或者从网上下载预先训练好的词嵌入模型
- 将词嵌入模型嵌入到我们所需要的小训练集上
- 可选，使用新的标记数据集对模型进行微调（finetune）



**词嵌入和人脸编码**

这两者之间有着有趣的联系。对于人脸识别，我们会训练一个 Siamese 网络来学习，用一个128维的向量来表示不同的面孔，然后比较这些编码用于识别。区别在于，对于人脸识别，我们通过训练一个网络来应对任意的人脸输入，而对于词嵌入，我们则会有一个单独的词汇库

![][4]



#### 1.3 Word embeddings 的性质

通过词嵌入，我们可以将单词转化为更小维度上的 feature vector，除了能够降低表征复杂度外，词嵌入还有一个重要的特性，它还能够帮助实现类比推理，比如 man - women 和 king - queen 之间就存在着相同的内在联系，同为男女对应，表现在 word vertors 中两两vectors之间的向量差是相似的

![][5]



我们使用余弦相似度函数（Cosine Similarity）来定义这一相似程度：


$$
\begin{aligned}
& sim(u,v) = \frac{u^T v}{\parallel u \parallel_2 \parallel v \parallel_2} \\
& u = e_w \\
& v = e_{king} - e_{man} + e_{women} 
\end{aligned}
$$


当然，我们也可以用欧几里得距离:


$$
\parallel u - v \parallel^2 
$$


技术上，这是对**非相似度**的计算，而不是**相似度**的计算。这两者的区别主要在于如何规范化 $u$ 和 $v$ 的长度。因此，嵌入词的一个显著特征是可以学习到类比关系的普遍性，比如说它可以学习男人对应女人之于男孩对应女孩，也能学到当日元是日本的货币时，那么日元对应日本正如卢布对应俄罗斯。

日元是日本的货币，那么日元对应日本正如卢布对应俄罗斯



#### 1.4  Embeddings matrix

当我们要学习一个词嵌入模型时，我们实际上是需要对词汇表学习一个嵌入矩阵 $E$ ，学习完毕后通过嵌入矩阵与对应词的 one-hot 向量相乘，得到词汇的 embeddings：


$$
e_j = E \cdot o_j
$$








[1]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_onehot.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_feature.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1567517280/DL_coursera/WordEmbeddings_implementation.jpg
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1567517720/DL_coursera/WordEmbeddings_RelationFaceEncoding.jpg
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1567519740/DL_coursera/WordEmbeddings_analogies.jpg