---

layout: post
title: Deep Learning by Andrew (5-2) 词嵌入（Word Embeddings）
category: 笔记
tags: 
  - deep learning

---

<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>





## 1. 词嵌入（Word Embeddings）简介

#### 1.1 word repressentation

**one-hot repressentation**

在之前的文本分析中，我们使用 one-hot 表征来表示对应模型中的字典，对应单词的位置用1表示，其余位置为0，这样的表征方式简单明了，易于代入计算，但这一方法只考虑了单个词在向量中的作用，没有考虑词与词之间的联系。由于不同的 one-hot vector 之间的乘积为0，在文本分析时无法有效的利用单词本身的各种属性，所以在NLP问题中，我们考虑使用各类特征来表示单词



![][1]



**featurized repressentation**

不同的单词由不同的特征来描述，从逻辑上也更加符合人类对于语言的认知，同时也降低了大规模语言任务中的计算难度

![][2]



#### 1.2 Word embeddings 的使用

Word Embeddings对不同单词进行了实现了特征化的表示，如下图所示

![][3]

当我们的数据集很小时，可能并不包含 durian（榴莲）和 cultivator（培育家）这两个单词，但如果我们提前做好  word embeddings，我们就可以知道”榴莲“和”苹果“都属于水果，”培育家“和”农民“相似，那我们便不难推断出 durian cultivator 的意思了，这就让我们有了从少量训练集中归纳出没有出现过的实体的能力，也就是泛化能力。

有了词嵌入，我们便有了迁移学习的能力：

- 从大量的文本中进行词迁移学习（1-100B words），或者从网上下载预先训练好的词嵌入模型
- 将词嵌入模型嵌入到我们所需要的小训练集上
- 可选，使用新的标记数据集对模型进行微调（finetune）



**词嵌入和人脸编码**

这两者之间有着有趣的联系。对于人脸识别，我们会训练一个 Siamese 网络来学习，用一个128维的向量来表示不同的面孔，然后比较这些编码用于识别。区别在于，对于人脸识别，我们通过训练一个网络来应对任意的人脸输入，而对于词嵌入，我们则会有一个单独的词汇库

![][4]



#### 1.3 Word embeddings 的性质

通过词嵌入，我们可以将单词转化为更小维度上的 feature vector，除了能够降低表征复杂度外，词嵌入还有一个重要的特性，它还能够帮助实现类比推理，比如 man - women 和 king - queen 之间就存在着相同的内在联系，同为男女对应，表现在 word vertors 中两两vectors之间的向量差是相似的

![][5]



我们使用余弦相似度函数（Cosine Similarity）来定义这一相似程度：


$$
\begin{aligned}
& sim(u,v) = \frac{u^T v}{\parallel u \parallel_2 \parallel v \parallel_2} \\
& u = e_w \\
& v = e_{king} - e_{man} + e_{women} 
\end{aligned}
$$


当然，我们也可以用欧几里得距离:


$$
\parallel u - v \parallel^2 
$$


技术上，这是对**非相似度**的计算，而不是**相似度**的计算。这两者的区别主要在于如何规范化 $u$ 和 $v$ 的长度。因此，嵌入词的一个显著特征是可以学习到类比关系的普遍性，比如说它可以学习男人对应女人之于男孩对应女孩，也能学到当日元是日本的货币时，那么日元对应日本正如卢布对应俄罗斯。

日元是日本的货币，那么日元对应日本正如卢布对应俄罗斯



#### 1.4  Embeddings matrix

当我们要学习一个词嵌入模型时，我们实际上是需要对词汇表学习一个嵌入矩阵 $E$ ，学习完毕后通过嵌入矩阵与对应词的 one-hot 向量相乘，得到词汇的 embeddings：



$$
e_j = E \cdot o_j
$$



但实际应用中，直接的矩阵相乘是非常费时的。同时，我们观察到，$o_j$ 是一个 one-hot 向量，这就意味着其实有大量的乘零运算，这些运算是无效的，实际上只有 $E$ 中的的某一列参与了有效的运算，所以我们可以使用特殊的算法来直接查找嵌入矩阵中对应的那一列作为运算的结果，而不是进行矩阵运算。




## 2 词嵌入学习：Word2Vec & GloVe

词嵌入的学习随着研究的进行变得越来越简单，但这里先从早期比较复杂的算法开始，能够让我们更好的看到词嵌入学习的机理。



#### 2.1 早期的学习算法

以 *I want a glass of orange juice* 为例，其中前面的6个单词为输入，*juice* 为输出，步骤如下：

- 将每个单词的 one-hot 向量与嵌入矩阵相乘，得到相应的 embeddings 向量
- 通过相应的窗口控制影响预测结果的单词数量，如这里我们只选择 *a glass of orange* ，也就是4个输入
- 通过隐藏层和 softmax 层，输入最后的单词的预测，其中模型的参数包括嵌入矩阵 $E$，隐藏从以及 softmax 层的参数 $w^{[1]}, b^{[1]},w^{[2]}, b^{[2]}$
- 可以利用反向传播来进行梯度下降，最大化模型的似然概率
- 当我们进行文本分析的时候，也可以结合上下语境来进行分析，这个时候窗口的选择可以扩展到前后若干个词或者某个、某几个单词



![][6]



#### 2.2 Word2Vec

Word2Vec算法是一种简单高效词嵌入的学习方式，以 *I want a glass of orange juice to go along with my cereal* 为例

**skip-gram 模型**

在这一模型中，我们需要找出一些语境词（Content）和目标词（Target）配对，从而建立监督学习的问题。其中，我们随机选择一个单词作为目标词（例如 *orange*），再以一定的窗口随机选择目标词（例如以前后五个单词为窗口，选中了 *juice* 或者 *glass*）所以我们建立的监督学习模型，就需要预测在给定语境词下，目标词会出现什么，以下是模型的细节：

- 使用一个具有大量词汇的词汇表，如Vocab size = 10000k
- 构建监督学习问题，语境词（C）到目标词（T）的映射 （$x \rightarrow y$）
- 网络的结构为 $o_c(one-hot) \rightarrow E(\text{词嵌入矩阵}) \rightarrow e_c \rightarrow Softmax \text{层} \rightarrow \hat{y} $
- Softmax: $p(t \mid c) = \frac{e^{\Theta_t^T e_c}}{\sum_{j=1}^{1000} e^{\Theta_j^T e_c}}$
- Loss: $L(\hat{y}, y) = - \sum_{i=1}^{1000} y_i log \hat{y_i}$
- 通过反向传播更新模型的参数



**缺点**

- softmax层需要对所有的单词进行指数运算，当词汇表很大时计算量太大，此时可以采用分级（hierarchical）softmax （一种树状结构）来简化运算
- 在随机采样时，一些常用词如 *the*，*of* 会经常出现，而 *durian* 这样的词会很少出现，实际应用中 $P(c)$ 并不会从文本中随机均匀采样，而是采用不同的启发模式来平衡常见词汇和罕见的词汇



#### 2.3 负采样 （Negative Sampling）

Skip-gram 模型可以有效的建立起监督学习的机制，将语境词（上下文）映射到目标词上，但使用 softmax 的计算代价太高，而负采样可以优化这一过程，提高模型的学习效率。

**模型简介**

- 定义新的学习问题，预测两个词是否为语境词与目标词的对应，如果是则为1，否则为0
- 选取训练集中已有的对照，如 *orange* 和  *juice*，输出为1，然后语境词不变，随机选取其他的单词作为目标词，输出为0，作为负样本
- 负样本的个数为k，对于小数据集，k建议取5~20，对于大数据集，k建议取2~5
- 负采样中才去 logistic 回归模型，$P(y=1 | c) = \sigma (\Theta^T e_c)$ 
- 每个正样本对应k个负样本，每次只针对这 k+1 个分类器进行学习，这样就降低了训练的计算量
- 在选取负样本上，如果按照单词出现的频率进行采样，会导致一些常见词频繁出现，如a，the，of等，而均匀采样又不能很好的代表单词的分布，所以根据作者 Mikolov 等人的经验，使用以下的概率进行采样能取得最好的效果 $P(w_i) = \frac{f(w_i)^{3/4}}{\sum f(w_j)^{3/4}}$



#### 2.4 GloVe 词向量

GloVe词向量NLP问题中另一种模型结构，他虽然没有 Word2Vec 那样应用广泛，但由于其简洁性，也拥有着一定的发展势头，GloVe 全称为 global vectors for word repressentation，在模型中我们定义一变量 $X_{ij}$，表示目标单词 $j$ 出现在情景词/上下文 $i$ 中的次数，模型的优化目标为：


$$
minimize \sum_{i=1}^{1000} \sum_{j=1}^{1000} f(X_{ij}) (\theta_i^T e_j + b_i + b_j' -log X_{ij})^2
$$


- 其中，当 $X_{ij}$ 为0时会出现对数函数无意义的情况，所以添加额外的权重项 $f(X_{ij}) = 0, if \; X_{ij}=0$ ，同时需要满足函数为递增函数，且增加到一定程度后需要增速衰减或者不在增加，有许多函数都满足这一性质，如何选择可以参考 GloVe 论文
- 在 $\theta_i^T e_j$ 中 $\theta_i$  和 $ e_j$ 是完全对称的，完全可以反过来或者对调他们，因此训练这个算法的一种方式是随机均匀的初始化他们，运用梯度下降来最小化目标函数，最后取平均数 $e_w^{(final)} = \frac{e_w + \theta_w}{2}$
- $b_i$ 和 $b_j'$ 属于 bias term，用于平衡常见词和罕见词的权重，使得常见词的权重不至于过高，罕见词的权重不至于过低



#### 2.5 词嵌入的特征化（featurization）

在 1.1 的 featurized representation 中，我们看到词嵌入向量的第一项表示了“性别”，第二项表示了”皇室属性“，但在实际的使用中，我们无法确保嵌入向量的坐标和这些属性是对齐的，甚至会出现并不正交的轴，如 GloVe 算法中，我们可以通过矩阵转换来转换成其他的向量：


$$
\theta_i^T e_j = \theta_i^T A^T A^{-T}  e_j = (A \theta_i)^T (A^{-T}  e_j)
$$


这说明这种算法无法保证特征的坐标能轻易地联系上人类所能理解的属性，这样嵌入向量的单个元素就很难具有可解释性。虽然有着可解释性上的困难，但 1.3 中用于描述类比的平行四边形图仍然是有效的。



## 3 词嵌入的应用



#### 3.1 情感分析（sentiment classification）

情感分类的问题在于很多时候训练集都很小，缺乏样本，但使用了词嵌入之后能够带来很好的效果，我们可以使用平均值/求和模型或者 RNN 模型来进行情感分析：

**平均值/求和模型**

- 使用训练好的词嵌入矩阵 E 将 one-hot 向量转化为词嵌入向量
- 对所得到的所有词嵌入向量取平均或者求和，输入到 softmax 分类器中，输出 $\hat{y}$
- 缺点：没有考虑词序，会导致多数积极词汇消除了消极词汇的影响，而导致错误的判断



**RNN 模型**

- 同样使用训练好的词嵌入矩阵 E 将 one-hot 向量转化为词嵌入向量
- 将词嵌入向量输入到 many-to-one 的 RNN 网络中，最后通过 softmax 分类器输出预测结果 $\hat{y}$
- 优点：考虑了词序，判断会更加精准



#### 3.2 词嵌入偏差消除

在目前的语料库中，我们可能会学习到一些带有性别、种族的偏见，而我们可以利用词嵌入来消除这种偏见：

- 首先定义偏见的方向，如性别
- 对大量的性别相对的词汇进行相减求平均：如 $e_{he} - e_{she},\; e_{male}-e_{femail}$  等
- 通过平均后的向量，可以得到一个或者多个偏见相关的纬度和大量不相关的纬度
- 中和化（Neutralize）：对于一个定义不明确的词汇，进行偏见的预处理（如 doctor、babysistter），减少这些词汇在偏见相关维度的值，如下图：

![][7]

步骤为：


$$

$$




- 均衡（Equalize ）：有些单词是必须要带有性别特征的，比如胡须就更加偏向男性，我们需要用一个分类器来识别哪些词语需要性别特征，哪些词语不需要，同时具有很强的性别对应关系的单词，如 actor 和 actress，我们需要做单独的均衡化处理，使得他们在非 bias 分量上完全对称，如下图所示：



![][8]



具体步骤为：

$$
\begin{aligned}
& \mu = \frac{e_{w1} + e_{w2}}{2} \\

& \mu_{B} = \frac {\mu \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \\

& \mu_{\perp} = \mu - \mu_{B} \\

& e_{w1B} = \frac {e_{w1} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \\

& e_{w2B} = \frac {e_{w2} \cdot \text{bias_axis}}{||\text{bias_axis}||_2^2} *\text{bias_axis} \\

& e_{w1B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w1B}} - \mu_B} {||(e_{w1} - \mu_{\perp}) - \mu_B||} \\

& e_{w2B}^{corrected} = \sqrt{ |{1 - ||\mu_{\perp} ||^2_2} |} * \frac{e_{\text{w2B}} - \mu_B} {||(e_{w2} - \mu_{\perp}) - \mu_B||} \\

& e_1 = e_{w1B}^{corrected} + \mu_{\perp} \\

& e_2 = e_{w2B}^{corrected} + \mu_{\perp} \\
\end{aligned}
$$



## 4 代码实现

以下代码参考自吴恩达的Deep Learning课程作业，并不是作业的答案，仅作加深理解和记忆之用，**希望仍在上课的学生能够独立完成代码** 



#### 4.1 Debiasing

首先，定义相似度函数：


$$
\text{CosineSimilarity(u, v)} = \frac {u . v} {||u||_2 ||v||_2} = cos(\theta)
$$


```python
def cosine_similarity(u, v):
    dot = np.dot(u,v)
    norm_u = np.linalg.norm(u)
    norm_v = np.linalg.norm(v)
    cosine_similarity = dot / (norm_u * norm_v)
    return cosine_similarity
```

接着，我们可以通过定义的相似度函数，找到词典中最接近的对应关系，如 italy -> italian 就对应 spain -> spanish：

```python
def complete_analogy(word_a, word_b, word_c, word_to_vec_map):
    
    word_a, word_b, word_c = word_a.lower(), word_b.lower(), word_c.lower()
    e_a, e_b, e_c = word_to_vec_map[word_a], word_to_vec_map[word_b], word_to_vec_map[word_c]
    
    words = word_to_vec_map.keys()
    max_cosine_sim = -100             
    best_word = None   

    for w in words:        
        if w in [word_a, word_b, word_c] :
            continue
        cosine_sim = cosine_similarity(e_b - e_a, word_to_vec_map[w] - e_c)
        if cosine_sim > max_cosine_sim:
            max_cosine_sim = cosine_sim
            best_word = w
            
    return best_word
```



中和化，如前公式所示：

```python
def neutralize(word, g, word_to_vec_map):
  
    e = word_to_vec_map[word]
    e_biascomponent = np.dot(e, g) / np.square(np.linalg.norm(g)) * g
    e_debiased = e - e_biascomponent
    
    return e_debiased
```

均衡化,如前公式所示：

```python
def equalize(pair, bias_axis, word_to_vec_map):

    w1, w2 = pair[0], pair[1]
    e_w1, e_w2 = word_to_vec_map[w1], word_to_vec_map[w2]
    
    mu = (e_w1 + e_w2) / 2
    mu_B = np.dot(mu, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis
    mu_orth = mu - mu_B

    e_w1B = np.dot(e_w1, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis
    e_w2B = np.dot(e_w2, bias_axis) / np.square(np.linalg.norm(bias_axis)) * bias_axis
    corrected_e_w1B = np.sqrt(np.abs(1 - np.square(np.linalg.norm(mu_orth)))) * (e_w1B - mu_B) / np.linalg.norm(e_w1 - mu_orth - mu_B)
    corrected_e_w2B = np.sqrt(np.abs(1 - np.square(np.linalg.norm(mu_orth)))) * (e_w2B - mu_B) / np.linalg.norm(e_w2 - mu_orth - mu_B)
    e1 = corrected_e_w1B + mu_orth
    e2 = corrected_e_w2B + mu_orth

    return e1, e2
```



#### 4.2 Emojify

Emojify 模型目标在于输入一段话，然后输出相应的表情，属于情感分析问题，根据之前的笔记，可以有平均值模型和 RNN 模型两种解决办法



**平均值模型**

![][9]



```python
def sentence_to_avg(sentence, word_to_vec_map):
    # Step 1: Split sentence into list of lower case words
    words = sentence.lower().split()
    # Initialize the average word vector, should have the same shape as your word vectors.
    avg = np.zeros((50,))
    # Step 2: average the word vectors. You can loop over the words in the list "words".
    for w in words:
        avg += word_to_vec_map[w]
    avg = avg / len(words)
    return avg
  
def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):
    # Define number of training examples
    m = Y.shape[0]                          # number of training examples
    n_y = 5                                 # number of classes  
    n_h = 50                                # dimensions of the GloVe vectors 
    
    # Initialize parameters using Xavier initialization
    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)
    b = np.zeros((n_y,))
    
    # Convert Y to Y_onehot with n_y classes
    Y_oh = convert_to_one_hot(Y, C = n_y) 
    
    # Optimization loop
    for t in range(num_iterations):  # Loop over the number of iterations
        for i in range(m):  # Loop over the training examples
            # Average the word vectors of the words from the i'th training example
            avg = sentence_to_avg(X[i], word_to_vec_map)
            # Forward propagate the avg through the softmax layer
            z = np.dot(W, avg) + b
            a = softmax(z)
            # Compute cost using the i'th training label's one hot representation and "A" (the output of the softmax)
            cost = - np.sum(Y_oh[i] * np.log(a))
            # Compute gradients 
            dz = a - Y_oh[i]
            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))
            db = dz
            # Update parameters with Stochastic Gradient Descent
            W = W - learning_rate * dW
            b = b - learning_rate * db
        if t % 100 == 0:
            print("Epoch: " + str(t) + " --- cost = " + str(cost))
            pred = predict(X, Y, W, b, word_to_vec_map)

    return pred, W, b
  
pred, W, b = model(X_train, Y_train, word_to_vec_map)
```



**LSTM 模型**

![][10]



```python
# 将一句话转换成相应的 indices 编号
def sentences_to_indices(X, word_to_index, max_len):
    m = X.shape[0]  # number of training examples

    # Initialize X_indices as a numpy matrix of zeros and the correct shape (≈ 1 line)
    X_indices = np.zeros((m, max_len))
    
    for i in range(m):    # loop over training examples
        # Convert the ith training sentence in lower case and split is into words. You should get a list of words.
        sentence_words = X[i].lower().split()
        # Initialize j to 0
        j = 0
        # Loop over the words of sentence_words
        for w in sentence_words:
            # Set the (i,j)th entry of X_indices to the index of the correct word.
            X_indices[i, j] = word_to_index[w]
            # Increment j to j + 1
            j = j+1
    
    return X_indices
  
# 建立 embedding 层，将相应单词的转化为词嵌入，这一层不参与训练
def pretrained_embedding_layer(word_to_vec_map, word_to_index):
    vocab_len = len(word_to_index) + 1  # adding 1 to fit Keras embedding (requirement)
    emb_dim = word_to_vec_map["cucumber"].shape[0]# define dimensionality of your GloVe word vectors (= 50)
    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)
    emb_matrix = np.zeros((vocab_len, emb_dim))
    # Set each row "index" of the embedding matrix to be the word vector representation of the "index"th word of the vocabulary
    for word, index in word_to_index.items():
        emb_matrix[index, :] = word_to_vec_map[word]
    # Define Keras embedding layer with the correct output/input sizes, make it non-trainable. Use Embedding(...). Make sure to set trainable=False. 
    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)
    # Build the embedding layer, it is required before setting the weights of the embedding layer. Do not modify the "None".
    embedding_layer.build((None,))
    # Set the weights of the embedding layer to the embedding matrix. Your layer is now pretrained.
    embedding_layer.set_weights([emb_matrix])
    
    return embedding_layer
  
# 建立 LSTM 模型，基于 Keras
def Emojify_V2(input_shape, word_to_vec_map, word_to_index):
    # Define sentence_indices as the input of the graph, it should be of shape input_shape and dtype 'int32' (as it contains indices).
    sentence_indices = Input(shape=input_shape, dtype='int32')
    # Create the embedding layer pretrained with GloVe Vectors (≈1 line)
    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)
    # Propagate sentence_indices through your embedding layer, you get back the embeddings
    embeddings = embedding_layer(sentence_indices)   
    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a batch of sequences.
    X = LSTM(128, return_sequences=True)(embeddings)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X trough another LSTM layer with 128-dimensional hidden state
    # Be careful, the returned output should be a single hidden state, not a batch of sequences.
    X = LSTM(128, return_sequences=False)(X)
    # Add dropout with a probability of 0.5
    X = Dropout(0.5)(X)
    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.
    X = Dense(5, activation='softmax')(X)
    # Add a softmax activation
    X = Activation('softmax')(X)
    
    # Create Model instance which converts sentence_indices into X.
    model = Model(inputs=sentence_indices, outputs=X)
    
    return model
  
# 模型的编译与训练，实际使用时根据需要选取相应的训练集
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)
Y_train_oh = convert_to_one_hot(Y_train, C = 5)
model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)
# 训练后网络性能的分析
X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)
Y_test_oh = convert_to_one_hot(Y_test, C = 5)
loss, acc = model.evaluate(X_test_indices, Y_test_oh)
# 预测，例子
x_test = np.array(['not feeling happy'])
model.predict(X_test_indices)
```





[1]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_onehot.jpg
[2]: https://res.cloudinary.com/bxy1994/image/upload/v1567514516/DL_coursera/WordEmbeddings_feature.png
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1567517280/DL_coursera/WordEmbeddings_implementation.jpg
[4]:https://res.cloudinary.com/bxy1994/image/upload/v1567517720/DL_coursera/WordEmbeddings_RelationFaceEncoding.jpg
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1567519740/DL_coursera/WordEmbeddings_analogies.jpg
[6]:https://res.cloudinary.com/bxy1994/image/upload/v1567676126/DL_coursera/WordEmbeddings_learn.png
[7]:https://res.cloudinary.com/bxy1994/image/upload/v1567960528/DL_coursera/WordEmbeddings_neutralization.png
[8]:https://res.cloudinary.com/bxy1994/image/upload/v1567960033/DL_coursera/WordEmbeddings_equalization.png
[9]:https://res.cloudinary.com/bxy1994/image/upload/v1567966417/DL_coursera/WordEmbeddings_emojify-v1.png
[10]:https://res.cloudinary.com/bxy1994/image/upload/v1567966417/DL_coursera/WordEmbeddings_emojify-v2.png