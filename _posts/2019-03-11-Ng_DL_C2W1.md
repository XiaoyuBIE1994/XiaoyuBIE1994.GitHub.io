---

layout: post
title: Deep Learning by Andrew (2-1) 数据集,Bias/Variance,正则与优化
category: 笔记DeepLearning)
tags: 
  - deep learning
---


<style>
img{
    width: 60%;
    padding-left: 20%;
}
</style>



## 1. 机器学习问题的设定与评估



####  1.1 训练集，验证集和测试集

对于一个机器学习问题，通常我们从提出想法（idea）到代码实施（code），再到实验验证（Experiment），再重新审视idea，在此过程中，我们需要针对不同的模型和超参进行反复的实验，通过一定的评估手段选出最优的模型和参数，最后评估我们得到的结果。所以，我们通常会将数据集分成以下三个部分：



> 训练集（training set）：用于对算法和模型进行训练
>
> 验证集（development set）：用于交叉验证，进行模型和参数的选择，也叫作交叉验证集（cross-validation  set）
>
> 测试集（testing set）：用于对我们最终的模型进行评估



对于数据量较小的数据集，比如100,1000,10000，可做如下划分：

> 无验证集：70%/30%
>
> 有验证集：60%/20%/20%



而对于量级特别巨大的数据集，过多的验证和测试样本并没有太大的必要，而将这一部分划分到训练集则可以有效的减少过拟合的情况，所以我们改变划分标准：



> 百万级数据量：98%/1%/1%/
>
> 更大的数据量：99.5%/0.25%/0.25%



**Notation:**

1. 尽量使验证集和测试集保持统一分部，这样调参所得到的最优结果也会在评估中得到最好的表现
2. 如果不需要对模型进行无偏估计，可以不需要测试集



#### 1.2 Bias 和 Variance

在Machine Learning的课程中已经对 Bias 和 Variance 进行过介绍，大致的定义如下：



>**High Bias**：训练集和测试集同时呈现出较大的误差，可以看做是欠拟合，也就是没有正确的找到数据的边界
>
>**High Variance**：训练集上的误差很小，但测试集上的误差较大，可以看作是过拟合，也就是模型过于复杂以至于在训练的过程中将噪声也考虑了进去
>
>**High Bias and High Variance**：表现与训练集和测试集误差都很大，而且两者的误差并不接近，也就是模型既没有找到边界，同时又在部分区域过度的拟合，在高维数据中较为可能出现



**Notation**

1. 误差是否大需要根据具体问题进行分析，通常认为满足贝叶斯误差时就已经达到了最优



面对 High Bias，可用的解决方案为：

1. 增加网络结构，如隐藏层的层数
2. 训练更长的时间
3. 寻找合适的网络架构



面对 High Variance，可用的解决方案为：

1. 获取更多的数据
2. 添加正则化（Regularization）
3. 寻找合适的网络架构



## 2. 正则化

正则化被广泛的应用于深度学习任务之中，可以有效的防止魔性的过拟合，常用的正则化手段包括添加损失函数中添加正则项和Dropout正则



#### 2.1 正则项（Regularization）


$$
J(\theta) = J(\theta)_{origin} + \frac{\lambda}{2m} \sum_{l=1}^{L} \parallel w^{[l]} \parallel^2_F
$$


由于在NN中参数 $ w^{[l]} $ 是矩阵，所以在机器学习中常用的L2范数在这里叫做 **Frobenius norm**



**Weight decay**

加入正则项之后，梯度变成了：


$$
dW^{[l]} = (form\_backprop) + \frac{\lambda}{m}W^{[l]}
$$


梯度更新变为：


$$
W^{[l]} := (1-\frac{\alpha\lambda}{m})W^{[l]} - \alpha (form\_backprop)
$$


其中， $(1-\frac{\alpha\lambda}{m} )$ 是一个小于1的项，会给原来的 $W^{[l]} $ 带来一个衰减，所以L2范数正则化也被称为”权重衰减（weight decay）“   



**Intuition for Regularization**

直观上理解，加入了正则项之后，各层网络的参数 $$ 会变得更小，甚至是趋近于0，相当于消除了部分神经元的影响，大网络因此而变成了小网络，也就避免了过拟合。以上的解释虽然直观，但实际上这些神经元仍然存在，只是影响变小，但并未消除。而数学上有着另一种解释，如下图所示：



![][1]



当 $\lambda $ 增加时，会导致 $W^{[l]}$ 减小，那么 $Z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$ 便会减小，若激活函数为 tanh, 在 z 较小的区域，函数接近线性化，也就是网络在一定程度上简化为一个线性网络，也就减少了过拟合的几率



#### 2.2 Dropout Regularization

Dropout是另外一种正则化的手段，具体做法是在训练时，随机消除网络中的部分节点，保留下来的神经元构成了一个节点较少，规模更小的网络，如下图所示：



![][2]

**Dropout 的实现**

dropout采用反向随机失活来实现（inverted dropout），假设对第 $l$ 层采用 dropout:





```python
keep_prob = 0.8
dl = np.random.rand(al.shape[0], al.shape[1]) < keep_prob
al = np.multiply(al, dl)
al /= keep_prob
```

最后一行的意义在于不影响 $Z^{[l+1]}$ 的期望值



**Intuition for Dropout**

从单一的神经元来看待这个问题，这个节点的任务是利用输入神经元来产生一个有意义的输出，但由于他的任意一个输入神经元都有可能失活，所以他必须要保证不能依赖于任何一个特征，因为每个都有可能被丢弃，所以也避免了这个节点给与某个输入神经元过大的权重，从而在传播的过程中产生了和L2范数一样的收缩权重的效果。



**Drawback**

dropout的一大缺点是使得 cost function 不能再被明确的定义，所以我们无法画出每次迭代后 $J(\theta)$ 的下降图。通常先关闭 dropout 来观察损失函数，确保其是单调递减的，然后再打开 dropout 并期待使用 dropout 的时候没有引入别的错误





**Notation**

1. 在测试阶段需要关闭dropout, 避免预测结果随机化
2. 对于不同的层，需要设置不同的keep_prob值，通常较小的层直接设置为1
3. Dropout 多用于CV领域，因为输入层的维度通常非常大，在其他领域应用并不多，除非确定已经过拟合，否则在其他领域不要过于频繁的使用 dropout



#### 2.3 其余的正则化手段



**数据扩增（Data augmentation）**

通过对图片的一些变化，如旋转、翻转、改变色域等，来扩充数据集



**Early Stopping**

在交叉验证集的误差上升之前的点停止迭代，避免过拟合。这种方法的缺点是无法同时解决bias和variance之间的最优

![][3]





## 3. 优化问题

#### 3.1 输入归一化

训练神经网络时，有一种加速训练的方法是对输入进行归一化：


$$
x = \frac{x - \mu}{\sigma^2}
$$


![][4]





如果没有进行数据归一化，不同维度之间数据的尺寸不平衡，那么在训练的时候就必须使用较小的学习率，这样会使得模型的收敛速度非常慢，而经过归一化之后的数据，无论从何处开始都会更容易想最小处而去。



**Notation**

1. 如果对数据进行缩放，务必保证测试集和训练集使用同样的 $\mu$ 和 $\sigma^2$



#### 3.2 梯度消失与梯度爆炸



![][5]

如上图所示，当网络足够深时，梯度函数会以指数级递增或者递减，导致训练难度上升，梯度下降算法的步长会变得非常非常小，训练时间会非常的长。这种在梯度函数上出现的指数级递增或递减就叫做梯度爆炸和梯度消失。

通过随机初始化网络参数可以一定程度上缓解梯度爆炸和梯度消失问题，如 **Xavier Initialization**：



```python
WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(1/n) # tanh activation
WL = np.random.randn(WL.shape[0],WL.shape[1])* np.sqrt(2/n) # ReLu activation
```



#### 3.3 梯度检查

当我们新建一个网络结构的时候， 我们需要一下网络是否搭建正确，尤其是其反向传播部分，这时可以利用双边差的方式来逼近导数：

双边导数：


$$
f'(\theta) = \mathop{\lim}_{\epsilon \to 0} \frac{f(\theta + \epsilon) - f(\theta - \epsilon)}{2 \epsilon}
$$
双边导数的误差为 $O(\epsilon^2)$ ，而单边导数的误差为  $O(\epsilon)$，[证明](https://stats.stackexchange.com/questions/318380/why-is-two-sided-gradient-checking-more-accurate)

通过比较双边差求导和反向传播得出导数来判断模型是否正确：


$$
\frac{\parallel d\theta_{approx} - d\theta \parallel_2}{\parallel d\theta_{approx} \parallel_2 + \parallel d\theta \parallel_2}
$$


**Notation**

1. 不要在训练过程中进行梯度检查，只在 debug 的时候使用
2. 不要忘记正则项
3. 梯度检查不能和 dropout 同时使用
4. 梯度检查可能在最开始检查不出梯度的错误，可以在训练的过程中每隔一段时间停下来进行检查



PS：这一部分的代码实现并不难，这里不做整理了





[1]: https://res.cloudinary.com/bxy1994/image/upload/v1552495877/DL_coursera/dnn_regularization.jpg
[2]:  https://res.cloudinary.com/bxy1994/image/upload/v1552496796/DL_coursera/dnn_dropout.jpg
[3]: https://res.cloudinary.com/bxy1994/image/upload/v1552498318/DL_coursera/dnn_EarlyStopping.jpg
[4]: https://res.cloudinary.com/bxy1994/image/upload/v1552498566/DL_coursera/dnn_norm.jpg
[5]: https://res.cloudinary.com/bxy1994/image/upload/v1552499005/DL_coursera/dnn_gradient_VanishingExploding.jpg


